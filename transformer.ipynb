{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNv3eORova9sRjWjVqTGIF8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohEder/bachelor_thesis_audio_ml/blob/master/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIBl-ybmvCJ7",
        "outputId": "379a033d-9751-4e75-c43c-a588016113c2"
      },
      "source": [
        "!pip install torchaudio\n",
        "!pip install pytorch-model-summary"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchaudio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/20/eab40caad8f4b97f5e91a5de8ba5ec29115e08fa4c9a808725490b7b4844/torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchaudio) (3.7.4.3)\n",
            "Installing collected packages: torchaudio\n",
            "Successfully installed torchaudio-0.9.0\n",
            "Collecting pytorch-model-summary\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/45/01d67be55fe3683a9221ac956ba46d1ca32da7bf96029b8d1c7667b8a55c/pytorch_model_summary-0.1.2-py3-none-any.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch-model-summary) (1.9.0+cu102)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-model-summary) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-model-summary) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-model-summary) (3.7.4.3)\n",
            "Installing collected packages: pytorch-model-summary\n",
            "Successfully installed pytorch-model-summary-0.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdn43FFt6jp0"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset \n",
        "import torchaudio\n",
        "import pandas as pd\n",
        "import os\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm9hEziO6l5F"
      },
      "source": [
        "#just copied the official import script for the dataset, custom preprocessing happens afterwards\n",
        "\"\"\" Import script for IDMT-Traffic dataset\n",
        "Ref:\n",
        "    J. Abeßer, S. Gourishetti, A. Kátai, T. Clauß, P. Sharma, J. Liebetrau: IDMT-Traffic: An Open Benchmark\n",
        "    Dataset for Acoustic Traffic Monitoring Research, EUSIPCO, 2021\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "__author__ = 'Jakob Abeßer (jakob.abesser@idmt.fraunhofer.de)'\n",
        "\n",
        "\n",
        "def import_idmt_traffic_dataset(fn_txt: str = \"idmt_traffic_all\") -> pd.DataFrame:\n",
        "    \"\"\" Import IDMT-Traffic dataset\n",
        "    Args:\n",
        "        fn_txt (str): Text file with all WAV files\n",
        "    Returns:\n",
        "        df_dataset (pd.Dataframe): File-wise metadata\n",
        "            Columns:\n",
        "                'file': WAV filename,\n",
        "                'is_background': True if recording contains background noise (no vehicle), False else\n",
        "                'date_time': Recording time (YYYY-MM-DD-HH-mm)\n",
        "                'location': Recording location\n",
        "                'speed_kmh': Speed limit at recording site (km/h), UNK if unknown,\n",
        "                'sample_pos': Sample position (centered) within the original audio recording,\n",
        "                'daytime': M(orning) or (A)fternoon,\n",
        "                'weather': (D)ry or (W)et road condition,\n",
        "                'vehicle': (B)us, (C)ar, (M)otorcycle, or (T)ruck,\n",
        "                'source_direction': Source direction of passing vehicle: from (L)eft or from (R)ight,\n",
        "                'microphone': (SE)= (high-quality) sE8 microphones, (ME) = (low-quality) MEMS microphones (ICS-43434),\n",
        "                'channel': Original stereo pair channel (12) or (34)\n",
        "    \"\"\"\n",
        "    # load file list\n",
        "    df_files = pd.read_csv(fn_txt, names=('file',))\n",
        "    fn_file_list = df_files['file'].to_list()\n",
        "\n",
        "    # load metadata from file names\n",
        "    df_dataset = []\n",
        "\n",
        "    for f, fn in enumerate(fn_file_list):\n",
        "        fn = fn.replace('.wav', '')\n",
        "        parts = fn.split('_')\n",
        "\n",
        "        # background noise files\n",
        "        if '-BG' in fn:\n",
        "            date_time, location, speed_kmh, sample_pos, mic, channel = parts\n",
        "            vehicle, source_direction, weather, daytime = 'None', 'None', 'None', 'None'\n",
        "            is_background = True\n",
        "\n",
        "        # files with vehicle passings\n",
        "        else:\n",
        "            date_time, location, speed_kmh, sample_pos, daytime, weather, vehicle_direction, mic, channel = parts\n",
        "            vehicle, source_direction = vehicle_direction\n",
        "            is_background = False\n",
        "\n",
        "        channel = channel.replace('-BG', '')\n",
        "        speed_kmh = speed_kmh.replace('unknownKmh', 'UNK')\n",
        "        speed_kmh = speed_kmh.replace('Kmh', '')\n",
        "\n",
        "        df_dataset.append({'file': fn,\n",
        "                           'is_background': is_background,\n",
        "                           'date_time': date_time,\n",
        "                           'location': location,\n",
        "                           'speed_kmh': speed_kmh,\n",
        "                           'sample_pos': sample_pos,\n",
        "                           'daytime': daytime,\n",
        "                           'weather': weather,\n",
        "                           'vehicle': vehicle,\n",
        "                           'source_direction': source_direction,\n",
        "                           'microphone': mic,\n",
        "                           'channel': channel})\n",
        "\n",
        "    df_dataset = pd.DataFrame(df_dataset, columns=('file', 'is_background', 'date_time', 'location', 'speed_kmh', 'sample_pos', 'daytime', 'weather', 'vehicle',\n",
        "                                                   'source_direction', 'microphone', 'channel'))\n",
        "\n",
        "    return df_dataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53-c6jdM6uLg"
      },
      "source": [
        "\"\"\"\n",
        "Anomalous Sound Transformer Model for my Bachelor Thesis\n",
        "\"\"\"\n",
        "\n",
        "__author__ = 'Johannes Eder (Jo.Eder@campus.lmu.de)'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAnBzNvl7V9Q",
        "outputId": "755a0c87-6f1a-4c18-b214-c45db00e001c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gArrnoTR7aiz"
      },
      "source": [
        "CLASSES = ['None','C','T', 'M', 'B'] #Background Noise, Car, Truck, Motorcycle, Bus\n",
        "NORMAL_CLASSES = ['None']\n",
        "ANOMALOUS_CLASSES = ['C','T', 'M', 'B']\n",
        "\n",
        "SAMPLE_RATE = 22500\n",
        "N_FFT=2048 #is also window size\n",
        "HOP_LENGTH=1024\n",
        "N_MELS=128\n",
        "NUMBER_OF_FRAMES = 4\n",
        "melspectogram = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=SAMPLE_RATE,\n",
        "        n_fft=N_FFT, # Frame Size\n",
        "        hop_length=HOP_LENGTH, #here half the frame size\n",
        "        n_mels=N_MELS\n",
        "    )\n",
        "\n",
        "transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(mode='L'),\n",
        "    #transforms.Grayscale(num_output_channels=3),\n",
        "    #transforms.Resize([224, 224]),\n",
        "    #transforms.RandomCrop(size=[N_MELS, NUMBER_OF_FRAMES]), #only train on random slice of the spectogram\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "AUDIO_DIR = \"/content/drive/My Drive/datasets/IDMT_Traffic/audio\"\n",
        "train_annotations = \"/content/drive/My Drive/datasets/IDMT_Traffic/annotation/eusipco_2021_train.csv\"\n",
        "test_annotatons = \"/content/drive/My Drive/datasets/IDMT_Traffic/annotation/eusipco_2021_test.csv\"\n",
        "all_annotations_txt = \"/content/drive/My Drive/datasets/IDMT_Traffic/annotation/idmt_traffic_all.txt\"\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "BATCH_SIZE_VAL = 1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjg39EMZ75O1"
      },
      "source": [
        "class IdmtTrafficDataSet(Dataset):\n",
        "\n",
        "    def __init__(self, annotations_file, audio_dir, audio_transformation, transformation, target_sample_rate, normal_classes):\n",
        "        self.annotations =  annotations_file if isinstance(annotations_file, pd.DataFrame) else pd.read_csv(annotations_file)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.audio_transformation = audio_transformation\n",
        "        self.transformation = transformation\n",
        "        self.target_sample_rate = target_sample_rate\n",
        "        #self.classes = ['None','C','T', 'M', 'B']\n",
        "        self.normal_classes = normal_classes\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        audio_sample_path = self._get_audio_sample_path(index)\n",
        "        label = self._get_audio_sample_label(index)\n",
        "        signal, sr = torchaudio.load(audio_sample_path)\n",
        "        signal = self._resample(signal, sr) #adjust sample rates\n",
        "        # signal -> (num_channels, samples) i.e. (2, 16000)\n",
        "        signal  = self._mix_down(signal) #stereo to mono\n",
        "        signal = self.audio_transformation(signal) #(1, 16000) -> torch.Size([1, 64, 63])\n",
        "        signal = self.transformation(signal)\n",
        "        #label = self.normal_classes.index(label)\n",
        "        label = 0 if label in self.normal_classes else 1 #1 for normal 0 for anomalous\n",
        "        return signal, label\n",
        "\n",
        "    def _resample(self, signal, sr):\n",
        "        if sr != self.target_sample_rate:\n",
        "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
        "            signal = resampler(signal)\n",
        "        return signal\n",
        "    \n",
        "    def _mix_down(self, signal):\n",
        "        if signal.shape[0] > 1: #(2, 16000)\n",
        "            #mean operation: aggregating multiple channels\n",
        "            signal = torch.mean(signal, 0, True)\n",
        "        return signal\n",
        "\n",
        "    def _get_audio_sample_path(self, index):\n",
        "        path = os.path.join(self.audio_dir, self.annotations.iloc[index, 0])\n",
        "        return path + '.wav'\n",
        "\n",
        "    def _get_audio_sample_label(self, index):\n",
        "        return self.annotations.iloc[index, 9]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf1bYw0c8Qe6"
      },
      "source": [
        "def get_train_and_val_idmt(audio_dir, train_annotations, test_annotations):\n",
        "    #this method is for the classifciation task\n",
        "\n",
        "    train_data = IdmtTrafficDataSet(train_annotations, audio_dir,melspectogram, transforms, SAMPLE_RATE)\n",
        "    test_data = IdmtTrafficDataSet(test_annotatons, audio_dir,melspectogram, transforms, SAMPLE_RATE)\n",
        "    return train_data, test_data\n",
        "\n",
        "def get_normal_and_anomalous_data(normal_classes, anomalous_classes, audio_dir, annotations, batch_size):\n",
        "    #this method is for the AD Task\n",
        "    if len((set(normal_classes) & set(anomalous_classes))) > 0:\n",
        "      raise Exception(\"Intersection between normal and anomalous classes should be empty!\")\n",
        "\n",
        "    all_data = import_idmt_traffic_dataset(annotations)\n",
        "\n",
        "    normal_data = all_data[all_data.vehicle.isin(normal_classes)]\n",
        "    anomalous_data = all_data[all_data.vehicle.isin(anomalous_classes)]\n",
        "\n",
        "    train_data = normal_data.iloc[:6000, :] #test data needs to have some amount of normal data as well\n",
        "    train_data = adjust_sample_number_to_batch_size(train_data, batch_size)\n",
        "\n",
        "    normal_test_data = normal_data.iloc[6001:6101, :] #later more all test samples need to be used, but for now it is faster\n",
        "    number_of_normal_test_sampels = len(normal_test_data)\n",
        "    print(f\"testing with {number_of_normal_test_sampels} normal samples\")\n",
        "\n",
        "    #sample same number of anomalous data to test\n",
        "    anomalous_data = anomalous_data.sample(number_of_normal_test_sampels)\n",
        "\n",
        "    frames = [anomalous_data, normal_test_data]\n",
        "    concatenated_test_data = pd.concat(frames)\n",
        "    concatenated_test_data.reset_index(drop=True, inplace=True)\n",
        "    concatenated_test_data = adjust_sample_number_to_batch_size(concatenated_test_data, batch_size)\n",
        "\n",
        "    normal_train_data = IdmtTrafficDataSet(train_data, audio_dir, melspectogram, transforms, SAMPLE_RATE, normal_classes)\n",
        "    test_data = IdmtTrafficDataSet(concatenated_test_data, audio_dir, melspectogram, transforms, SAMPLE_RATE, normal_classes)\n",
        "\n",
        "    return normal_train_data, test_data\n",
        "\n",
        "def adjust_sample_number_to_batch_size(data, batch_size):\n",
        "  if len(data) % batch_size == 0:\n",
        "    return data\n",
        "  else:\n",
        "    remainder = len(data) % batch_size\n",
        "    return data.iloc[remainder + 1:,:]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuGiGUSt8beP",
        "outputId": "7d344ead-bf9b-4b8b-cac1-5aa4157e81fa"
      },
      "source": [
        "train_data, test_data = get_normal_and_anomalous_data(NORMAL_CLASSES, ANOMALOUS_CLASSES, audio_dir=AUDIO_DIR, annotations=all_annotations_txt, batch_size=BATCH_SIZE)\n",
        "first_sample, first_label = train_data[0]\n",
        "input_dim = NUMBER_OF_FRAMES #first_sample.shape[1] * first_sample.shape[2]\n",
        "print(f\"Train Data Shape: {first_sample.shape}\") #Train Data Shape: torch.Size([1, 128, 44]), Frame Size 2: Train Data Shape: torch.Size([1, 128, 2])\n",
        "print(f\"Input dimension: {input_dim}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing with 100 normal samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwLjzSG0HkIC"
      },
      "source": [
        "class TransformerModel(nn.Module):\n",
        "  def __init__(self, d_model, input_dim, n_heads, dim_feedforward, n_encoder_layers, dropout=0.5):\n",
        "    super(TransformerModel, self).__init__()\n",
        "    self.model_type = 'Transformer'\n",
        "    self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "    encoder_layers = TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "    self.transformer_encoder = TransformerEncoder(encoder_layers, n_encoder_layers)\n",
        "    self.patch_embedding = PatchEmbedding(input_dim, d_model)\n",
        "    self.input_dim = input_dim\n",
        "    self.d_model = d_model\n",
        "    self.decoder = nn.Linear(in_features=d_model, out_features=input_dim)\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    initrange = 0.1\n",
        "    #self.patch_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "    self.decoder.bias.data.zero_()\n",
        "    self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "  \n",
        "  def forward(self, input):\n",
        "    embedded = self.patch_embedding(input)  * math.sqrt(self.d_model) #is scaling necessary? yes, otherwise values are incredibly small\n",
        "    pos_encoded_embedded = self.pos_encoder(embedded)\n",
        "    transformer_out = self.transformer_encoder(pos_encoded_embedded)\n",
        "    output = self.decoder(transformer_out)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be9tNwH7OyRC"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, embedding_dim, dropout=0.1, max_len=5000):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    pe = torch.zeros(max_len, embedding_dim)\n",
        "    #print(f\"Shape: {pe.shape}\")\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    #print(f\"Position shape: {position.shape}\")\n",
        "    div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "    self.register_buffer('pe', pe)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = x + self.pe[:x.size(0), :]\n",
        "    return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s468t9EyVgkm"
      },
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self, input_dim, embedding_dimension):\n",
        "    super().__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.embedding_layer = nn.Linear(input_dim, embedding_dimension)\n",
        "  \n",
        "  def forward(self, input_data):\n",
        "    embedding = self.embedding_layer(input_data)\n",
        "    return embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7MOkCsOWp7X"
      },
      "source": [
        "NUMBER_OF_FRAMES = 2\n",
        "EMBEDDING_SIZE = 512\n",
        "\"\"\"\n",
        "sample_input = torch.rand(8, 1, 128, 44) #batch_size, channels, mel_filters, frames\n",
        "unfold = nn.Unfold(kernel_size=(128, NUMBER_OF_FRAMES), stride=NUMBER_OF_FRAMES) #patching the spectogram\n",
        "unfolded = unfold(sample_input) #(batch_size, features, number_of_patches)\n",
        "unfolded = unfolded.transpose(1, 2) #(batch_size, number_of_patches, features)\n",
        "print(unfolded.shape)\n",
        "patch_embedding_module = PatchEmbedding(unfolded.shape[2], EMBEDDING_SIZE)\n",
        "embedded = patch_embedding_module(unfolded)\n",
        "print(embedded.shape) #(batch_size: 8, num_patches: 22, features: 512)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar2Mdd-ZpjuB"
      },
      "source": [
        "def patch_batch(input_batch, number_of_frames):\n",
        "  #input of shape (batch_size, channels, mel_filters, frames)\n",
        "  unfold = nn.Unfold(kernel_size=(input_batch.shape[2], NUMBER_OF_FRAMES), stride=NUMBER_OF_FRAMES) #patching the spectogram\n",
        "  unfolded_batch = unfold(sample_input) #(batch_size, features, number_of_patches)\n",
        "  unfolded_batch = unfolded_batch.transpose(1, 2) #(batch_size, number_of_patches, features)\n",
        "  return unfolded_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf-p_86McKfC"
      },
      "source": [
        "\"\"\"\n",
        "positional_encoder = PositionalEncoding(EMBEDDING_SIZE)\n",
        "pos_encoded = positional_encoder(embedded)\n",
        "print(pos_encoded.shape) #torch.Size([1024, 1024, 512]) pos_enc, ,embed_size\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D68e7futiiU"
      },
      "source": [
        "train_data, test_data = get_normal_and_anomalous_data(NORMAL_CLASSES, ANOMALOUS_CLASSES, audio_dir=AUDIO_DIR, annotations=all_annotations_txt, batch_size=BATCH_SIZE)\n",
        "first_sample, first_label = train_data[0]\n",
        "input_dim = first_sample.shape[2]\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "#test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79YOWc_meQ1R"
      },
      "source": [
        "N_HEADS = 2\n",
        "N_ENCODER_LAYERS = 2\n",
        "DROPOUT = 0.2\n",
        "DIM_FEED_FORWARD = 512\n",
        "input_dim = 256\n",
        "transformer = TransformerModel(EMBEDDING_SIZE, input_dim, N_HEADS, DIM_FEED_FORWARD, N_ENCODER_LAYERS, DROPOUT)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-svPSAyJpfnZ"
      },
      "source": [
        "\"\"\"\n",
        "pixelwise_loss = nn.functional.mse_loss(unfolded, output, size_average=None, reduce=None, reduction='none')\n",
        "print(loss.shape)\n",
        "#if second patch in every image in this batch was masked\n",
        "print(loss[:,2,:].sum())\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JahOBDTmNt4"
      },
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "def mask_input_batch(input):\n",
        "  number_of_specs = input.shape[0]\n",
        "  input_masked = []\n",
        "  masks_index_list = []\n",
        "  for i in range(number_of_specs):\n",
        "    mask_idx = random.randint(0, input.shape[1]-1)\n",
        "    mask = torch.ones(input.shape[1], input.shape[2])\n",
        "    mask[mask_idx, :] = 0\n",
        "    current_spec_masked = input[i, : , :].mul(mask)\n",
        "\n",
        "    input_masked.append(current_spec_masked) #maybe just tuples (current_spec_masked, mask_idx)\n",
        "    masks_index_list.append(torch.as_tensor(mask_idx))\n",
        "\n",
        "  return torch.stack(input_masked), torch.stack(masks_index_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwOvyi3HtL2T"
      },
      "source": [
        "\"\"\"\n",
        "sample_input = torch.rand(16, 22, 512) #batch_size, patches, embedded_features\n",
        "mask_idx = random.randint(0, 22)\n",
        "print(mask_idx)\n",
        "mask = torch.ones(22, 512)\n",
        "mask[mask_idx, :] = 0\n",
        "print(mask.shape)\n",
        "masked_sample = mask.mul(sample_input[1, :, :])\n",
        "print(masked_sample.shape)\n",
        "#print(masked_sample[mask_idx, :])\n",
        "#print(masked_sample[mask_idx - 1, :])\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc0nRoSDyfU1"
      },
      "source": [
        "\"\"\"\n",
        "sample_input = torch.rand(16, 22, 512) #batch_size, patches, embedded_features\n",
        "sample_output = torch.rand(16, 22, 512)\n",
        "masked_input, mask_idxs = mask_input_batch(sample_input)\n",
        "print(masked_input.shape)\n",
        "print(mask_idxs.shape)\n",
        "print(masked_input[0, mask_idxs[0],:]) #masked\n",
        "print(masked_input[15, mask_idxs[14],:]) #not masked\n",
        "pixelwise_loss = nn.functional.mse_loss(sample_input, sample_output, size_average=None, reduce=None, reduction='none')\n",
        "print(pixelwise_loss.shape)\n",
        "loss_per_batch = 0\n",
        "for i in range(len(mask_idxs)):\n",
        "  loss_at_masked = pixelwise_loss[i, mask_idxs[i], :].sum()\n",
        "  loss_per_batch +=loss_at_masked\n",
        "\n",
        "print(loss_per_batch)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "057KTyNylH8s"
      },
      "source": [
        "def calculate_loss_masked(input, output, mask_idxs):\n",
        "  pixelwise_loss = nn.functional.mse_loss(input, output, size_average=None, reduce=None, reduction='none')\n",
        "  print(pixelwise_loss.shape)\n",
        "  loss_per_batch = 0\n",
        "  for i in range(len(mask_idxs)):\n",
        "    loss_at_masked = pixelwise_loss[i, mask_idxs[i], :].sum()\n",
        "    loss_per_batch +=loss_at_masked\n",
        "\n",
        "  return loss_per_batch\n",
        "\n",
        "def calculate_loss_total(input, output, mask_idxs):\n",
        "  raise Exception(\"Not implemented\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC49Nt07fofv"
      },
      "source": [
        "lr = 5.0 # learning rate, gets adapted\n",
        "optimizer = torch.optim.SGD(transformer.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "EPOCHS = 1\n",
        "def train(model, learning_rate, optimizer, scheduler, epoch):\n",
        "  print(f\"Starting Epoch {epoch}\")\n",
        "  for batch_index, (data_batch, _) in enumerate(train_loader):\n",
        "    print(data_batch.shape)\n",
        "    data_batch = patch_batch(data_batch, NUMBER_OF_FRAMES)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    masked_input, mask_idxs = mask_input_batch(data_batch)\n",
        "    output = model(masked_input)\n",
        "    loss = calculate_loss_masked(data_batch, output, mask_idxs)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optimizer.step()\n",
        "\n",
        "    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR: {}'.format(epoch, batch_index * len(data_batch), len(train_loader.dataset),100. * batch_index / len(train_loader), loss.item(), lr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_lDE1p4m1h2"
      },
      "source": [
        "transformer.train() #mode\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  train(transformer, lr, optimizer, scheduler, epoch)\n",
        "  scheduler.step()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}