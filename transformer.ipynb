{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMKxW3hvvvqDV+HszCgMgG9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohEder/bachelor_thesis_audio_ml/blob/master/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIBl-ybmvCJ7",
        "outputId": "dad101f4-feb9-4b68-f036-bb6ad928727d"
      },
      "source": [
        "!pip install torchaudio\n",
        "!pip install pytorch-model-summary"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchaudio) (3.7.4.3)\n",
            "Requirement already satisfied: pytorch-model-summary in /usr/local/lib/python3.7/dist-packages (0.1.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch-model-summary) (1.9.0+cu102)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-model-summary) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-model-summary) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-model-summary) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdn43FFt6jp0"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import torchaudio\n",
        "import pandas as pd\n",
        "import os\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm9hEziO6l5F"
      },
      "source": [
        "#just copied the official import script for the dataset, custom preprocessing happens afterwards\n",
        "\"\"\" Import script for IDMT-Traffic dataset\n",
        "Ref:\n",
        "    J. Abeßer, S. Gourishetti, A. Kátai, T. Clauß, P. Sharma, J. Liebetrau: IDMT-Traffic: An Open Benchmark\n",
        "    Dataset for Acoustic Traffic Monitoring Research, EUSIPCO, 2021\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "__author__ = 'Jakob Abeßer (jakob.abesser@idmt.fraunhofer.de)'\n",
        "\n",
        "\n",
        "def import_idmt_traffic_dataset(fn_txt: str = \"idmt_traffic_all\") -> pd.DataFrame:\n",
        "    \"\"\" Import IDMT-Traffic dataset\n",
        "    Args:\n",
        "        fn_txt (str): Text file with all WAV files\n",
        "    Returns:\n",
        "        df_dataset (pd.Dataframe): File-wise metadata\n",
        "            Columns:\n",
        "                'file': WAV filename,\n",
        "                'is_background': True if recording contains background noise (no vehicle), False else\n",
        "                'date_time': Recording time (YYYY-MM-DD-HH-mm)\n",
        "                'location': Recording location\n",
        "                'speed_kmh': Speed limit at recording site (km/h), UNK if unknown,\n",
        "                'sample_pos': Sample position (centered) within the original audio recording,\n",
        "                'daytime': M(orning) or (A)fternoon,\n",
        "                'weather': (D)ry or (W)et road condition,\n",
        "                'vehicle': (B)us, (C)ar, (M)otorcycle, or (T)ruck,\n",
        "                'source_direction': Source direction of passing vehicle: from (L)eft or from (R)ight,\n",
        "                'microphone': (SE)= (high-quality) sE8 microphones, (ME) = (low-quality) MEMS microphones (ICS-43434),\n",
        "                'channel': Original stereo pair channel (12) or (34)\n",
        "    \"\"\"\n",
        "    # load file list\n",
        "    df_files = pd.read_csv(fn_txt, names=('file',))\n",
        "    fn_file_list = df_files['file'].to_list()\n",
        "\n",
        "    # load metadata from file names\n",
        "    df_dataset = []\n",
        "\n",
        "    for f, fn in enumerate(fn_file_list):\n",
        "        fn = fn.replace('.wav', '')\n",
        "        parts = fn.split('_')\n",
        "\n",
        "        # background noise files\n",
        "        if '-BG' in fn:\n",
        "            date_time, location, speed_kmh, sample_pos, mic, channel = parts\n",
        "            vehicle, source_direction, weather, daytime = 'None', 'None', 'None', 'None'\n",
        "            is_background = True\n",
        "\n",
        "        # files with vehicle passings\n",
        "        else:\n",
        "            date_time, location, speed_kmh, sample_pos, daytime, weather, vehicle_direction, mic, channel = parts\n",
        "            vehicle, source_direction = vehicle_direction\n",
        "            is_background = False\n",
        "\n",
        "        channel = channel.replace('-BG', '')\n",
        "        speed_kmh = speed_kmh.replace('unknownKmh', 'UNK')\n",
        "        speed_kmh = speed_kmh.replace('Kmh', '')\n",
        "\n",
        "        df_dataset.append({'file': fn,\n",
        "                           'is_background': is_background,\n",
        "                           'date_time': date_time,\n",
        "                           'location': location,\n",
        "                           'speed_kmh': speed_kmh,\n",
        "                           'sample_pos': sample_pos,\n",
        "                           'daytime': daytime,\n",
        "                           'weather': weather,\n",
        "                           'vehicle': vehicle,\n",
        "                           'source_direction': source_direction,\n",
        "                           'microphone': mic,\n",
        "                           'channel': channel})\n",
        "\n",
        "    df_dataset = pd.DataFrame(df_dataset, columns=('file', 'is_background', 'date_time', 'location', 'speed_kmh', 'sample_pos', 'daytime', 'weather', 'vehicle',\n",
        "                                                   'source_direction', 'microphone', 'channel'))\n",
        "\n",
        "    return df_dataset"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53-c6jdM6uLg"
      },
      "source": [
        "\"\"\"\n",
        "Anomalous Sound Transformer Model for my Bachelor Thesis\n",
        "\"\"\"\n",
        "\n",
        "__author__ = 'Johannes Eder (Jo.Eder@campus.lmu.de)'\n",
        "\n",
        "#print(len(all_data[all_data.is_background])) #8144 -> #9362 labbelled background sounds\n",
        "#print(len(all_data[all_data.vehicle == 'C'])) #7804\n",
        "#print(len(all_data[all_data.vehicle == 'M'])) #430\n",
        "#print(len(all_data[all_data.vehicle == 'T'])) #1022\n",
        "#print(len(all_data[all_data.vehicle == 'B'])) #106"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAnBzNvl7V9Q",
        "outputId": "052d8546-6b3f-4334-f75e-b98165a49c48"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gArrnoTR7aiz"
      },
      "source": [
        "CLASSES = ['None','C','T', 'M', 'B'] #Background Noise, Car, Truck, Motorcycle, Bus\n",
        "NORMAL_CLASSES = ['None']\n",
        "ANOMALOUS_CLASSES = ['C','T', 'M', 'B']\n",
        "\n",
        "SAMPLE_RATE = 22500\n",
        "N_FFT=2048 #is also window size\n",
        "HOP_LENGTH=1024\n",
        "N_MELS=128\n",
        "NUMBER_OF_FRAMES = 4\n",
        "melspectogram = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=SAMPLE_RATE,\n",
        "        n_fft=N_FFT, # Frame Size\n",
        "        hop_length=HOP_LENGTH, #here half the frame size\n",
        "        n_mels=N_MELS\n",
        "    )\n",
        "\n",
        "transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(mode='L'),\n",
        "    #transforms.Grayscale(num_output_channels=3),\n",
        "    #transforms.Resize([224, 224]),\n",
        "    #transforms.RandomCrop(size=[N_MELS, NUMBER_OF_FRAMES]), #only train on random slice of the spectogram\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "AUDIO_DIR = \"/content/drive/My Drive/datasets/IDMT_Traffic/audio\"\n",
        "train_annotations = \"/content/drive/My Drive/datasets/IDMT_Traffic/annotation/eusipco_2021_train.csv\"\n",
        "test_annotatons = \"/content/drive/My Drive/datasets/IDMT_Traffic/annotation/eusipco_2021_test.csv\"\n",
        "all_annotations_txt = \"/content/drive/My Drive/datasets/IDMT_Traffic/annotation/idmt_traffic_all.txt\"\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "BATCH_SIZE_VAL = 1\n",
        "\n",
        "\n",
        "NUMBER_OF_FRAMES = 2\n",
        "EMBEDDING_SIZE = 512\n",
        "N_HEADS = 2\n",
        "N_ENCODER_LAYERS = 2\n",
        "DROPOUT = 0.2\n",
        "DIM_FEED_FORWARD = 512\n",
        "input_dim = 256\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjg39EMZ75O1"
      },
      "source": [
        "class IdmtTrafficDataSet(Dataset):\n",
        "\n",
        "    def __init__(self, annotations_file, audio_dir, audio_transformation, transformation, target_sample_rate, normal_classes):\n",
        "        self.annotations =  annotations_file if isinstance(annotations_file, pd.DataFrame) else pd.read_csv(annotations_file)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.audio_transformation = audio_transformation\n",
        "        self.transformation = transformation\n",
        "        self.target_sample_rate = target_sample_rate\n",
        "        #self.classes = ['None','C','T', 'M', 'B']\n",
        "        self.normal_classes = normal_classes\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        audio_sample_path = self._get_audio_sample_path(index)\n",
        "        label = self._get_audio_sample_label(index)\n",
        "        signal, sr = torchaudio.load(audio_sample_path)\n",
        "        signal = self._resample(signal, sr) #adjust sample rates\n",
        "        # signal -> (num_channels, samples) i.e. (2, 16000)\n",
        "        signal  = self._mix_down(signal) #stereo to mono\n",
        "        signal = self.audio_transformation(signal) #(1, 16000) -> torch.Size([1, 64, 63])\n",
        "        signal = self.transformation(signal)\n",
        "        #label = self.normal_classes.index(label)\n",
        "        label = 0 if label in self.normal_classes else 1\n",
        "        return signal, label\n",
        "\n",
        "    def _resample(self, signal, sr):\n",
        "        if sr != self.target_sample_rate:\n",
        "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
        "            signal = resampler(signal)\n",
        "        return signal\n",
        "    \n",
        "    def _mix_down(self, signal):\n",
        "        if signal.shape[0] > 1: #(2, 16000)\n",
        "            #mean operation: aggregating multiple channels\n",
        "            signal = torch.mean(signal, 0, True)\n",
        "        return signal\n",
        "\n",
        "    def _get_audio_sample_path(self, index):\n",
        "        path = os.path.join(self.audio_dir, self.annotations.iloc[index, 0])\n",
        "        return path + '.wav'\n",
        "\n",
        "    def _get_audio_sample_label(self, index):\n",
        "        return self.annotations.iloc[index, 9]"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf1bYw0c8Qe6"
      },
      "source": [
        "def get_normal_and_anomalous_data(normal_classes, anomalous_classes, audio_dir, annotations, batch_size):\n",
        "    if len((set(normal_classes) & set(anomalous_classes))) > 0:\n",
        "      raise Exception(\"Intersection between normal and anomalous classes should be empty!\")\n",
        "\n",
        "    all_data = import_idmt_traffic_dataset(annotations)\n",
        "\n",
        "    normal_data = all_data[all_data.vehicle.isin(normal_classes)]\n",
        "    anomalous_data = all_data[all_data.vehicle.isin(anomalous_classes)]\n",
        "\n",
        "    train_data = normal_data.iloc[:5000, :] #test data needs to have some amount of normal data as well\n",
        "    train_data = adjust_sample_number_to_batch_size(train_data, batch_size)\n",
        "\n",
        "    normal_test_data = normal_data.iloc[5001:5101, :] #later more all test samples need to be used, but for now it is faster\n",
        "    number_of_normal_test_sampels = len(normal_test_data)\n",
        "    print(f\"testing with {number_of_normal_test_sampels} normal samples\")\n",
        "\n",
        "    #sample same number of anomalous data to test\n",
        "    anomalous_data = anomalous_data.sample(number_of_normal_test_sampels)\n",
        "    print(f\"testing with {len(anomalous_data)} anomalous samples\")\n",
        "\n",
        "    frames = [anomalous_data, normal_test_data]\n",
        "    concatenated_test_data = pd.concat(frames)\n",
        "    concatenated_test_data.reset_index(drop=True, inplace=True)\n",
        "    concatenated_test_data = adjust_sample_number_to_batch_size(concatenated_test_data, batch_size)\n",
        "\n",
        "    normal_train_data = IdmtTrafficDataSet(train_data, audio_dir, melspectogram, transforms, SAMPLE_RATE, normal_classes)\n",
        "    test_data = IdmtTrafficDataSet(concatenated_test_data, audio_dir, melspectogram, transforms, SAMPLE_RATE, normal_classes)\n",
        "\n",
        "    return normal_train_data, test_data\n",
        "\n",
        "def adjust_sample_number_to_batch_size(data, batch_size):\n",
        "  if len(data) % batch_size == 0:\n",
        "    print(\"no data discarded.\")\n",
        "    return data\n",
        "  else:\n",
        "    remainder = len(data) % batch_size\n",
        "    print(str(remainder + 1) + \" samples discarded.\")\n",
        "    return data.iloc[remainder + 1:,:]"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuGiGUSt8beP"
      },
      "source": [
        "#train_data, test_data = get_normal_and_anomalous_data(NORMAL_CLASSES, ANOMALOUS_CLASSES, audio_dir=AUDIO_DIR, annotations=all_annotations_txt, batch_size=BATCH_SIZE)\n",
        "#first_sample, first_label = train_data[0]\n",
        "#print(f\"Train Data Shape: {first_sample.shape}\") #Train Data Shape: torch.Size([1, 128, 44]), Frame Size 2: Train Data Shape: torch.Size([1, 128, 2])\n",
        "#\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwLjzSG0HkIC"
      },
      "source": [
        "class TransformerModel(nn.Module):\n",
        "  def __init__(self, d_model, input_dim, n_heads, dim_feedforward, n_encoder_layers, dropout=0.5):\n",
        "    super(TransformerModel, self).__init__()\n",
        "    self.model_type = 'Transformer'\n",
        "    self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "    encoder_layers = TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "    self.transformer_encoder = TransformerEncoder(encoder_layers, n_encoder_layers)\n",
        "    self.patch_embedding = PatchEmbedding(input_dim, d_model)\n",
        "    self.input_dim = input_dim\n",
        "    self.d_model = d_model\n",
        "    self.decoder = nn.Linear(in_features=d_model, out_features=input_dim)\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    initrange = 0.1\n",
        "    #self.patch_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "    self.decoder.bias.data.zero_()\n",
        "    self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "  \n",
        "  def forward(self, input):\n",
        "    embedded = self.patch_embedding(input) * math.sqrt(self.input_dim) #is scaling necessary? yes, otherwise values are incredibly small\n",
        "    pos_encoded_embedded = self.pos_encoder(embedded)\n",
        "    transformer_out = self.transformer_encoder(pos_encoded_embedded)\n",
        "    output = self.decoder(transformer_out)\n",
        "    return output"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be9tNwH7OyRC"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, embedding_dim, dropout=0.1, max_len=5000):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    pe = torch.zeros(max_len, embedding_dim)\n",
        "    #print(f\"Shape: {pe.shape}\")\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    #print(f\"Position shape: {position.shape}\")\n",
        "    div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "    self.register_buffer('pe', pe)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = x + self.pe[:x.size(0), :]\n",
        "    return self.dropout(x)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s468t9EyVgkm"
      },
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self, input_dim, embedding_dimension):\n",
        "    super().__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.embedding_layer = nn.Linear(input_dim, embedding_dimension)\n",
        "  \n",
        "  def forward(self, input_data):\n",
        "    embedding = self.embedding_layer(input_data)\n",
        "    return embedding"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar2Mdd-ZpjuB"
      },
      "source": [
        "def patch_batch(input_batch, number_of_frames):\n",
        "  #input of shape (batch_size, channels, mel_filters, frames)\n",
        "  unfold = nn.Unfold(kernel_size=(input_batch.shape[2], NUMBER_OF_FRAMES), stride=NUMBER_OF_FRAMES) #patching the spectogram\n",
        "  unfolded_batch = unfold(input_batch) #(batch_size, features, number_of_patches)\n",
        "  unfolded_batch = unfolded_batch.transpose(1, 2) #(batch_size, number_of_patches, features)\n",
        "  return unfolded_batch"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D68e7futiiU",
        "outputId": "0ed34aad-06da-4767-b5c4-d07976e9287a"
      },
      "source": [
        "train_data, test_data = get_normal_and_anomalous_data(NORMAL_CLASSES, ANOMALOUS_CLASSES, audio_dir=AUDIO_DIR, annotations=all_annotations_txt, batch_size=BATCH_SIZE)\n",
        "first_sample, first_label = train_data[0]\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE_VAL, shuffle=True)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9 samples discarded.\n",
            "testing with 100 normal samples\n",
            "testing with 100 anomalous samples\n",
            "9 samples discarded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf-uvDDYwokC",
        "outputId": "d12f76b3-d409-4fd2-fcdb-84d7a201a742"
      },
      "source": [
        "for data in test_loader:\n",
        "  sample, target = data\n",
        "  if target == 0:\n",
        "    print(\"Target 0\")\n",
        "    break;"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79YOWc_meQ1R"
      },
      "source": [
        "transformer = TransformerModel(EMBEDDING_SIZE, input_dim, N_HEADS, DIM_FEED_FORWARD, N_ENCODER_LAYERS, DROPOUT)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JahOBDTmNt4"
      },
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "def mask_input_batch(input, device, specific_mask_idx=None):\n",
        "  if specific_mask_idx != None:\n",
        "    assert specific_mask_idx < input.shape[1]\n",
        "  number_of_specs = input.shape[0]\n",
        "  input_masked = []\n",
        "  masks_index_list = []\n",
        "  for i in range(number_of_specs):\n",
        "    mask_idx = specific_mask_idx if specific_mask_idx != None else random.randint(0, input.shape[1]-1)\n",
        "    mask = torch.ones(input.shape[1], input.shape[2])\n",
        "    mask[mask_idx, :] = 0\n",
        "    current_spec_masked = input[i, : , :].mul(mask)\n",
        "\n",
        "    input_masked.append(current_spec_masked) #maybe just tuples (current_spec_masked, mask_idx)\n",
        "    masks_index_list.append(torch.as_tensor(mask_idx))\n",
        "\n",
        "  return torch.stack(input_masked), torch.stack(masks_index_list)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsvxmTd6LS-3",
        "outputId": "3287163f-9904-49e5-9da2-aec0daa71089"
      },
      "source": [
        "x = torch.rand(2, 1)\n",
        "y = torch.zeros(2, 1)\n",
        "a = []\n",
        "a.append(x)\n",
        "a.append(y)\n",
        "z = torch.stack(a)\n",
        "print(z)\n",
        "print(z.shape)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0.3834],\n",
            "         [0.7759]],\n",
            "\n",
            "        [[0.0000],\n",
            "         [0.0000]]])\n",
            "torch.Size([2, 2, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "057KTyNylH8s"
      },
      "source": [
        "def calculate_loss_masked(input_batch, output_batch, mask_idxs, sum_up):\n",
        "  #print(input_batch.shape)\n",
        "  #print(pixelwise_loss.shape) #batch_size, patches, features\n",
        "  loss_func = nn.MSELoss();\n",
        "  loss_per_batch = 0\n",
        "  for i in range(len(mask_idxs)):\n",
        "    input_at_masked = input_batch[i, mask_idxs[i], :]\n",
        "    output_at_masked = output_batch[i, mask_idxs[i], :]\n",
        "    loss = loss_func(input_at_masked, output_at_masked)\n",
        "    #print(loss)\n",
        "    loss_per_batch += loss\n",
        "  return loss_per_batch\n",
        "\n",
        "\n",
        "def calculate_loss_total(input, output):\n",
        "  loss = nn.MSELoss();\n",
        "  return loss(input, output)"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsXP-g_RimLh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b7eb828-2dd6-43ff-c456-0f06a3731771"
      },
      "source": [
        "#tryout\n",
        "input_sample = torch.rand(1, 1, 128, 44) #one sample\n",
        "data_batch = patch_batch(input_sample, NUMBER_OF_FRAMES)\n",
        "print(data_batch.shape) #torch.Size([16, 22, 256])\n",
        "#every patch needs to be masked once and the masked loss calculated added and divided by number of patches\n",
        "loss_total = 0\n",
        "for i in range(data_batch.shape[1]):\n",
        "  masked_input, mask_idxs = mask_input_batch(data_batch, i)\n",
        "  output = transformer(masked_input)\n",
        "  loss = calculate_loss_masked(data_batch, output, mask_idxs, True) # last argument (sum) does not make a difference for batch size 1\n",
        "  loss_total +=loss\n",
        "#loss_total /= data_batch.shape[1] #divide by number of patches\n",
        "print(loss_total)\n",
        "\n",
        "#masked_input, mask_idxs = mask_input_batch(data_batch, 2)\n",
        "#print(masked_input)\n",
        "#output = transformer(data_batch)\n",
        "#print(output)\n",
        "#loss = calculate_loss_masked(data_batch, output, mask_idxs, False)\n",
        "#print(len(loss))\n",
        "#print(loss)\n",
        "#loss_mse_total = calculate_loss_total(data_batch, output)\n",
        "#print(loss_mse_total)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 22, 256])\n",
            "tensor(44.1016, grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC49Nt07fofv"
      },
      "source": [
        "LEARNING_RATE = 0.01 # learning rate, gets adapted\n",
        "optimizer = torch.optim.SGD(transformer.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "EPOCHS = 1\n",
        "def train(model, learning_rate, optimizer, scheduler, epoch):\n",
        "  print(f\"Starting Epoch {epoch}\")\n",
        "  for batch_index, (data_batch, _) in enumerate(train_loader):\n",
        "    #print(data_batch.shape)\n",
        "    data_batch = patch_batch(data_batch, NUMBER_OF_FRAMES)\n",
        "\n",
        "    masked_input, mask_idxs = mask_input_batch(data_batch)\n",
        "    print(masked_input.shape)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(masked_input)\n",
        "    loss = calculate_loss_masked(data_batch, output, mask_idxs, True)\n",
        "    #loss_total = calculate_loss_total(data_batch, output)\n",
        "    #print(f\"Loss patches: {loss}\\nLoss total: {loss_total}\")\n",
        "    loss.backward()\n",
        "    #torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optimizer.step()\n",
        "\n",
        "    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR: {}'.format(epoch, batch_index * len(data_batch), len(train_loader.dataset),100. * batch_index / len(train_loader), loss.item(), LEARNING_RATE))"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_lDE1p4m1h2",
        "outputId": "2b7024ac-aa56-4318-e461-f294ed61e7c6"
      },
      "source": [
        "transformer.to(device)\n",
        "transformer.train() #mode\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  train(transformer, LEARNING_RATE, optimizer, scheduler, epoch)\n",
        "  scheduler.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Epoch 1\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.880032\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [16/6000 (0%)]\tLoss: 0.842771\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [32/6000 (1%)]\tLoss: 0.785725\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [48/6000 (1%)]\tLoss: 0.695221\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [64/6000 (1%)]\tLoss: 0.676021\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [80/6000 (1%)]\tLoss: 0.621091\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [96/6000 (2%)]\tLoss: 0.588331\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [112/6000 (2%)]\tLoss: 0.531428\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [128/6000 (2%)]\tLoss: 0.513822\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [144/6000 (2%)]\tLoss: 0.497068\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [160/6000 (3%)]\tLoss: 0.449297\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [176/6000 (3%)]\tLoss: 0.467680\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [192/6000 (3%)]\tLoss: 0.469969\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [208/6000 (3%)]\tLoss: 0.429601\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [224/6000 (4%)]\tLoss: 0.434441\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [240/6000 (4%)]\tLoss: 0.414932\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [256/6000 (4%)]\tLoss: 0.398541\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [272/6000 (5%)]\tLoss: 0.364549\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [288/6000 (5%)]\tLoss: 0.394019\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [304/6000 (5%)]\tLoss: 0.372575\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [320/6000 (5%)]\tLoss: 0.385531\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [336/6000 (6%)]\tLoss: 0.328644\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [352/6000 (6%)]\tLoss: 0.347195\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [368/6000 (6%)]\tLoss: 0.326092\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [384/6000 (6%)]\tLoss: 0.323125\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [400/6000 (7%)]\tLoss: 0.325903\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [416/6000 (7%)]\tLoss: 0.323143\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [432/6000 (7%)]\tLoss: 0.321912\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [448/6000 (7%)]\tLoss: 0.325484\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [464/6000 (8%)]\tLoss: 0.304623\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [480/6000 (8%)]\tLoss: 0.306801\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [496/6000 (8%)]\tLoss: 0.296788\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [512/6000 (9%)]\tLoss: 0.298523\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [528/6000 (9%)]\tLoss: 0.296195\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [544/6000 (9%)]\tLoss: 0.286646\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [560/6000 (9%)]\tLoss: 0.284687\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [576/6000 (10%)]\tLoss: 0.277592\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [592/6000 (10%)]\tLoss: 0.265818\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [608/6000 (10%)]\tLoss: 0.280378\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [624/6000 (10%)]\tLoss: 0.269233\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [640/6000 (11%)]\tLoss: 0.254182\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [656/6000 (11%)]\tLoss: 0.272365\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [672/6000 (11%)]\tLoss: 0.275730\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [688/6000 (11%)]\tLoss: 0.259453\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [704/6000 (12%)]\tLoss: 0.267205\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [720/6000 (12%)]\tLoss: 0.246456\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [736/6000 (12%)]\tLoss: 0.230952\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [752/6000 (13%)]\tLoss: 0.254397\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [768/6000 (13%)]\tLoss: 0.267612\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [784/6000 (13%)]\tLoss: 0.222183\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [800/6000 (13%)]\tLoss: 0.241864\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [816/6000 (14%)]\tLoss: 0.243867\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [832/6000 (14%)]\tLoss: 0.226859\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [848/6000 (14%)]\tLoss: 0.224885\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [864/6000 (14%)]\tLoss: 0.229492\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [880/6000 (15%)]\tLoss: 0.236729\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [896/6000 (15%)]\tLoss: 0.242513\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [912/6000 (15%)]\tLoss: 0.218390\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [928/6000 (15%)]\tLoss: 0.212215\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [944/6000 (16%)]\tLoss: 0.244434\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [960/6000 (16%)]\tLoss: 0.217410\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [976/6000 (16%)]\tLoss: 0.210455\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [992/6000 (17%)]\tLoss: 0.215655\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1008/6000 (17%)]\tLoss: 0.221889\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1024/6000 (17%)]\tLoss: 0.207542\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1040/6000 (17%)]\tLoss: 0.221848\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1056/6000 (18%)]\tLoss: 0.219657\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1072/6000 (18%)]\tLoss: 0.216073\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1088/6000 (18%)]\tLoss: 0.217916\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1104/6000 (18%)]\tLoss: 0.205670\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1120/6000 (19%)]\tLoss: 0.195332\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1136/6000 (19%)]\tLoss: 0.202005\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1152/6000 (19%)]\tLoss: 0.213169\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1168/6000 (19%)]\tLoss: 0.234732\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1184/6000 (20%)]\tLoss: 0.197404\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1200/6000 (20%)]\tLoss: 0.200379\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1216/6000 (20%)]\tLoss: 0.200876\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1232/6000 (21%)]\tLoss: 0.199380\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1248/6000 (21%)]\tLoss: 0.193508\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1264/6000 (21%)]\tLoss: 0.198738\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1280/6000 (21%)]\tLoss: 0.191695\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1296/6000 (22%)]\tLoss: 0.190286\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1312/6000 (22%)]\tLoss: 0.197905\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1328/6000 (22%)]\tLoss: 0.189201\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1344/6000 (22%)]\tLoss: 0.180915\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1360/6000 (23%)]\tLoss: 0.198357\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1376/6000 (23%)]\tLoss: 0.207393\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1392/6000 (23%)]\tLoss: 0.183616\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1408/6000 (23%)]\tLoss: 0.192237\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1424/6000 (24%)]\tLoss: 0.184525\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1440/6000 (24%)]\tLoss: 0.184558\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1456/6000 (24%)]\tLoss: 0.194545\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1472/6000 (25%)]\tLoss: 0.196141\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1488/6000 (25%)]\tLoss: 0.179187\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1504/6000 (25%)]\tLoss: 0.180665\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1520/6000 (25%)]\tLoss: 0.180918\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1536/6000 (26%)]\tLoss: 0.183554\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1552/6000 (26%)]\tLoss: 0.171068\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1568/6000 (26%)]\tLoss: 0.193243\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1584/6000 (26%)]\tLoss: 0.187033\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1600/6000 (27%)]\tLoss: 0.182226\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1616/6000 (27%)]\tLoss: 0.183586\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1632/6000 (27%)]\tLoss: 0.195411\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1648/6000 (27%)]\tLoss: 0.185773\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1664/6000 (28%)]\tLoss: 0.173131\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1680/6000 (28%)]\tLoss: 0.182747\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1696/6000 (28%)]\tLoss: 0.170622\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1712/6000 (29%)]\tLoss: 0.167234\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1728/6000 (29%)]\tLoss: 0.181304\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1744/6000 (29%)]\tLoss: 0.170704\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1760/6000 (29%)]\tLoss: 0.174501\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1776/6000 (30%)]\tLoss: 0.174406\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1792/6000 (30%)]\tLoss: 0.178007\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1808/6000 (30%)]\tLoss: 0.164070\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1824/6000 (30%)]\tLoss: 0.177036\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1840/6000 (31%)]\tLoss: 0.176578\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1856/6000 (31%)]\tLoss: 0.176568\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1872/6000 (31%)]\tLoss: 0.176305\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1888/6000 (31%)]\tLoss: 0.171304\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1904/6000 (32%)]\tLoss: 0.176597\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1920/6000 (32%)]\tLoss: 0.175122\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1936/6000 (32%)]\tLoss: 0.160137\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1952/6000 (33%)]\tLoss: 0.180692\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1968/6000 (33%)]\tLoss: 0.173132\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1984/6000 (33%)]\tLoss: 0.168545\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2000/6000 (33%)]\tLoss: 0.158851\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2016/6000 (34%)]\tLoss: 0.167267\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2032/6000 (34%)]\tLoss: 0.167771\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2048/6000 (34%)]\tLoss: 0.172593\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2064/6000 (34%)]\tLoss: 0.173923\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2080/6000 (35%)]\tLoss: 0.165250\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2096/6000 (35%)]\tLoss: 0.176993\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2112/6000 (35%)]\tLoss: 0.165107\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2128/6000 (35%)]\tLoss: 0.169817\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2144/6000 (36%)]\tLoss: 0.174444\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2160/6000 (36%)]\tLoss: 0.150688\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2176/6000 (36%)]\tLoss: 0.159599\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2192/6000 (37%)]\tLoss: 0.154685\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2208/6000 (37%)]\tLoss: 0.169003\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2224/6000 (37%)]\tLoss: 0.167280\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2240/6000 (37%)]\tLoss: 0.157414\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2256/6000 (38%)]\tLoss: 0.154370\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2272/6000 (38%)]\tLoss: 0.159421\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2288/6000 (38%)]\tLoss: 0.160766\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2304/6000 (38%)]\tLoss: 0.152099\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2320/6000 (39%)]\tLoss: 0.158466\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2336/6000 (39%)]\tLoss: 0.163098\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2352/6000 (39%)]\tLoss: 0.158599\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2368/6000 (39%)]\tLoss: 0.151692\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2384/6000 (40%)]\tLoss: 0.155158\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2400/6000 (40%)]\tLoss: 0.151900\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2416/6000 (40%)]\tLoss: 0.147706\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2432/6000 (41%)]\tLoss: 0.156258\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2448/6000 (41%)]\tLoss: 0.151401\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2464/6000 (41%)]\tLoss: 0.165627\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2480/6000 (41%)]\tLoss: 0.152726\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2496/6000 (42%)]\tLoss: 0.146191\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2512/6000 (42%)]\tLoss: 0.151432\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2528/6000 (42%)]\tLoss: 0.146418\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2544/6000 (42%)]\tLoss: 0.164143\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2560/6000 (43%)]\tLoss: 0.149519\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2576/6000 (43%)]\tLoss: 0.164298\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2592/6000 (43%)]\tLoss: 0.151729\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2608/6000 (43%)]\tLoss: 0.149655\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2624/6000 (44%)]\tLoss: 0.149948\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2640/6000 (44%)]\tLoss: 0.157990\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2656/6000 (44%)]\tLoss: 0.147888\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2672/6000 (45%)]\tLoss: 0.161980\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2688/6000 (45%)]\tLoss: 0.159662\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2704/6000 (45%)]\tLoss: 0.155264\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2720/6000 (45%)]\tLoss: 0.149197\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2736/6000 (46%)]\tLoss: 0.149534\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2752/6000 (46%)]\tLoss: 0.161868\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2768/6000 (46%)]\tLoss: 0.149413\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2784/6000 (46%)]\tLoss: 0.149439\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2800/6000 (47%)]\tLoss: 0.154172\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2816/6000 (47%)]\tLoss: 0.149109\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2832/6000 (47%)]\tLoss: 0.156102\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2848/6000 (47%)]\tLoss: 0.148991\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2864/6000 (48%)]\tLoss: 0.149081\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2880/6000 (48%)]\tLoss: 0.166155\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2896/6000 (48%)]\tLoss: 0.145862\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2912/6000 (49%)]\tLoss: 0.139125\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2928/6000 (49%)]\tLoss: 0.162568\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2944/6000 (49%)]\tLoss: 0.153863\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2960/6000 (49%)]\tLoss: 0.156517\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2976/6000 (50%)]\tLoss: 0.141647\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [2992/6000 (50%)]\tLoss: 0.142584\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3008/6000 (50%)]\tLoss: 0.147759\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3024/6000 (50%)]\tLoss: 0.146903\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3040/6000 (51%)]\tLoss: 0.142808\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3056/6000 (51%)]\tLoss: 0.144894\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3072/6000 (51%)]\tLoss: 0.141499\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3088/6000 (51%)]\tLoss: 0.141940\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3104/6000 (52%)]\tLoss: 0.145367\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3120/6000 (52%)]\tLoss: 0.148077\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3136/6000 (52%)]\tLoss: 0.150922\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3152/6000 (53%)]\tLoss: 0.144654\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3168/6000 (53%)]\tLoss: 0.140672\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3184/6000 (53%)]\tLoss: 0.141908\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3200/6000 (53%)]\tLoss: 0.142907\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3216/6000 (54%)]\tLoss: 0.143546\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3232/6000 (54%)]\tLoss: 0.141564\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3248/6000 (54%)]\tLoss: 0.146553\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3264/6000 (54%)]\tLoss: 0.161263\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3280/6000 (55%)]\tLoss: 0.141452\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3296/6000 (55%)]\tLoss: 0.136396\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3312/6000 (55%)]\tLoss: 0.146643\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3328/6000 (55%)]\tLoss: 0.148652\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3344/6000 (56%)]\tLoss: 0.145101\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3360/6000 (56%)]\tLoss: 0.142778\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3376/6000 (56%)]\tLoss: 0.142700\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3392/6000 (57%)]\tLoss: 0.147750\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3408/6000 (57%)]\tLoss: 0.146234\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3424/6000 (57%)]\tLoss: 0.133255\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3440/6000 (57%)]\tLoss: 0.140136\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3456/6000 (58%)]\tLoss: 0.136237\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3472/6000 (58%)]\tLoss: 0.141385\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3488/6000 (58%)]\tLoss: 0.135652\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3504/6000 (58%)]\tLoss: 0.134023\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3520/6000 (59%)]\tLoss: 0.147009\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3536/6000 (59%)]\tLoss: 0.137251\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3552/6000 (59%)]\tLoss: 0.140919\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3568/6000 (59%)]\tLoss: 0.134279\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3584/6000 (60%)]\tLoss: 0.137503\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3600/6000 (60%)]\tLoss: 0.139479\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3616/6000 (60%)]\tLoss: 0.139090\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3632/6000 (61%)]\tLoss: 0.134878\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3648/6000 (61%)]\tLoss: 0.145502\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3664/6000 (61%)]\tLoss: 0.134150\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3680/6000 (61%)]\tLoss: 0.134872\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3696/6000 (62%)]\tLoss: 0.144854\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3712/6000 (62%)]\tLoss: 0.133007\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3728/6000 (62%)]\tLoss: 0.138940\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3744/6000 (62%)]\tLoss: 0.138231\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3760/6000 (63%)]\tLoss: 0.147212\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3776/6000 (63%)]\tLoss: 0.137167\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3792/6000 (63%)]\tLoss: 0.145598\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3808/6000 (63%)]\tLoss: 0.139370\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3824/6000 (64%)]\tLoss: 0.154015\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3840/6000 (64%)]\tLoss: 0.148785\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3856/6000 (64%)]\tLoss: 0.155240\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3872/6000 (65%)]\tLoss: 0.143536\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3888/6000 (65%)]\tLoss: 0.139673\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3904/6000 (65%)]\tLoss: 0.139062\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3920/6000 (65%)]\tLoss: 0.148955\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3936/6000 (66%)]\tLoss: 0.135645\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3952/6000 (66%)]\tLoss: 0.134248\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3968/6000 (66%)]\tLoss: 0.139495\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [3984/6000 (66%)]\tLoss: 0.144409\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4000/6000 (67%)]\tLoss: 0.139973\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4016/6000 (67%)]\tLoss: 0.138945\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4032/6000 (67%)]\tLoss: 0.132198\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4048/6000 (67%)]\tLoss: 0.138004\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4064/6000 (68%)]\tLoss: 0.136862\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4080/6000 (68%)]\tLoss: 0.138029\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4096/6000 (68%)]\tLoss: 0.134975\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4112/6000 (69%)]\tLoss: 0.131727\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4128/6000 (69%)]\tLoss: 0.126712\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4144/6000 (69%)]\tLoss: 0.128545\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4160/6000 (69%)]\tLoss: 0.131158\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4176/6000 (70%)]\tLoss: 0.142240\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4192/6000 (70%)]\tLoss: 0.136805\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4208/6000 (70%)]\tLoss: 0.135464\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4224/6000 (70%)]\tLoss: 0.128866\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4240/6000 (71%)]\tLoss: 0.135778\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4256/6000 (71%)]\tLoss: 0.129404\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4272/6000 (71%)]\tLoss: 0.136576\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4288/6000 (71%)]\tLoss: 0.125722\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4304/6000 (72%)]\tLoss: 0.145287\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4320/6000 (72%)]\tLoss: 0.133160\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4336/6000 (72%)]\tLoss: 0.125944\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4352/6000 (73%)]\tLoss: 0.133576\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4368/6000 (73%)]\tLoss: 0.133417\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4384/6000 (73%)]\tLoss: 0.127919\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4400/6000 (73%)]\tLoss: 0.132980\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4416/6000 (74%)]\tLoss: 0.133871\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4432/6000 (74%)]\tLoss: 0.126232\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4448/6000 (74%)]\tLoss: 0.136923\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4464/6000 (74%)]\tLoss: 0.132760\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4480/6000 (75%)]\tLoss: 0.134287\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4496/6000 (75%)]\tLoss: 0.128745\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4512/6000 (75%)]\tLoss: 0.129728\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4528/6000 (75%)]\tLoss: 0.128241\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4544/6000 (76%)]\tLoss: 0.121605\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4560/6000 (76%)]\tLoss: 0.126481\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4576/6000 (76%)]\tLoss: 0.141496\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4592/6000 (77%)]\tLoss: 0.131463\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4608/6000 (77%)]\tLoss: 0.129065\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4624/6000 (77%)]\tLoss: 0.129130\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4640/6000 (77%)]\tLoss: 0.124222\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4656/6000 (78%)]\tLoss: 0.131712\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4672/6000 (78%)]\tLoss: 0.122486\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4688/6000 (78%)]\tLoss: 0.126048\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4704/6000 (78%)]\tLoss: 0.126107\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4720/6000 (79%)]\tLoss: 0.123566\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4736/6000 (79%)]\tLoss: 0.131864\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4752/6000 (79%)]\tLoss: 0.117654\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4768/6000 (79%)]\tLoss: 0.132354\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4784/6000 (80%)]\tLoss: 0.123257\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4800/6000 (80%)]\tLoss: 0.145533\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4816/6000 (80%)]\tLoss: 0.130055\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4832/6000 (81%)]\tLoss: 0.126277\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4848/6000 (81%)]\tLoss: 0.141628\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4864/6000 (81%)]\tLoss: 0.131269\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4880/6000 (81%)]\tLoss: 0.128539\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4896/6000 (82%)]\tLoss: 0.128275\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4912/6000 (82%)]\tLoss: 0.126146\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4928/6000 (82%)]\tLoss: 0.119913\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4944/6000 (82%)]\tLoss: 0.128200\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4960/6000 (83%)]\tLoss: 0.128834\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4976/6000 (83%)]\tLoss: 0.129134\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [4992/6000 (83%)]\tLoss: 0.139929\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5008/6000 (83%)]\tLoss: 0.123948\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5024/6000 (84%)]\tLoss: 0.126367\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5040/6000 (84%)]\tLoss: 0.130953\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5056/6000 (84%)]\tLoss: 0.123645\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5072/6000 (85%)]\tLoss: 0.133105\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5088/6000 (85%)]\tLoss: 0.125183\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5104/6000 (85%)]\tLoss: 0.121396\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5120/6000 (85%)]\tLoss: 0.129241\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5136/6000 (86%)]\tLoss: 0.124090\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5152/6000 (86%)]\tLoss: 0.140614\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5168/6000 (86%)]\tLoss: 0.123184\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5184/6000 (86%)]\tLoss: 0.127499\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5200/6000 (87%)]\tLoss: 0.128551\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5216/6000 (87%)]\tLoss: 0.139960\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5232/6000 (87%)]\tLoss: 0.124353\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5248/6000 (87%)]\tLoss: 0.128841\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5264/6000 (88%)]\tLoss: 0.120762\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5280/6000 (88%)]\tLoss: 0.131128\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5296/6000 (88%)]\tLoss: 0.122819\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5312/6000 (89%)]\tLoss: 0.134433\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5328/6000 (89%)]\tLoss: 0.123912\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5344/6000 (89%)]\tLoss: 0.123340\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5360/6000 (89%)]\tLoss: 0.121490\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5376/6000 (90%)]\tLoss: 0.124660\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5392/6000 (90%)]\tLoss: 0.130858\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5408/6000 (90%)]\tLoss: 0.129404\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5424/6000 (90%)]\tLoss: 0.127481\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5440/6000 (91%)]\tLoss: 0.119083\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5456/6000 (91%)]\tLoss: 0.126837\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5472/6000 (91%)]\tLoss: 0.131677\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5488/6000 (91%)]\tLoss: 0.128640\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5504/6000 (92%)]\tLoss: 0.117250\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5520/6000 (92%)]\tLoss: 0.132131\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5536/6000 (92%)]\tLoss: 0.134642\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5552/6000 (93%)]\tLoss: 0.121233\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5568/6000 (93%)]\tLoss: 0.121107\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5584/6000 (93%)]\tLoss: 0.121610\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5600/6000 (93%)]\tLoss: 0.120346\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5616/6000 (94%)]\tLoss: 0.112467\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5632/6000 (94%)]\tLoss: 0.118285\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5648/6000 (94%)]\tLoss: 0.129370\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5664/6000 (94%)]\tLoss: 0.122349\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5680/6000 (95%)]\tLoss: 0.119608\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5696/6000 (95%)]\tLoss: 0.116839\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5712/6000 (95%)]\tLoss: 0.127171\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5728/6000 (95%)]\tLoss: 0.118039\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5744/6000 (96%)]\tLoss: 0.126069\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5760/6000 (96%)]\tLoss: 0.125875\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5776/6000 (96%)]\tLoss: 0.121354\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5792/6000 (97%)]\tLoss: 0.128357\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5808/6000 (97%)]\tLoss: 0.124874\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5824/6000 (97%)]\tLoss: 0.124299\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5840/6000 (97%)]\tLoss: 0.116942\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5856/6000 (98%)]\tLoss: 0.132762\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5872/6000 (98%)]\tLoss: 0.132618\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5888/6000 (98%)]\tLoss: 0.110510\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5904/6000 (98%)]\tLoss: 0.125176\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5920/6000 (99%)]\tLoss: 0.116891\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5936/6000 (99%)]\tLoss: 0.121185\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5952/6000 (99%)]\tLoss: 0.118236\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5968/6000 (99%)]\tLoss: 0.134322\tLR: 0.01\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [5984/6000 (100%)]\tLoss: 0.129926\tLR: 0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ljNdSSLFukl"
      },
      "source": [
        "def save_model(model, name):\n",
        "  name += '.pth'\n",
        "  torch.save(model, '/content/drive/My Drive/models/transformers/' + name)\n",
        "  return name\n",
        "\n",
        "def load_model(name):\n",
        "  name +='.pth'\n",
        "  model = torch.load('/content/drive/My Drive/models/transformers/' + name)\n",
        "  return model\n",
        "\n",
        "MODEL_NAME = 'transformer_01_scale_loss_diff' + ''.join(NORMAL_CLASSES)\n",
        "name = save_model(transformer, MODEL_NAME)\n",
        "model = load_model(MODEL_NAME)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2NudZ295rdD"
      },
      "source": [
        "def evaluate(model, val_loader, device, number_of_frames):\n",
        "  #currently the batch size for evaluation needs to be 1\n",
        "  total_anom_scores = []\n",
        "  total_targets = []\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for _, data in enumerate(val_loader, 0):\n",
        "      inputs, target = data\n",
        "      #print(inputs.shape)\n",
        "      inputs = patch_batch(inputs, NUMBER_OF_FRAMES)\n",
        "      #print(inputs.shape) #(n_spectograms, n_patches, features)\n",
        "      #every patch needs to be masked once and the masked loss calculated added and divided by number of patches\n",
        "      loss_total_current_spec = 0\n",
        "      for i in range(inputs.shape[1]): #iterate through patches\n",
        "        masked_input, mask_idxs = mask_input_batch(inputs, i) #mask every patch once\n",
        "        output = model(masked_input)\n",
        "        loss = calculate_loss_masked(inputs, output, mask_idxs, True) # last argument (sum) does not make a difference for batch size 1\n",
        "        loss_total_current_spec += loss\n",
        "      \n",
        "      loss_total_current_spec /= inputs.shape[1] #divide by number of patches\n",
        "      print(loss_total_current_spec)\n",
        "      total_anom_scores.append(loss_total_current_spec.cpu().numpy()) #coverting to numpy for processing with scikit\n",
        "      total_targets.append(target)\n",
        "    return total_anom_scores, total_targets"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "717_tKu_A3z9"
      },
      "source": [
        "def evaluate_one_index(model, val_loader, device):\n",
        "  total_anom_scores = []\n",
        "  total_targets = []\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for _, data in enumerate(val_loader, 0):\n",
        "      inputs, batch_targets = data\n",
        "      #print(inputs.shape)\n",
        "      inputs = inputs.to(device)\n",
        "      inputs = patch_batch(inputs, NUMBER_OF_FRAMES)\n",
        "      #print(inputs.shape) #(n_spectograms, n_patches, features)\n",
        "      masked_input, mask_idxs = mask_input_batch(inputs,device, 10) #calculate mask for every spectogram in the batch at index\n",
        "      outputs = model(masked_input) #(n_spectograms, n_patches_reconstruces, features)\n",
        "      batch_anom_scores = calculate_loss_masked(inputs, outputs, mask_idxs, True)\n",
        "      print(batch_anom_scores)\n",
        "\n",
        "      total_anom_scores.append(batch_anom_scores.cpu().numpy()) #coverting to numpy for processing with scikit\n",
        "      total_targets += [x.cpu().numpy() for x in batch_targets]\n",
        "    return total_anom_scores, total_targets"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZHc28mmTGIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d016c09-e867-454b-d68c-f6bde4aef54e"
      },
      "source": [
        "#anom_scores, targets = evaluate_one_index(model, test_loader, device)\n",
        "anom_scores, targets = evaluate(model, test_loader, device, NUMBER_OF_FRAMES)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.7616)\n",
            "tensor(1.7754)\n",
            "tensor(1.7949)\n",
            "tensor(1.9033)\n",
            "tensor(1.7977)\n",
            "tensor(1.9446)\n",
            "tensor(1.7701)\n",
            "tensor(1.8616)\n",
            "tensor(1.8493)\n",
            "tensor(1.7516)\n",
            "tensor(1.7624)\n",
            "tensor(1.7609)\n",
            "tensor(1.7500)\n",
            "tensor(1.7622)\n",
            "tensor(1.7667)\n",
            "tensor(1.7870)\n",
            "tensor(1.9335)\n",
            "tensor(1.7532)\n",
            "tensor(1.7613)\n",
            "tensor(1.7842)\n",
            "tensor(1.7560)\n",
            "tensor(1.9661)\n",
            "tensor(1.9325)\n",
            "tensor(1.7602)\n",
            "tensor(1.7673)\n",
            "tensor(1.8331)\n",
            "tensor(1.7605)\n",
            "tensor(1.7530)\n",
            "tensor(1.9135)\n",
            "tensor(1.7613)\n",
            "tensor(1.9324)\n",
            "tensor(1.9207)\n",
            "tensor(1.7763)\n",
            "tensor(1.9328)\n",
            "tensor(1.7580)\n",
            "tensor(1.7872)\n",
            "tensor(1.9423)\n",
            "tensor(1.8555)\n",
            "tensor(1.7626)\n",
            "tensor(1.9623)\n",
            "tensor(1.7685)\n",
            "tensor(1.7685)\n",
            "tensor(1.9134)\n",
            "tensor(1.8844)\n",
            "tensor(1.7566)\n",
            "tensor(1.7526)\n",
            "tensor(1.7622)\n",
            "tensor(1.7576)\n",
            "tensor(1.7546)\n",
            "tensor(1.7608)\n",
            "tensor(1.7491)\n",
            "tensor(1.7605)\n",
            "tensor(1.7636)\n",
            "tensor(1.7589)\n",
            "tensor(1.7941)\n",
            "tensor(1.7649)\n",
            "tensor(1.9863)\n",
            "tensor(1.9098)\n",
            "tensor(1.7791)\n",
            "tensor(1.7507)\n",
            "tensor(1.7512)\n",
            "tensor(1.7770)\n",
            "tensor(1.7586)\n",
            "tensor(1.9294)\n",
            "tensor(1.7609)\n",
            "tensor(1.7619)\n",
            "tensor(1.7509)\n",
            "tensor(1.8246)\n",
            "tensor(1.7641)\n",
            "tensor(1.8420)\n",
            "tensor(1.9380)\n",
            "tensor(1.8023)\n",
            "tensor(1.9305)\n",
            "tensor(1.8768)\n",
            "tensor(1.7597)\n",
            "tensor(1.7987)\n",
            "tensor(1.7588)\n",
            "tensor(1.7793)\n",
            "tensor(1.7608)\n",
            "tensor(1.9339)\n",
            "tensor(1.7589)\n",
            "tensor(1.7625)\n",
            "tensor(1.7765)\n",
            "tensor(1.7609)\n",
            "tensor(1.7963)\n",
            "tensor(1.7603)\n",
            "tensor(1.7725)\n",
            "tensor(1.9185)\n",
            "tensor(1.7580)\n",
            "tensor(1.7633)\n",
            "tensor(1.7542)\n",
            "tensor(1.7506)\n",
            "tensor(1.9595)\n",
            "tensor(1.7575)\n",
            "tensor(1.9555)\n",
            "tensor(1.7627)\n",
            "tensor(1.8887)\n",
            "tensor(1.7712)\n",
            "tensor(1.7483)\n",
            "tensor(1.7513)\n",
            "tensor(1.7596)\n",
            "tensor(1.7627)\n",
            "tensor(1.7599)\n",
            "tensor(1.9397)\n",
            "tensor(1.7480)\n",
            "tensor(1.7655)\n",
            "tensor(1.9207)\n",
            "tensor(1.7958)\n",
            "tensor(1.9223)\n",
            "tensor(1.8907)\n",
            "tensor(1.7534)\n",
            "tensor(1.7665)\n",
            "tensor(1.7631)\n",
            "tensor(1.7620)\n",
            "tensor(1.9157)\n",
            "tensor(1.7611)\n",
            "tensor(1.7688)\n",
            "tensor(1.7564)\n",
            "tensor(1.8278)\n",
            "tensor(1.9393)\n",
            "tensor(1.7675)\n",
            "tensor(1.9327)\n",
            "tensor(1.7612)\n",
            "tensor(1.7777)\n",
            "tensor(1.7903)\n",
            "tensor(1.7611)\n",
            "tensor(1.9232)\n",
            "tensor(1.7847)\n",
            "tensor(1.9032)\n",
            "tensor(1.9713)\n",
            "tensor(1.9176)\n",
            "tensor(1.9075)\n",
            "tensor(1.7667)\n",
            "tensor(1.9469)\n",
            "tensor(1.7478)\n",
            "tensor(1.7742)\n",
            "tensor(1.9603)\n",
            "tensor(1.9431)\n",
            "tensor(1.7552)\n",
            "tensor(2.0006)\n",
            "tensor(1.7594)\n",
            "tensor(1.7581)\n",
            "tensor(1.7659)\n",
            "tensor(1.7559)\n",
            "tensor(1.7966)\n",
            "tensor(1.9378)\n",
            "tensor(1.7775)\n",
            "tensor(1.8056)\n",
            "tensor(1.9451)\n",
            "tensor(1.7498)\n",
            "tensor(1.9261)\n",
            "tensor(1.9257)\n",
            "tensor(1.7660)\n",
            "tensor(1.7619)\n",
            "tensor(1.9633)\n",
            "tensor(1.9258)\n",
            "tensor(1.7591)\n",
            "tensor(1.7630)\n",
            "tensor(1.7621)\n",
            "tensor(1.7541)\n",
            "tensor(1.7743)\n",
            "tensor(1.9228)\n",
            "tensor(1.8426)\n",
            "tensor(1.7619)\n",
            "tensor(1.7706)\n",
            "tensor(1.7604)\n",
            "tensor(1.9700)\n",
            "tensor(1.7934)\n",
            "tensor(1.9351)\n",
            "tensor(1.7668)\n",
            "tensor(1.7777)\n",
            "tensor(1.9049)\n",
            "tensor(1.7709)\n",
            "tensor(1.7822)\n",
            "tensor(1.7555)\n",
            "tensor(1.7618)\n",
            "tensor(1.9221)\n",
            "tensor(1.7779)\n",
            "tensor(1.7562)\n",
            "tensor(1.7781)\n",
            "tensor(1.7533)\n",
            "tensor(1.9332)\n",
            "tensor(1.7602)\n",
            "tensor(1.7646)\n",
            "tensor(1.7742)\n",
            "tensor(1.7620)\n",
            "tensor(1.9301)\n",
            "tensor(1.9414)\n",
            "tensor(1.7537)\n",
            "tensor(1.7597)\n",
            "tensor(1.7590)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwVshIaHNTMt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f85d923-a0d4-46e8-d8a4-eb6be2711da8"
      },
      "source": [
        "print(len(anom_scores))\n",
        "print(len(targets))\n",
        "print(anom_scores[5])\n",
        "print(targets[5])\n",
        "print(anom_scores[1])\n",
        "print(targets[1])\n",
        "print(anom_scores[3])\n",
        "print(targets[3])\n",
        "print(anom_scores[50])\n",
        "print(targets[50])\n",
        "print(anom_scores[100])\n",
        "print(targets[100])"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "191\n",
            "191\n",
            "1.8710115\n",
            "1\n",
            "1.757793\n",
            "0\n",
            "1.7782617\n",
            "1\n",
            "1.9053049\n",
            "1\n",
            "1.8762075\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9A6rZApGWcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "ea728dc3-e18f-4223-bcff-13558d1a4fa4"
      },
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import softmax\n",
        "import time\n",
        "current_time = time.asctime( time.localtime(time.time()) )\n",
        "\n",
        "fp_rate, tp_rate, _ = roc_curve(targets, anom_scores, pos_label=1)\n",
        "roc_auc = roc_auc_score(targets, anom_scores)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fp_rate, tp_rate, color='blue', label=f\"ROC_AUC ={roc_auc}\")\n",
        "\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve of ' + MODEL_NAME +' with normal Classes: ' + ''.join(NORMAL_CLASSES))\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('/content/drive/My Drive/models/transformers/roc_graph' + MODEL_NAME+ ''.join(NORMAL_CLASSES) + \"_\" + str(roc_auc) + \".jpg\")\n",
        "plt.show()"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnqcTXZJGreX"
      },
      "source": [
        "import pytorch_model_summary as pms\n",
        "summary = pms.summary(model, torch.ones(16, 22, 256).to(device))"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC950s26HEkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e155b5c6-de65-4b9e-8b9c-977ea6fb5cd5"
      },
      "source": [
        "print(\"Traininig finished! Saving Config...\")\n",
        "from google.colab import files\n",
        "\n",
        "#save hyperparams and result:\n",
        "with open('/content/drive/My Drive/models/transformers/hyper_params' + MODEL_NAME + '.txt', 'w') as f:\n",
        "  f.write(f\"Epochs: {EPOCHS}, LR: {LEARNING_RATE}, BatchSize: {BATCH_SIZE}, SAMPLE_RATE = {SAMPLE_RATE}, N_FFT/WINDOW_SIZE = {N_FFT}, HOP_LENGTH = {HOP_LENGTH}, N_MELS = {N_MELS}\" \n",
        "          + f\"NUMBER_OF_FRAMES: {NUMBER_OF_FRAMES}, EMBEDDING_SIZE = {EMBEDDING_SIZE}, N_HEADS = {N_HEADS}, N_ENCODER_LAYERS = {N_ENCODER_LAYERS}, DROPOUT = {DROPOUT}, DIM_FEED_FORWARD = {DIM_FEED_FORWARD}\"\n",
        "  + f\"\\nNormal Classes: {NORMAL_CLASSES}, Anomalous Classes: {ANOMALOUS_CLASSES}, ROC_AUC Score: {roc_auc} + \\n\\n {summary}\")"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traininig finished! Saving Config...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}