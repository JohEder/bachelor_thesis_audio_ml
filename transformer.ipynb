{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNeae5bBab3stbXU5/HRvhQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohEder/bachelor_thesis_audio_ml/blob/master/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIBl-ybmvCJ7",
        "outputId": "7834aa4b-4e70-448c-8ee9-0f5e2a1c0a2a"
      },
      "source": [
        "!pip install torchaudio\n",
        "!pip install pytorch-model-summary"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchaudio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/20/eab40caad8f4b97f5e91a5de8ba5ec29115e08fa4c9a808725490b7b4844/torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchaudio) (3.7.4.3)\n",
            "Installing collected packages: torchaudio\n",
            "Successfully installed torchaudio-0.9.0\n",
            "Collecting pytorch-model-summary\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/45/01d67be55fe3683a9221ac956ba46d1ca32da7bf96029b8d1c7667b8a55c/pytorch_model_summary-0.1.2-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-model-summary) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-model-summary) (4.41.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch-model-summary) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-model-summary) (3.7.4.3)\n",
            "Installing collected packages: pytorch-model-summary\n",
            "Successfully installed pytorch-model-summary-0.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdn43FFt6jp0"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset \n",
        "import torchaudio\n",
        "import pandas as pd\n",
        "import os\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm9hEziO6l5F"
      },
      "source": [
        "#just copied the official import script for the dataset, custom preprocessing happens afterwards\n",
        "\"\"\" Import script for IDMT-Traffic dataset\n",
        "Ref:\n",
        "    J. Abeßer, S. Gourishetti, A. Kátai, T. Clauß, P. Sharma, J. Liebetrau: IDMT-Traffic: An Open Benchmark\n",
        "    Dataset for Acoustic Traffic Monitoring Research, EUSIPCO, 2021\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "__author__ = 'Jakob Abeßer (jakob.abesser@idmt.fraunhofer.de)'\n",
        "\n",
        "\n",
        "def import_idmt_traffic_dataset(fn_txt: str = \"idmt_traffic_all\") -> pd.DataFrame:\n",
        "    \"\"\" Import IDMT-Traffic dataset\n",
        "    Args:\n",
        "        fn_txt (str): Text file with all WAV files\n",
        "    Returns:\n",
        "        df_dataset (pd.Dataframe): File-wise metadata\n",
        "            Columns:\n",
        "                'file': WAV filename,\n",
        "                'is_background': True if recording contains background noise (no vehicle), False else\n",
        "                'date_time': Recording time (YYYY-MM-DD-HH-mm)\n",
        "                'location': Recording location\n",
        "                'speed_kmh': Speed limit at recording site (km/h), UNK if unknown,\n",
        "                'sample_pos': Sample position (centered) within the original audio recording,\n",
        "                'daytime': M(orning) or (A)fternoon,\n",
        "                'weather': (D)ry or (W)et road condition,\n",
        "                'vehicle': (B)us, (C)ar, (M)otorcycle, or (T)ruck,\n",
        "                'source_direction': Source direction of passing vehicle: from (L)eft or from (R)ight,\n",
        "                'microphone': (SE)= (high-quality) sE8 microphones, (ME) = (low-quality) MEMS microphones (ICS-43434),\n",
        "                'channel': Original stereo pair channel (12) or (34)\n",
        "    \"\"\"\n",
        "    # load file list\n",
        "    df_files = pd.read_csv(fn_txt, names=('file',))\n",
        "    fn_file_list = df_files['file'].to_list()\n",
        "\n",
        "    # load metadata from file names\n",
        "    df_dataset = []\n",
        "\n",
        "    for f, fn in enumerate(fn_file_list):\n",
        "        fn = fn.replace('.wav', '')\n",
        "        parts = fn.split('_')\n",
        "\n",
        "        # background noise files\n",
        "        if '-BG' in fn:\n",
        "            date_time, location, speed_kmh, sample_pos, mic, channel = parts\n",
        "            vehicle, source_direction, weather, daytime = 'None', 'None', 'None', 'None'\n",
        "            is_background = True\n",
        "\n",
        "        # files with vehicle passings\n",
        "        else:\n",
        "            date_time, location, speed_kmh, sample_pos, daytime, weather, vehicle_direction, mic, channel = parts\n",
        "            vehicle, source_direction = vehicle_direction\n",
        "            is_background = False\n",
        "\n",
        "        channel = channel.replace('-BG', '')\n",
        "        speed_kmh = speed_kmh.replace('unknownKmh', 'UNK')\n",
        "        speed_kmh = speed_kmh.replace('Kmh', '')\n",
        "\n",
        "        df_dataset.append({'file': fn,\n",
        "                           'is_background': is_background,\n",
        "                           'date_time': date_time,\n",
        "                           'location': location,\n",
        "                           'speed_kmh': speed_kmh,\n",
        "                           'sample_pos': sample_pos,\n",
        "                           'daytime': daytime,\n",
        "                           'weather': weather,\n",
        "                           'vehicle': vehicle,\n",
        "                           'source_direction': source_direction,\n",
        "                           'microphone': mic,\n",
        "                           'channel': channel})\n",
        "\n",
        "    df_dataset = pd.DataFrame(df_dataset, columns=('file', 'is_background', 'date_time', 'location', 'speed_kmh', 'sample_pos', 'daytime', 'weather', 'vehicle',\n",
        "                                                   'source_direction', 'microphone', 'channel'))\n",
        "\n",
        "    return df_dataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53-c6jdM6uLg"
      },
      "source": [
        "\"\"\"\n",
        "Anomalous Sound Transformer Model for my Bachelor Thesis\n",
        "\"\"\"\n",
        "\n",
        "__author__ = 'Johannes Eder (Jo.Eder@campus.lmu.de)'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAnBzNvl7V9Q",
        "outputId": "3381ad6b-6fc5-44bb-ba2a-c79341ffdad3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gArrnoTR7aiz"
      },
      "source": [
        "CLASSES = ['None','C','T', 'M', 'B'] #Background Noise, Car, Truck, Motorcycle, Bus\n",
        "NORMAL_CLASSES = ['None']\n",
        "ANOMALOUS_CLASSES = ['C','T', 'M', 'B']\n",
        "\n",
        "SAMPLE_RATE = 22500\n",
        "N_FFT=2048 #is also window size\n",
        "HOP_LENGTH=1024\n",
        "N_MELS=128\n",
        "NUMBER_OF_FRAMES = 4\n",
        "melspectogram = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=SAMPLE_RATE,\n",
        "        n_fft=N_FFT, # Frame Size\n",
        "        hop_length=HOP_LENGTH, #here half the frame size\n",
        "        n_mels=N_MELS\n",
        "    )\n",
        "\n",
        "transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(mode='L'),\n",
        "    #transforms.Grayscale(num_output_channels=3),\n",
        "    #transforms.Resize([224, 224]),\n",
        "    #transforms.RandomCrop(size=[N_MELS, NUMBER_OF_FRAMES]), #only train on random slice of the spectogram\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "AUDIO_DIR = \"/content/drive/My Drive/datasets/IDMT_Traffic/audio\"\n",
        "train_annotations = \"/content/drive/My Drive/datasets/IDMT_Traffic/annotation/eusipco_2021_train.csv\"\n",
        "test_annotatons = \"/content/drive/My Drive/datasets/IDMT_Traffic/annotation/eusipco_2021_test.csv\"\n",
        "all_annotations_txt = \"/content/drive/My Drive/datasets/IDMT_Traffic/annotation/idmt_traffic_all.txt\"\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "BATCH_SIZE_VAL = 1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjg39EMZ75O1"
      },
      "source": [
        "class IdmtTrafficDataSet(Dataset):\n",
        "\n",
        "    def __init__(self, annotations_file, audio_dir, audio_transformation, transformation, target_sample_rate, normal_classes):\n",
        "        self.annotations =  annotations_file if isinstance(annotations_file, pd.DataFrame) else pd.read_csv(annotations_file)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.audio_transformation = audio_transformation\n",
        "        self.transformation = transformation\n",
        "        self.target_sample_rate = target_sample_rate\n",
        "        #self.classes = ['None','C','T', 'M', 'B']\n",
        "        self.normal_classes = normal_classes\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        audio_sample_path = self._get_audio_sample_path(index)\n",
        "        label = self._get_audio_sample_label(index)\n",
        "        signal, sr = torchaudio.load(audio_sample_path)\n",
        "        signal = self._resample(signal, sr) #adjust sample rates\n",
        "        # signal -> (num_channels, samples) i.e. (2, 16000)\n",
        "        signal  = self._mix_down(signal) #stereo to mono\n",
        "        signal = self.audio_transformation(signal) #(1, 16000) -> torch.Size([1, 64, 63])\n",
        "        signal = self.transformation(signal)\n",
        "        #label = self.normal_classes.index(label)\n",
        "        label = 0 if label in self.normal_classes else 1 #1 for normal 0 for anomalous\n",
        "        return signal, label\n",
        "\n",
        "    def _resample(self, signal, sr):\n",
        "        if sr != self.target_sample_rate:\n",
        "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
        "            signal = resampler(signal)\n",
        "        return signal\n",
        "    \n",
        "    def _mix_down(self, signal):\n",
        "        if signal.shape[0] > 1: #(2, 16000)\n",
        "            #mean operation: aggregating multiple channels\n",
        "            signal = torch.mean(signal, 0, True)\n",
        "        return signal\n",
        "\n",
        "    def _get_audio_sample_path(self, index):\n",
        "        path = os.path.join(self.audio_dir, self.annotations.iloc[index, 0])\n",
        "        return path + '.wav'\n",
        "\n",
        "    def _get_audio_sample_label(self, index):\n",
        "        return self.annotations.iloc[index, 9]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf1bYw0c8Qe6"
      },
      "source": [
        "def get_train_and_val_idmt(audio_dir, train_annotations, test_annotations):\n",
        "    #this method is for the classifciation task\n",
        "\n",
        "    train_data = IdmtTrafficDataSet(train_annotations, audio_dir,melspectogram, transforms, SAMPLE_RATE)\n",
        "    test_data = IdmtTrafficDataSet(test_annotatons, audio_dir,melspectogram, transforms, SAMPLE_RATE)\n",
        "    return train_data, test_data\n",
        "\n",
        "def get_normal_and_anomalous_data(normal_classes, anomalous_classes, audio_dir, annotations, batch_size):\n",
        "    #this method is for the AD Task\n",
        "    if len((set(normal_classes) & set(anomalous_classes))) > 0:\n",
        "      raise Exception(\"Intersection between normal and anomalous classes should be empty!\")\n",
        "\n",
        "    all_data = import_idmt_traffic_dataset(annotations)\n",
        "\n",
        "    normal_data = all_data[all_data.vehicle.isin(normal_classes)]\n",
        "    anomalous_data = all_data[all_data.vehicle.isin(anomalous_classes)]\n",
        "\n",
        "    train_data = normal_data.iloc[:6000, :] #test data needs to have some amount of normal data as well\n",
        "    train_data = adjust_sample_number_to_batch_size(train_data, batch_size)\n",
        "\n",
        "    normal_test_data = normal_data.iloc[6001:6101, :] #later more all test samples need to be used, but for now it is faster\n",
        "    number_of_normal_test_sampels = len(normal_test_data)\n",
        "    print(f\"testing with {number_of_normal_test_sampels} normal samples\")\n",
        "\n",
        "    #sample same number of anomalous data to test\n",
        "    anomalous_data = anomalous_data.sample(number_of_normal_test_sampels)\n",
        "\n",
        "    frames = [anomalous_data, normal_test_data]\n",
        "    concatenated_test_data = pd.concat(frames)\n",
        "    concatenated_test_data.reset_index(drop=True, inplace=True)\n",
        "    concatenated_test_data = adjust_sample_number_to_batch_size(concatenated_test_data, batch_size)\n",
        "\n",
        "    normal_train_data = IdmtTrafficDataSet(train_data, audio_dir, melspectogram, transforms, SAMPLE_RATE, normal_classes)\n",
        "    test_data = IdmtTrafficDataSet(concatenated_test_data, audio_dir, melspectogram, transforms, SAMPLE_RATE, normal_classes)\n",
        "\n",
        "    return normal_train_data, test_data\n",
        "\n",
        "def adjust_sample_number_to_batch_size(data, batch_size):\n",
        "  if len(data) % batch_size == 0:\n",
        "    return data\n",
        "  else:\n",
        "    remainder = len(data) % batch_size\n",
        "    return data.iloc[remainder + 1:,:]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuGiGUSt8beP",
        "outputId": "a76819d5-b8a6-4525-be7c-7688cf812b6f"
      },
      "source": [
        "train_data, test_data = get_normal_and_anomalous_data(NORMAL_CLASSES, ANOMALOUS_CLASSES, audio_dir=AUDIO_DIR, annotations=all_annotations_txt, batch_size=BATCH_SIZE)\n",
        "first_sample, first_label = train_data[0]\n",
        "input_dim = NUMBER_OF_FRAMES #first_sample.shape[1] * first_sample.shape[2]\n",
        "print(f\"Train Data Shape: {first_sample.shape}\") #Train Data Shape: torch.Size([1, 128, 44]), Frame Size 2: Train Data Shape: torch.Size([1, 128, 2])\n",
        "print(f\"Input dimension: {input_dim}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing with 100 normal samples\n",
            "Train Data Shape: torch.Size([1, 128, 44])\n",
            "Input dimension: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwLjzSG0HkIC"
      },
      "source": [
        "class TransformerModel(nn.Module):\n",
        "  def __init__(self, d_model, input_dim, n_heads, dim_feedforward, n_encoder_layers, dropout=0.5):\n",
        "    super(TransformerModel, self).__init__()\n",
        "    self.model_type = 'Transformer'\n",
        "    self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "    encoder_layers = TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "    self.transformer_encoder = TransformerEncoder(encoder_layers, n_encoder_layers)\n",
        "    self.patch_embedding = PatchEmbedding(input_dim, d_model)\n",
        "    self.input_dim = input_dim\n",
        "    self.d_model = d_model\n",
        "    self.decoder = nn.Linear(in_features=d_model, out_features=input_dim)\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    initrange = 0.1\n",
        "    #self.patch_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "    self.decoder.bias.data.zero_()\n",
        "    self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "  \n",
        "  def forward(self, input):\n",
        "    embedded = self.patch_embedding(input)  * math.sqrt(self.d_model) #is scaling necessary? yes, otherwise values are incredibly small\n",
        "    pos_encoded_embedded = self.pos_encoder(embedded)\n",
        "    transformer_out = self.transformer_encoder(pos_encoded_embedded)\n",
        "    output = self.decoder(transformer_out)\n",
        "    return output"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be9tNwH7OyRC"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, embedding_dim, dropout=0.1, max_len=5000):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    pe = torch.zeros(max_len, embedding_dim)\n",
        "    #print(f\"Shape: {pe.shape}\")\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    #print(f\"Position shape: {position.shape}\")\n",
        "    div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "    self.register_buffer('pe', pe)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = x + self.pe[:x.size(0), :]\n",
        "    return self.dropout(x)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s468t9EyVgkm"
      },
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self, input_dim, embedding_dimension):\n",
        "    super().__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.embedding_layer = nn.Linear(input_dim, embedding_dimension)\n",
        "  \n",
        "  def forward(self, input_data):\n",
        "    embedding = self.embedding_layer(input_data)\n",
        "    return embedding"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7MOkCsOWp7X"
      },
      "source": [
        "NUMBER_OF_FRAMES = 2\n",
        "EMBEDDING_SIZE = 512"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar2Mdd-ZpjuB"
      },
      "source": [
        "def patch_batch(input_batch, number_of_frames):\n",
        "  #input of shape (batch_size, channels, mel_filters, frames)\n",
        "  unfold = nn.Unfold(kernel_size=(input_batch.shape[2], NUMBER_OF_FRAMES), stride=NUMBER_OF_FRAMES) #patching the spectogram\n",
        "  unfolded_batch = unfold(input_batch) #(batch_size, features, number_of_patches)\n",
        "  unfolded_batch = unfolded_batch.transpose(1, 2) #(batch_size, number_of_patches, features)\n",
        "  return unfolded_batch"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hf-p_86McKfC",
        "outputId": "32398600-c7a0-4dc2-f5ca-1aabe2631ef4"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\npositional_encoder = PositionalEncoding(EMBEDDING_SIZE)\\npos_encoded = positional_encoder(embedded)\\nprint(pos_encoded.shape) #torch.Size([1024, 1024, 512]) pos_enc, ,embed_size\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D68e7futiiU",
        "outputId": "d33cac53-52f7-4185-9d13-360bc424e51c"
      },
      "source": [
        "train_data, test_data = get_normal_and_anomalous_data(NORMAL_CLASSES, ANOMALOUS_CLASSES, audio_dir=AUDIO_DIR, annotations=all_annotations_txt, batch_size=BATCH_SIZE)\n",
        "first_sample, first_label = train_data[0]\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "#test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing with 100 normal samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79YOWc_meQ1R",
        "outputId": "65ce79bd-c414-44c4-9f64-cff6fb15e440"
      },
      "source": [
        "N_HEADS = 2\n",
        "N_ENCODER_LAYERS = 2\n",
        "DROPOUT = 0.2\n",
        "DIM_FEED_FORWARD = 512\n",
        "input_dim = 256\n",
        "print(input_dim)\n",
        "transformer = TransformerModel(EMBEDDING_SIZE, input_dim, N_HEADS, DIM_FEED_FORWARD, N_ENCODER_LAYERS, DROPOUT)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "-svPSAyJpfnZ",
        "outputId": "ca193cf6-a967-4339-a290-4d52188b4c50"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\npixelwise_loss = nn.functional.mse_loss(unfolded, output, size_average=None, reduce=None, reduction='none')\\nprint(loss.shape)\\n#if second patch in every image in this batch was masked\\nprint(loss[:,2,:].sum())\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JahOBDTmNt4"
      },
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "def mask_input_batch(input):\n",
        "  number_of_specs = input.shape[0]\n",
        "  input_masked = []\n",
        "  masks_index_list = []\n",
        "  for i in range(number_of_specs):\n",
        "    mask_idx = random.randint(0, input.shape[1]-1)\n",
        "    mask = torch.ones(input.shape[1], input.shape[2])\n",
        "    mask[mask_idx, :] = 0\n",
        "    current_spec_masked = input[i, : , :].mul(mask)\n",
        "\n",
        "    input_masked.append(current_spec_masked) #maybe just tuples (current_spec_masked, mask_idx)\n",
        "    masks_index_list.append(torch.as_tensor(mask_idx))\n",
        "\n",
        "  return torch.stack(input_masked), torch.stack(masks_index_list)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "JwOvyi3HtL2T",
        "outputId": "9aba152b-ebe8-4214-def1-6b1831b4e714"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nsample_input = torch.rand(16, 22, 512) #batch_size, patches, embedded_features\\nmask_idx = random.randint(0, 22)\\nprint(mask_idx)\\nmask = torch.ones(22, 512)\\nmask[mask_idx, :] = 0\\nprint(mask.shape)\\nmasked_sample = mask.mul(sample_input[1, :, :])\\nprint(masked_sample.shape)\\n#print(masked_sample[mask_idx, :])\\n#print(masked_sample[mask_idx - 1, :])\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "pc0nRoSDyfU1",
        "outputId": "ca661a35-c4c5-45e0-ec8b-f44bb1f70837"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nsample_input = torch.rand(16, 22, 512) #batch_size, patches, embedded_features\\nsample_output = torch.rand(16, 22, 512)\\nmasked_input, mask_idxs = mask_input_batch(sample_input)\\nprint(masked_input.shape)\\nprint(mask_idxs.shape)\\nprint(masked_input[0, mask_idxs[0],:]) #masked\\nprint(masked_input[15, mask_idxs[14],:]) #not masked\\npixelwise_loss = nn.functional.mse_loss(sample_input, sample_output, size_average=None, reduce=None, reduction='none')\\nprint(pixelwise_loss.shape)\\nloss_per_batch = 0\\nfor i in range(len(mask_idxs)):\\n  loss_at_masked = pixelwise_loss[i, mask_idxs[i], :].sum()\\n  loss_per_batch +=loss_at_masked\\n\\nprint(loss_per_batch)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "057KTyNylH8s"
      },
      "source": [
        "def calculate_loss_masked(input, output, mask_idxs, sum_up):\n",
        "  pixelwise_loss = nn.functional.mse_loss(input, output, size_average=None, reduce=None, reduction='none')\n",
        "  print(pixelwise_loss.shape)\n",
        "  loss_per_batch = 0\n",
        "  losses = []\n",
        "  for i in range(len(mask_idxs)):\n",
        "    loss_at_masked = pixelwise_loss[i, mask_idxs[i], :].sum()\n",
        "    if sum_up:\n",
        "      loss_per_batch +=loss_at_masked\n",
        "    else:\n",
        "      losses.append(loss_at_masked)\n",
        "  if sum_up:\n",
        "    return loss_per_batch/len(input) #scaling the loss? can i do that?\n",
        "  else \n",
        "    return losses\n",
        "\n",
        "def calculate_loss_total(input, output, mask_idxs):\n",
        "  raise Exception(\"Not implemented\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC49Nt07fofv"
      },
      "source": [
        "LEARNING_RATE = 0.0001 # learning rate, gets adapted\n",
        "optimizer = torch.optim.SGD(transformer.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "EPOCHS = 1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "def train(model, learning_rate, optimizer, scheduler, epoch):\n",
        "  print(f\"Starting Epoch {epoch}\")\n",
        "  for batch_index, (data_batch, _) in enumerate(train_loader):\n",
        "    #print(data_batch.shape)\n",
        "    data_batch = patch_batch(data_batch, NUMBER_OF_FRAMES)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    masked_input, mask_idxs = mask_input_batch(data_batch)\n",
        "    print(masked_input.shape)\n",
        "    output = model(masked_input)\n",
        "    loss = calculate_loss_masked(data_batch, output, mask_idxs, True)\n",
        "    loss.backward()\n",
        "    #torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optimizer.step()\n",
        "\n",
        "    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR: {}'.format(epoch, batch_index * len(data_batch), len(train_loader.dataset),100. * batch_index / len(train_loader), loss.item(), LEARNING_RATE))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_lDE1p4m1h2",
        "outputId": "b7016604-03c0-47c1-e554-05af6dbb9e13"
      },
      "source": [
        "transformer.to(device)\n",
        "transformer.train() #mode\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  train(transformer, LEARNING_RATE, optimizer, scheduler, epoch)\n",
        "  scheduler.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Epoch 1\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [0/6000 (0%)]\tLoss: 231.616592\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [16/6000 (0%)]\tLoss: 204.081375\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [32/6000 (1%)]\tLoss: 150.286194\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [48/6000 (1%)]\tLoss: 131.832550\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [64/6000 (1%)]\tLoss: 115.452507\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [80/6000 (1%)]\tLoss: 105.814751\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [96/6000 (2%)]\tLoss: 105.144760\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [112/6000 (2%)]\tLoss: 102.879898\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [128/6000 (2%)]\tLoss: 93.196457\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [144/6000 (2%)]\tLoss: 87.621056\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [160/6000 (3%)]\tLoss: 85.778557\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [176/6000 (3%)]\tLoss: 87.982971\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [192/6000 (3%)]\tLoss: 83.791519\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [208/6000 (3%)]\tLoss: 81.102890\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [224/6000 (4%)]\tLoss: 78.349289\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [240/6000 (4%)]\tLoss: 72.582520\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [256/6000 (4%)]\tLoss: 71.567574\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [272/6000 (5%)]\tLoss: 74.012764\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [288/6000 (5%)]\tLoss: 73.948242\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [304/6000 (5%)]\tLoss: 73.817047\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [320/6000 (5%)]\tLoss: 68.489731\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [336/6000 (6%)]\tLoss: 67.330063\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [352/6000 (6%)]\tLoss: 66.676476\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [368/6000 (6%)]\tLoss: 67.131783\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [384/6000 (6%)]\tLoss: 67.387627\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [400/6000 (7%)]\tLoss: 62.950752\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [416/6000 (7%)]\tLoss: 68.943672\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [432/6000 (7%)]\tLoss: 60.835865\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [448/6000 (7%)]\tLoss: 66.057739\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [464/6000 (8%)]\tLoss: 60.589096\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [480/6000 (8%)]\tLoss: 62.601006\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [496/6000 (8%)]\tLoss: 59.634033\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [512/6000 (9%)]\tLoss: 64.589760\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [528/6000 (9%)]\tLoss: 56.385853\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [544/6000 (9%)]\tLoss: 57.706356\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [560/6000 (9%)]\tLoss: 55.760490\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [576/6000 (10%)]\tLoss: 56.763687\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [592/6000 (10%)]\tLoss: 62.555092\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [608/6000 (10%)]\tLoss: 62.131142\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [624/6000 (10%)]\tLoss: 59.455032\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [640/6000 (11%)]\tLoss: 55.742599\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [656/6000 (11%)]\tLoss: 52.289410\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [672/6000 (11%)]\tLoss: 55.407833\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [688/6000 (11%)]\tLoss: 51.697693\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [704/6000 (12%)]\tLoss: 57.589424\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [720/6000 (12%)]\tLoss: 57.244804\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [736/6000 (12%)]\tLoss: 52.677498\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [752/6000 (13%)]\tLoss: 50.120247\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [768/6000 (13%)]\tLoss: 51.693230\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [784/6000 (13%)]\tLoss: 54.618858\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [800/6000 (13%)]\tLoss: 50.993828\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [816/6000 (14%)]\tLoss: 53.253365\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [832/6000 (14%)]\tLoss: 52.355247\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [848/6000 (14%)]\tLoss: 53.618084\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [864/6000 (14%)]\tLoss: 52.299419\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [880/6000 (15%)]\tLoss: 51.300800\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [896/6000 (15%)]\tLoss: 47.882980\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [912/6000 (15%)]\tLoss: 47.569901\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [928/6000 (15%)]\tLoss: 45.765156\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [944/6000 (16%)]\tLoss: 49.804573\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [960/6000 (16%)]\tLoss: 49.682621\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [976/6000 (16%)]\tLoss: 48.467804\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [992/6000 (17%)]\tLoss: 45.846035\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1008/6000 (17%)]\tLoss: 47.245014\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1024/6000 (17%)]\tLoss: 46.832977\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1040/6000 (17%)]\tLoss: 46.408203\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1056/6000 (18%)]\tLoss: 48.143799\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1072/6000 (18%)]\tLoss: 46.304558\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1088/6000 (18%)]\tLoss: 49.586182\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1104/6000 (18%)]\tLoss: 44.520458\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1120/6000 (19%)]\tLoss: 46.732777\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1136/6000 (19%)]\tLoss: 44.794231\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1152/6000 (19%)]\tLoss: 48.245663\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1168/6000 (19%)]\tLoss: 49.434063\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1184/6000 (20%)]\tLoss: 47.458118\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1200/6000 (20%)]\tLoss: 45.374542\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1216/6000 (20%)]\tLoss: 46.278671\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1232/6000 (21%)]\tLoss: 44.571033\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1248/6000 (21%)]\tLoss: 48.698112\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1264/6000 (21%)]\tLoss: 45.637550\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1280/6000 (21%)]\tLoss: 45.876816\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1296/6000 (22%)]\tLoss: 46.086956\tLR: 0.0001\n",
            "torch.Size([16, 22, 256])\n",
            "torch.Size([16, 22, 256])\n",
            "Train Epoch: 1 [1312/6000 (22%)]\tLoss: 43.885971\tLR: 0.0001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ljNdSSLFukl"
      },
      "source": [
        "def save_model(model, name):\n",
        "  name += '.pth'\n",
        "  torch.save(model, '/content/drive/My Drive/models/transformers/' + name)\n",
        "  return name\n",
        "\n",
        "def load_model(name):\n",
        "  model = torch.load('/content/drive/My Drive/models/transformers/' + name)\n",
        "  return model\n",
        "\n",
        "MODEL_NAME = 'transformer_01_'.join(NORMAL_CLASSES)\n",
        "name = save_model(autoencoder, MODEL_NAME)\n",
        "#model = load_model('transformer_01.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2NudZ295rdD"
      },
      "source": [
        "def evaluate(model, val_loader, device, number_of_frames):\n",
        "  anom_scores = []\n",
        "  targets = []\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for _, data in enumerate(val_loader, 0):\n",
        "      inputs, targets = data\n",
        "      inputs = patch_batch(inputs, number_of_frames)\n",
        "      inputs, mask_idxs = mask_input_batch(inputs)\n",
        "\n",
        "      outputs = model(inputs)\n",
        "      loss\n",
        "      \n",
        "      pixelwise_anom_scores_per_patch = nn.functional.mse_loss(inputs, outputs, size_average=None, reduce=None, reduction='none') #reconstrucion error(pixelwise) for each patch\n",
        "      #try to do that with axes!\n",
        "      anom_scores_per_patch = [torch.sum(x).cpu().numpy() for x in pixelwise_anom_scores_per_patch] #sum up reconstruction errors for each patch\n",
        "      anom_score_total = sum(anom_scores_per_patch) / len(anom_scores_per_patch) #calculate reconstruction error for total spectogram\n",
        "      #print(anom_scores_total)\n",
        "      batch_targets = [x.cpu().numpy() for x in batch_targets] #convert to numpy for processig with scikit\n",
        "      anom_scores.append(anom_score_total)\n",
        "      targets += batch_targets\n",
        "    return anom_scores, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9A6rZApGWcd"
      },
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import softmax\n",
        "import time\n",
        "current_time = time.asctime( time.localtime(time.time()) )\n",
        "\n",
        "fp_rate, tp_rate, _ = roc_curve(targets, anom_scores, pos_label=1)\n",
        "roc_auc = roc_auc_score(targets, anom_scores)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fp_rate, tp_rate, color='blue', label=f\"ROC_AUC ={roc_auc}\")\n",
        "\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve of ' + MODEL_NAME +' with normal Classes: '.join(NORMAL_CLASSES))\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('/content/drive/My Drive/models/transformers/roc_graph' + MODEL_NAME.join(NORMAL_CLASSES) + \"_\" + str(roc_auc) + \".jpg\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnqcTXZJGreX"
      },
      "source": [
        "import pytorch_model_summary as pms\n",
        "shape = first_sample.shape\n",
        "summary = pms.summary(transformer, torch.ones((shape[0], shape[1], shape[2])).to(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC950s26HEkX"
      },
      "source": [
        "print(\"Traininig finished! Saving Config...\")\n",
        "from google.colab import files\n",
        "\n",
        "#save hyperparams and result:\n",
        "with open('/content/drive/My Drive/models/transformers/hyper_params' + MODEL_NAME + '.txt', 'w') as f:\n",
        "  f.write(f\"Epochs: {EPOCHS}, LR: {LEARNING_RATE}, BatchSize: {BATCH_SIZE}, SAMPLE_RATE = {SAMPLE_RATE}, N_FFT/WINDOW_SIZE = {N_FFT}, HOP_LENGTH = {HOP_LENGTH}, N_MELS = {N_MELS}\" \n",
        "          + f\"NUMBER_OF_FRAMES: {NUMBER_OF_FRAMES}, EMBEDDING_SIZE = {EMBEDDING_SIZE}, N_HEADS = {N_HEADS}, N_ENCODER_LAYERS = {N_ENCODER_LAYERS}, DROPOUT = {DROPOUT}, DIM_FEED_FORWARD = {DIM_FEED_FORWARD}\"\n",
        "  + f\"\\nNormal Classes: {NORMAL_CLASSES}, Anomalous Classes: {ANOMALOUS_CLASSES}, ROC_AUC Score: {roc_auc} + \\n\\n {summary}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}