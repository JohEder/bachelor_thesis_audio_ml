{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+cf9bETPSRe1IOvCsxtl7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohEder/bachelor_thesis_audio_ml/blob/master/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIBl-ybmvCJ7",
        "outputId": "b7810497-3d51-4fba-e7d9-6fb53e63ef69"
      },
      "source": [
        "!pip install torchaudio\n",
        "!pip install pytorch-model-summary\n",
        "!pip install -q git+https://github.com/huggingface/transformers"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchaudio) (3.7.4.3)\n",
            "Requirement already satisfied: pytorch-model-summary in /usr/local/lib/python3.7/dist-packages (0.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-model-summary) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-model-summary) (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch-model-summary) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-model-summary) (3.7.4.3)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdn43FFt6jp0"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import torchaudio\n",
        "import pandas as pd\n",
        "import os\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "import math\n",
        "import time\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import softmax\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import datetime\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm9hEziO6l5F"
      },
      "source": [
        "#just copied the official import script for the dataset, custom preprocessing happens afterwards\n",
        "\"\"\" Import script for IDMT-Traffic dataset\n",
        "Ref:\n",
        "    J. Abeßer, S. Gourishetti, A. Kátai, T. Clauß, P. Sharma, J. Liebetrau: IDMT-Traffic: An Open Benchmark\n",
        "    Dataset for Acoustic Traffic Monitoring Research, EUSIPCO, 2021\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "__author__ = 'Jakob Abeßer (jakob.abesser@idmt.fraunhofer.de)'\n",
        "\n",
        "\n",
        "def import_idmt_traffic_dataset(fn_txt: str = \"idmt_traffic_all\") -> pd.DataFrame:\n",
        "    \"\"\" Import IDMT-Traffic dataset\n",
        "    Args:\n",
        "        fn_txt (str): Text file with all WAV files\n",
        "    Returns:\n",
        "        df_dataset (pd.Dataframe): File-wise metadata\n",
        "            Columns:\n",
        "                'file': WAV filename,\n",
        "                'is_background': True if recording contains background noise (no vehicle), False else\n",
        "                'date_time': Recording time (YYYY-MM-DD-HH-mm)\n",
        "                'location': Recording location\n",
        "                'speed_kmh': Speed limit at recording site (km/h), UNK if unknown,\n",
        "                'sample_pos': Sample position (centered) within the original audio recording,\n",
        "                'daytime': M(orning) or (A)fternoon,\n",
        "                'weather': (D)ry or (W)et road condition,\n",
        "                'vehicle': (B)us, (C)ar, (M)otorcycle, or (T)ruck,\n",
        "                'source_direction': Source direction of passing vehicle: from (L)eft or from (R)ight,\n",
        "                'microphone': (SE)= (high-quality) sE8 microphones, (ME) = (low-quality) MEMS microphones (ICS-43434),\n",
        "                'channel': Original stereo pair channel (12) or (34)\n",
        "    \"\"\"\n",
        "    # load file list\n",
        "    df_files = pd.read_csv(fn_txt, names=('file',))\n",
        "    fn_file_list = df_files['file'].to_list()\n",
        "\n",
        "    # load metadata from file names\n",
        "    df_dataset = []\n",
        "\n",
        "    for f, fn in enumerate(fn_file_list):\n",
        "        fn = fn.replace('.wav', '')\n",
        "        parts = fn.split('_')\n",
        "\n",
        "        # background noise files\n",
        "        if '-BG' in fn:\n",
        "            date_time, location, speed_kmh, sample_pos, mic, channel = parts\n",
        "            vehicle, source_direction, weather, daytime = 'None', 'None', 'None', 'None'\n",
        "            is_background = True\n",
        "\n",
        "        # files with vehicle passings\n",
        "        else:\n",
        "            date_time, location, speed_kmh, sample_pos, daytime, weather, vehicle_direction, mic, channel = parts\n",
        "            vehicle, source_direction = vehicle_direction\n",
        "            is_background = False\n",
        "\n",
        "        channel = channel.replace('-BG', '')\n",
        "        speed_kmh = speed_kmh.replace('unknownKmh', 'UNK')\n",
        "        speed_kmh = speed_kmh.replace('Kmh', '')\n",
        "\n",
        "        df_dataset.append({'file': fn,\n",
        "                           'is_background': is_background,\n",
        "                           'date_time': date_time,\n",
        "                           'location': location,\n",
        "                           'speed_kmh': speed_kmh,\n",
        "                           'sample_pos': sample_pos,\n",
        "                           'daytime': daytime,\n",
        "                           'weather': weather,\n",
        "                           'vehicle': vehicle,\n",
        "                           'source_direction': source_direction,\n",
        "                           'microphone': mic,\n",
        "                           'channel': channel})\n",
        "\n",
        "    df_dataset = pd.DataFrame(df_dataset, columns=('file', 'is_background', 'date_time', 'location', 'speed_kmh', 'sample_pos', 'daytime', 'weather', 'vehicle',\n",
        "                                                   'source_direction', 'microphone', 'channel'))\n",
        "\n",
        "    return df_dataset"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53-c6jdM6uLg"
      },
      "source": [
        "\"\"\"\n",
        "Anomalous Sound Transformer Model for my Bachelor Thesis\n",
        "\"\"\"\n",
        "\n",
        "__author__ = 'Johannes Eder (Jo.Eder@campus.lmu.de)'\n",
        "\n",
        "#print(len(all_data[all_data.is_background])) #8144 -> #9362 labbelled background sounds\n",
        "#print(len(all_data[all_data.vehicle == 'C'])) #7804\n",
        "#print(len(all_data[all_data.vehicle == 'M'])) #430\n",
        "#print(len(all_data[all_data.vehicle == 'T'])) #1022\n",
        "#print(len(all_data[all_data.vehicle == 'B'])) #106"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAnBzNvl7V9Q",
        "outputId": "4900e9fd-1d01-4b25-a71a-c155d53a48b3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gArrnoTR7aiz"
      },
      "source": [
        "CLASSES = ['None','C','T', 'M', 'B'] #Background Noise, Car, Truck, Motorcycle, Bus\n",
        "NORMAL_CLASSES = ['None', 'C']\n",
        "ANOMALOUS_CLASSES = ['T', 'M', 'B']\n",
        "\n",
        "SAMPLE_RATE = 22500\n",
        "N_FFT=2048 #is also window size\n",
        "HOP_LENGTH=1024\n",
        "N_MELS=128\n",
        "NUMBER_OF_FRAMES = 2\n",
        "melspectogram = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=SAMPLE_RATE,\n",
        "        n_fft=N_FFT, # Frame Size\n",
        "        hop_length=HOP_LENGTH, #here half the frame size\n",
        "        n_mels=N_MELS\n",
        "    )\n",
        "\n",
        "transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(mode='L'),\n",
        "    #transforms.Grayscale(num_output_channels=3),\n",
        "    #transforms.Resize([224, 224]),\n",
        "    #transforms.RandomCrop(size=[N_MELS, NUMBER_OF_FRAMES]), #only train on random slice of the spectogram\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "AUDIO_DIR = \"/content/drive/My Drive/datasets/IDMT_Traffic/audio\"\n",
        "train_annotations = \"/content/drive/My Drive/datasets/IDMT_Traffic/annotation/eusipco_2021_train.csv\"\n",
        "test_annotatons = \"/content/drive/My Drive/datasets/IDMT_Traffic/annotation/eusipco_2021_test.csv\"\n",
        "all_annotations_txt = \"/content/drive/My Drive/datasets/IDMT_Traffic/annotation/idmt_traffic_all.txt\"\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "BATCH_SIZE_VAL = 1\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "\n",
        "EMBEDDING_SIZE = 128\n",
        "N_HEADS = 4\n",
        "N_ENCODER_LAYERS = 4\n",
        "DROPOUT = 0.0 #is dropout needed for AD? used in most Transformer papers\n",
        "DIM_FEED_FORWARD = 256\n",
        "input_dim = 256\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjg39EMZ75O1"
      },
      "source": [
        "class IdmtTrafficDataSet(Dataset):\n",
        "\n",
        "    def __init__(self, annotations_file, audio_dir, audio_transformation, transformation, target_sample_rate, normal_classes):\n",
        "        self.annotations =  annotations_file if isinstance(annotations_file, pd.DataFrame) else pd.read_csv(annotations_file)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.audio_transformation = audio_transformation\n",
        "        self.transformation = transformation\n",
        "        self.target_sample_rate = target_sample_rate\n",
        "        #self.classes = ['None','C','T', 'M', 'B']\n",
        "        self.normal_classes = normal_classes\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        audio_sample_path = self._get_audio_sample_path(index)\n",
        "        label = self._get_audio_sample_label(index)\n",
        "        signal, sr = torchaudio.load(audio_sample_path)\n",
        "        signal = self._resample(signal, sr) #adjust sample rates\n",
        "        # signal -> (num_channels, samples) i.e. (2, 16000)\n",
        "        signal  = self._mix_down(signal) #stereo to mono\n",
        "        signal = self.audio_transformation(signal) #(1, 16000) -> torch.Size([1, 64, 63])\n",
        "        signal = self.transformation(signal)\n",
        "        #label = self.normal_classes.index(label)\n",
        "        label = 0 if label in self.normal_classes else 1\n",
        "        return signal, label\n",
        "\n",
        "    def _resample(self, signal, sr):\n",
        "        if sr != self.target_sample_rate:\n",
        "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
        "            signal = resampler(signal)\n",
        "        return signal\n",
        "    \n",
        "    def _mix_down(self, signal):\n",
        "        if signal.shape[0] > 1: #(2, 16000)\n",
        "            #mean operation: aggregating multiple channels\n",
        "            signal = torch.mean(signal, 0, True)\n",
        "        return signal\n",
        "\n",
        "    def _get_audio_sample_path(self, index):\n",
        "        path = os.path.join(self.audio_dir, self.annotations.iloc[index, 0])\n",
        "        return path + '.wav'\n",
        "\n",
        "    def _get_audio_sample_label(self, index):\n",
        "        return self.annotations.iloc[index, 9]"
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf1bYw0c8Qe6"
      },
      "source": [
        "def get_normal_and_anomalous_data(normal_classes, anomalous_classes, audio_dir, annotations, batch_size):\n",
        "    if len((set(normal_classes) & set(anomalous_classes))) > 0:\n",
        "      raise Exception(\"Intersection between normal and anomalous classes should be empty!\")\n",
        "\n",
        "    all_data = import_idmt_traffic_dataset(annotations)\n",
        "\n",
        "    normal_data = all_data[all_data.vehicle.isin(normal_classes)]\n",
        "    anomalous_data = all_data[all_data.vehicle.isin(anomalous_classes)]\n",
        "\n",
        "    train_data, test_data_normal = train_test_split(normal_data, test_size=0.1, shuffle=False)\n",
        "    train_data = adjust_sample_number_to_batch_size(train_data, batch_size)\n",
        "    print(f\"training with {len(train_data)} (normal) samples\")\n",
        "\n",
        "    number_of_normal_test_sampels = len(test_data_normal)\n",
        "    print(f\"testing with {number_of_normal_test_sampels} normal samples\")\n",
        "\n",
        "    #sample same number of anomalous data to test\n",
        "    number_anomlous = number_of_normal_test_sampels if number_of_normal_test_sampels < len(anomalous_data) else len(anomalous_data)\n",
        "    anomalous_data = anomalous_data.sample(number_anomlous)\n",
        "    print(f\"testing with {len(anomalous_data)} anomalous samples\")\n",
        "\n",
        "    frames = [anomalous_data, test_data_normal]\n",
        "    concatenated_test_data = pd.concat(frames)\n",
        "    concatenated_test_data.reset_index(drop=True, inplace=True)\n",
        "    concatenated_test_data = adjust_sample_number_to_batch_size(concatenated_test_data, batch_size)\n",
        "\n",
        "    normal_train_data = IdmtTrafficDataSet(train_data, audio_dir, melspectogram, transforms, SAMPLE_RATE, normal_classes)\n",
        "    test_data = IdmtTrafficDataSet(concatenated_test_data, audio_dir, melspectogram, transforms, SAMPLE_RATE, normal_classes)\n",
        "\n",
        "    return normal_train_data, test_data\n",
        "\n",
        "def adjust_sample_number_to_batch_size(data, batch_size):\n",
        "  if len(data) % batch_size == 0:\n",
        "    print(\"no data discarded.\")\n",
        "    return data\n",
        "  else:\n",
        "    remainder = len(data) % batch_size\n",
        "    print(str(remainder + 1) + \" samples discarded.\")\n",
        "    return data.iloc[remainder + 1:,:]"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwLjzSG0HkIC"
      },
      "source": [
        "class TransformerModel(nn.Module):\n",
        "  def __init__(self, d_model, input_dim, n_heads, dim_feedforward, n_encoder_layers, dropout=0.5):\n",
        "    super(TransformerModel, self).__init__()\n",
        "    self.model_type = 'Transformer'\n",
        "    self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "    encoder_layers = TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "    self.transformer_encoder = TransformerEncoder(encoder_layers, n_encoder_layers)\n",
        "    self.patch_embedding = PatchEmbedding(input_dim, d_model)\n",
        "    self.input_dim = input_dim\n",
        "    self.d_model = d_model\n",
        "    self.decoder = Decoder(d_model, input_dim)\n",
        "\n",
        "    self.mask_token = nn.Parameter(torch.randn(d_model, requires_grad=True))\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    initrange = 0.1\n",
        "    #self.patch_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "    #self.decoder.bias.data.zero_()\n",
        "    #self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "  \n",
        "  def forward(self, input, mask_index=None):\n",
        "    embedded = self.patch_embedding(input) #* math.sqrt(self.input_dim) #is scaling necessary? yes, otherwise values are incredibly small\n",
        "    embedded_masked, mask_idxs = self.mask_embedded_tokens(embedded)\n",
        "    pos_encoded_embedded = self.pos_encoder(embedded_masked)\n",
        "    transformer_out = self.transformer_encoder(pos_encoded_embedded)\n",
        "    output = self.decoder(transformer_out)\n",
        "    return output, mask_idxs\n",
        "\n",
        "  def mask_embedded_tokens(self, input, specific_mask_idx=None):\n",
        "    if specific_mask_idx != None:\n",
        "      assert specific_mask_idx < input.shape[1]\n",
        "    number_of_specs = input.shape[0]\n",
        "    input_masked = []\n",
        "    masks_index_list = []\n",
        "    for i in range(number_of_specs):\n",
        "      mask_idx = specific_mask_idx if specific_mask_idx != None else random.randint(0, input.shape[1]-1)\n",
        "\n",
        "      input[i, mask_idx, :] = self.mask_token\n",
        "\n",
        "      input_masked.append(input[i,:,:]) #maybe just tuples (current_spec_masked, mask_idx)\n",
        "      masks_index_list.append(torch.as_tensor(mask_idx))\n",
        "\n",
        "    return torch.stack(input_masked), torch.stack(masks_index_list)"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLVUdkvh1zBH"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, transformer_out, out_put_total):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.input_dim = transformer_out\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(in_features=transformer_out, out_features=transformer_out),  #evtl 2*d_model\n",
        "        nn.GELU(),\n",
        "        nn.Linear(in_features=transformer_out, out_features=out_put_total),)\n",
        "    \n",
        "  def forward(self, input):\n",
        "    x = self.mlp(input)\n",
        "    return x"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be9tNwH7OyRC"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, embedding_dim, dropout=0.1, max_len=5000):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    pe = torch.zeros(max_len, embedding_dim)\n",
        "    #print(f\"Shape: {pe.shape}\")\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    #print(f\"Position shape: {position.shape}\")\n",
        "    div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "    self.register_buffer('pe', pe)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = x + self.pe[:x.size(0), :]\n",
        "    return self.dropout(x)"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s468t9EyVgkm"
      },
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self, input_dim, embedding_dimension):\n",
        "    super().__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.embedding_layer = nn.Linear(input_dim, embedding_dimension)\n",
        "  \n",
        "  def forward(self, input_data):\n",
        "    embedding = self.embedding_layer(input_data)\n",
        "    return embedding"
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar2Mdd-ZpjuB"
      },
      "source": [
        "def patch_batch(input_batch, number_of_frames):\n",
        "  #input of shape (batch_size, channels, mel_filters, frames)\n",
        "  unfold = nn.Unfold(kernel_size=(input_batch.shape[2], NUMBER_OF_FRAMES), stride=NUMBER_OF_FRAMES) #patching the spectogram\n",
        "  unfolded_batch = unfold(input_batch) #(batch_size, features, number_of_patches)\n",
        "  unfolded_batch = unfolded_batch.transpose(1, 2) #(batch_size, number_of_patches, features)\n",
        "  return unfolded_batch"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtmeWmAkFEM_"
      },
      "source": [
        "def plot_samples(samples):\n",
        "  fig = plt.figure(figsize=(100, 400))\n",
        "  print(len(samples))\n",
        "  for i in range(len(samples)):\n",
        "    sample, label = samples[i]\n",
        "    sample = sample.squeeze()\n",
        "    fig.add_subplot(1, len(samples), i+1)\n",
        "    plt.axis('off')\n",
        "    plt.title('normal' if label==0 else 'anomalous')\n",
        "    plt.imshow(sample)\n",
        "  plt.show()"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D68e7futiiU",
        "outputId": "5b8cb0ef-f310-47e8-ca22-f974379eeab7"
      },
      "source": [
        "train_data, test_data = get_normal_and_anomalous_data(NORMAL_CLASSES, ANOMALOUS_CLASSES, audio_dir=AUDIO_DIR, annotations=all_annotations_txt, batch_size=BATCH_SIZE)\n",
        "first_sample, first_label = train_data[0]\n",
        "samples_to_plot = [ test_data[i] for i in range(0,len(test_data), 100) ] #plotte jedes 100. test sampel damit sowohl anomalien als auch normale daten dabei sind\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE_VAL, shuffle=True)"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18 samples discarded.\n",
            "training with 14335 (normal) samples\n",
            "testing with 1595 normal samples\n",
            "testing with 1558 anomalous samples\n",
            "82 samples discarded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nONMifHzKeOt"
      },
      "source": [
        "plot_samples(samples_to_plot)"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79YOWc_meQ1R"
      },
      "source": [
        "transformer = TransformerModel(EMBEDDING_SIZE, input_dim, N_HEADS, DIM_FEED_FORWARD, N_ENCODER_LAYERS)"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JahOBDTmNt4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "368d86a6-4182-4114-dd1e-eb12bd118bdd"
      },
      "source": [
        "\"\"\"old\n",
        "def mask_input_batch(input, device, specific_mask_idx=None):\n",
        "  if specific_mask_idx != None:\n",
        "    assert specific_mask_idx < input.shape[1]\n",
        "  number_of_specs = input.shape[0]\n",
        "  input_masked = []\n",
        "  masks_index_list = []\n",
        "  for i in range(number_of_specs):\n",
        "    mask_idx = specific_mask_idx if specific_mask_idx != None else random.randint(0, input.shape[1]-1)\n",
        "    mask = torch.ones(input.shape[1], input.shape[2])\n",
        "    mask[mask_idx, :] = 0\n",
        "    current_spec_masked = input[i, : , :].mul(mask)\n",
        "\n",
        "    input_masked.append(current_spec_masked) #maybe just tuples (current_spec_masked, mask_idx)\n",
        "    masks_index_list.append(torch.as_tensor(mask_idx))\n",
        "\n",
        "  return torch.stack(input_masked), torch.stack(masks_index_list)\n",
        "\"\"\""
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'old\\ndef mask_input_batch(input, device, specific_mask_idx=None):\\n  if specific_mask_idx != None:\\n    assert specific_mask_idx < input.shape[1]\\n  number_of_specs = input.shape[0]\\n  input_masked = []\\n  masks_index_list = []\\n  for i in range(number_of_specs):\\n    mask_idx = specific_mask_idx if specific_mask_idx != None else random.randint(0, input.shape[1]-1)\\n    mask = torch.ones(input.shape[1], input.shape[2])\\n    mask[mask_idx, :] = 0\\n    current_spec_masked = input[i, : , :].mul(mask)\\n\\n    input_masked.append(current_spec_masked) #maybe just tuples (current_spec_masked, mask_idx)\\n    masks_index_list.append(torch.as_tensor(mask_idx))\\n\\n  return torch.stack(input_masked), torch.stack(masks_index_list)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "057KTyNylH8s"
      },
      "source": [
        "def calculate_loss_masked(input_batch, output_batch, mask_idxs, sum_up):\n",
        "  #print(input_batch.shape)\n",
        "  loss_per_batch = 0\n",
        "  for i in range(len(mask_idxs)):\n",
        "    input_at_masked = input_batch[i, mask_idxs[i], :]\n",
        "    output_at_masked = output_batch[i, mask_idxs[i], :]\n",
        "    scores = torch.sum((input_at_masked - output_at_masked) ** 2, dim=tuple(range(1, output_at_masked.dim()))) #brauche ich die dimension? ist glaube ich immer 1\n",
        "    loss_per_batch += scores\n",
        "  return loss_per_batch.mean()\n",
        "\n",
        "\n",
        "def calculate_loss_total(input, output):\n",
        "  loss = nn.MSELoss();\n",
        "  return loss(input, output)"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "phUe045ofcZf",
        "outputId": "02bd6701-1112-435f-9487-f1c4c3c76c56"
      },
      "source": [
        "\"\"\"tryout\n",
        "input_test = torch.rand(64, 22, 256) #22 frames\n",
        "output, mask_idxs = transformer(input_test\n",
        "print(output.shape)\n",
        "print(output[0, mask_idxs[0],:]) #mask token\n",
        "print(mask_idxs[0])\n",
        "loss= calculate_loss_masked(input_test, output, mask_idxs, True)\n",
        "print(loss_per_batch)\n",
        "\"\"\""
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'tryout\\ninput_test = torch.rand(64, 22, 256) #22 frames\\noutput, mask_idxs = transformer(input_test\\nprint(output.shape)\\nprint(output[0, mask_idxs[0],:]) #mask token\\nprint(mask_idxs[0])\\nloss= calculate_loss_masked(input_test, output, mask_idxs, True)\\nprint(loss_per_batch)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC49Nt07fofv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f7bab46-9c87-4ef8-925d-6b16153d77ed"
      },
      "source": [
        "LEARNING_RATE = 0.00005\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=LEARNING_RATE) \n",
        "EPOCHS = 50 #later over hundred\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "warm_up_steps = math.ceil(total_steps * 0.1)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, warm_up_steps, total_steps)\n",
        "print(f\"Total Steps: {total_steps}, Warm up steps: {warm_up_steps}, Ratio: {warm_up_steps / total_steps}\")\n",
        "def train(model, optimizer, scheduler, epoch, device):\n",
        "  print(f\"Starting Epoch {epoch}\")\n",
        "  epoch_loss = []\n",
        "  for batch_index, (data_batch, _) in enumerate(train_loader):\n",
        "    #print(data_batch.shape)\n",
        "    data_batch = patch_batch(data_batch, NUMBER_OF_FRAMES)\n",
        "    data_batch = data_batch.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output, mask_idxs = model(data_batch)\n",
        "    loss = calculate_loss_masked(data_batch, output, mask_idxs, True)\n",
        "    #loss_total = calculate_loss_total(data_batch, output)\n",
        "    #print(f\"Loss patches: {loss}\\nLoss total: {loss_total}\")\n",
        "    epoch_loss.append(loss.item())\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optimizer.step()\n",
        "    current_lr = scheduler.get_last_lr()\n",
        "    scheduler.step()\n",
        "    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR: {}'.format(epoch, batch_index * len(data_batch), len(train_loader.dataset),100. * batch_index / len(train_loader), loss.item(), current_lr))\n",
        "  return epoch_loss\n"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Steps: 2800, Warm up steps: 280, Ratio: 0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2NudZ295rdD"
      },
      "source": [
        "def evaluate(model, val_loader, device, number_of_frames, number_of_batches_eval=None):\n",
        "  #currently the batch size for evaluation needs to be 1\n",
        "  total_anom_scores = []\n",
        "  total_targets = []\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch_number, data in enumerate(val_loader, 0):\n",
        "      if (number_of_batches_eval != None) and (batch_number > number_of_batches_eval):\n",
        "        break\n",
        "      if (batch_number % 20 == 0):\n",
        "        print(f\"Progress: {batch_number}/{len(val_loader)}\")\n",
        "      inputs, target = data\n",
        "      inputs = inputs.to(device)\n",
        "      #print(inputs.shape)\n",
        "      inputs = patch_batch(inputs, NUMBER_OF_FRAMES)\n",
        "      #print(inputs.shape) #(n_spectograms, n_patches, features)\n",
        "      #every patch needs to be masked once and the masked loss calculated added and divided by number of patches\n",
        "      loss_total_current_spec = 0\n",
        "      for i in range(inputs.shape[1]): #iterate through patches\n",
        "        output, index = model(inputs, i) #patch i gets masked\n",
        "        #print(output)\n",
        "        #print(index)\n",
        "        loss = calculate_loss_masked(inputs, output, index, True) # last argument (sum) does not make a difference for batch size 1\n",
        "        loss_total_current_spec += loss.item()\n",
        "      \n",
        "      loss_total_current_spec /= inputs.shape[1] #divide by number of patches\n",
        "      #print(loss_total_current_spec)\n",
        "      total_anom_scores.append(loss_total_current_spec) #coverting to numpy for processing with scikit\n",
        "      total_targets.append(target)\n",
        "    return total_anom_scores, total_targets"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "717_tKu_A3z9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "e5f59e7e-8941-421c-fc73-abbe22f54ae8"
      },
      "source": [
        "\"\"\"old\n",
        "def evaluate_one_index(model, val_loader, device):\n",
        "  total_anom_scores = []\n",
        "  total_targets = []\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for _, data in enumerate(val_loader, 0):\n",
        "      inputs, batch_targets = data\n",
        "      #print(inputs.shape)\n",
        "      inputs = inputs.to(device)\n",
        "      inputs = patch_batch(inputs, NUMBER_OF_FRAMES)\n",
        "      #print(inputs.shape) #(n_spectograms, n_patches, features)\n",
        "      masked_input, mask_idxs = mask_input_batch(inputs,device, 10) #calculate mask for every spectogram in the batch at index\n",
        "      outputs = model(masked_input) #(n_spectograms, n_patches_reconstruces, features)\n",
        "      batch_anom_scores = calculate_loss_masked(inputs, outputs, mask_idxs, True)\n",
        "      print(batch_anom_scores)\n",
        "\n",
        "      total_anom_scores.append(batch_anom_scores.cpu().numpy()) #coverting to numpy for processing with scikit\n",
        "      total_targets += [x.cpu().numpy() for x in batch_targets]\n",
        "    return total_anom_scores, total_targets\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'old\\ndef evaluate_one_index(model, val_loader, device):\\n  total_anom_scores = []\\n  total_targets = []\\n  model.to(device)\\n  model.eval()\\n  with torch.no_grad():\\n    for _, data in enumerate(val_loader, 0):\\n      inputs, batch_targets = data\\n      #print(inputs.shape)\\n      inputs = inputs.to(device)\\n      inputs = patch_batch(inputs, NUMBER_OF_FRAMES)\\n      #print(inputs.shape) #(n_spectograms, n_patches, features)\\n      masked_input, mask_idxs = mask_input_batch(inputs,device, 10) #calculate mask for every spectogram in the batch at index\\n      outputs = model(masked_input) #(n_spectograms, n_patches_reconstruces, features)\\n      batch_anom_scores = calculate_loss_masked(inputs, outputs, mask_idxs, True)\\n      print(batch_anom_scores)\\n\\n      total_anom_scores.append(batch_anom_scores.cpu().numpy()) #coverting to numpy for processing with scikit\\n      total_targets += [x.cpu().numpy() for x in batch_targets]\\n    return total_anom_scores, total_targets\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ljNdSSLFukl"
      },
      "source": [
        "model_name = 'transformer_05_grad_clip_no_dropout_lin_sched_adam_bigger_batch_'\n",
        "def save_model(model, name, epoch):\n",
        "  global model_name\n",
        "  model_name += '_' + str(epoch)\n",
        "  name = model_name + '.pth'\n",
        "  torch.save(model, '/content/drive/My Drive/models/transformers/' + name)\n",
        "  return name\n",
        "\n",
        "def load_model(name):\n",
        "  name +='.pth'\n",
        "  model = torch.load('/content/drive/My Drive/models/transformers/' + name)\n",
        "  return model\n"
      ],
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_lDE1p4m1h2",
        "outputId": "2caf03ae-2ccf-41ed-b648-369ae2dcabf3"
      },
      "source": [
        "transformer.to(device)\n",
        "transformer.train() #mode\n",
        "roc_auc_best = 0\n",
        "training_start = datetime.datetime.now()\n",
        "losses = []\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  losses_epoch = train(transformer, optimizer, scheduler, epoch, device)\n",
        "  val_anom_scores, val_targets = evaluate(transformer, test_loader, device, NUMBER_OF_FRAMES, number_of_batches_eval=100) #batch size in evalution is only one\n",
        "  roc_auc = roc_auc_score(val_targets, val_anom_scores)\n",
        "  losses += losses_epoch\n",
        "  if roc_auc > roc_auc_best:\n",
        "    save_model(transformer, model_name, epoch)\n",
        "    roc_auc_best = roc_auc\n",
        "    print(f\"saved model with best validaton in epoch{epoch}\")\n",
        "  print(f\"Evaluation ROC Score in epoch {epoch} is {roc_auc}, Best ROC Score is:{roc_auc_best}\")\n",
        "\n",
        "  if epoch == EPOCHS+1:\n",
        "    save_model(transformer, model_name, epoch)\n",
        "    print(f\"Saved Model in last epoch with validation {roc_auc}\")\n",
        "\n",
        "training_finished = datetime.datetime.now()\n",
        "print(f\"Total Training Time: {training_finished - training_start}\")"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Epoch 1\n",
            "Train Epoch: 1 [0/14335 (0%)]\tLoss: 7772.958008\tLR: [0.0]\n",
            "Train Epoch: 1 [256/14335 (2%)]\tLoss: 7165.507812\tLR: [1.7857142857142858e-07]\n",
            "Train Epoch: 1 [512/14335 (4%)]\tLoss: 6938.657715\tLR: [3.5714285714285716e-07]\n",
            "Train Epoch: 1 [768/14335 (5%)]\tLoss: 7345.861816\tLR: [5.357142857142858e-07]\n",
            "Train Epoch: 1 [1024/14335 (7%)]\tLoss: 7688.182617\tLR: [7.142857142857143e-07]\n",
            "Train Epoch: 1 [1280/14335 (9%)]\tLoss: 7431.591797\tLR: [8.928571428571428e-07]\n",
            "Train Epoch: 1 [1536/14335 (11%)]\tLoss: 6948.700684\tLR: [1.0714285714285716e-06]\n",
            "Train Epoch: 1 [1792/14335 (12%)]\tLoss: 6663.788574\tLR: [1.25e-06]\n",
            "Train Epoch: 1 [2048/14335 (14%)]\tLoss: 7359.132812\tLR: [1.4285714285714286e-06]\n",
            "Train Epoch: 1 [2304/14335 (16%)]\tLoss: 6616.388184\tLR: [1.6071428571428572e-06]\n",
            "Train Epoch: 1 [2560/14335 (18%)]\tLoss: 7126.492676\tLR: [1.7857142857142857e-06]\n",
            "Train Epoch: 1 [2816/14335 (20%)]\tLoss: 7056.443848\tLR: [1.9642857142857144e-06]\n",
            "Train Epoch: 1 [3072/14335 (21%)]\tLoss: 6773.394043\tLR: [2.142857142857143e-06]\n",
            "Train Epoch: 1 [3328/14335 (23%)]\tLoss: 7699.547363\tLR: [2.3214285714285715e-06]\n",
            "Train Epoch: 1 [3584/14335 (25%)]\tLoss: 6863.596680\tLR: [2.5e-06]\n",
            "Train Epoch: 1 [3840/14335 (27%)]\tLoss: 7643.150391\tLR: [2.6785714285714285e-06]\n",
            "Train Epoch: 1 [4096/14335 (29%)]\tLoss: 8376.039062\tLR: [2.8571428571428573e-06]\n",
            "Train Epoch: 1 [4352/14335 (30%)]\tLoss: 8158.668457\tLR: [3.035714285714286e-06]\n",
            "Train Epoch: 1 [4608/14335 (32%)]\tLoss: 7188.766602\tLR: [3.2142857142857143e-06]\n",
            "Train Epoch: 1 [4864/14335 (34%)]\tLoss: 6768.767090\tLR: [3.3928571428571426e-06]\n",
            "Train Epoch: 1 [5120/14335 (36%)]\tLoss: 7359.824219\tLR: [3.5714285714285714e-06]\n",
            "Train Epoch: 1 [5376/14335 (38%)]\tLoss: 7318.955078\tLR: [3.75e-06]\n",
            "Train Epoch: 1 [5632/14335 (39%)]\tLoss: 7183.204102\tLR: [3.928571428571429e-06]\n",
            "Train Epoch: 1 [5888/14335 (41%)]\tLoss: 7095.167969\tLR: [4.107142857142857e-06]\n",
            "Train Epoch: 1 [6144/14335 (43%)]\tLoss: 7028.146973\tLR: [4.285714285714286e-06]\n",
            "Train Epoch: 1 [6400/14335 (45%)]\tLoss: 6199.363281\tLR: [4.464285714285715e-06]\n",
            "Train Epoch: 1 [6656/14335 (46%)]\tLoss: 7459.022461\tLR: [4.642857142857143e-06]\n",
            "Train Epoch: 1 [6912/14335 (48%)]\tLoss: 6881.395996\tLR: [4.821428571428572e-06]\n",
            "Train Epoch: 1 [7168/14335 (50%)]\tLoss: 7946.953613\tLR: [5e-06]\n",
            "Train Epoch: 1 [7424/14335 (52%)]\tLoss: 6952.480469\tLR: [5.1785714285714296e-06]\n",
            "Train Epoch: 1 [7680/14335 (54%)]\tLoss: 6542.916016\tLR: [5.357142857142857e-06]\n",
            "Train Epoch: 1 [7936/14335 (55%)]\tLoss: 6675.938965\tLR: [5.535714285714285e-06]\n",
            "Train Epoch: 1 [8192/14335 (57%)]\tLoss: 6549.698242\tLR: [5.7142857142857145e-06]\n",
            "Train Epoch: 1 [8448/14335 (59%)]\tLoss: 7075.058105\tLR: [5.892857142857143e-06]\n",
            "Train Epoch: 1 [8704/14335 (61%)]\tLoss: 7642.382812\tLR: [6.071428571428572e-06]\n",
            "Train Epoch: 1 [8960/14335 (62%)]\tLoss: 6912.051758\tLR: [6.25e-06]\n",
            "Train Epoch: 1 [9216/14335 (64%)]\tLoss: 6615.248047\tLR: [6.428571428571429e-06]\n",
            "Train Epoch: 1 [9472/14335 (66%)]\tLoss: 7032.411621\tLR: [6.607142857142858e-06]\n",
            "Train Epoch: 1 [9728/14335 (68%)]\tLoss: 7394.402832\tLR: [6.785714285714285e-06]\n",
            "Train Epoch: 1 [9984/14335 (70%)]\tLoss: 6948.326660\tLR: [6.964285714285715e-06]\n",
            "Train Epoch: 1 [10240/14335 (71%)]\tLoss: 7160.262695\tLR: [7.142857142857143e-06]\n",
            "Train Epoch: 1 [10496/14335 (73%)]\tLoss: 6585.910645\tLR: [7.321428571428572e-06]\n",
            "Train Epoch: 1 [10752/14335 (75%)]\tLoss: 6819.802734\tLR: [7.5e-06]\n",
            "Train Epoch: 1 [11008/14335 (77%)]\tLoss: 6874.221680\tLR: [7.67857142857143e-06]\n",
            "Train Epoch: 1 [11264/14335 (79%)]\tLoss: 6336.348145\tLR: [7.857142857142858e-06]\n",
            "Train Epoch: 1 [11520/14335 (80%)]\tLoss: 6752.665527\tLR: [8.035714285714286e-06]\n",
            "Train Epoch: 1 [11776/14335 (82%)]\tLoss: 6671.090332\tLR: [8.214285714285714e-06]\n",
            "Train Epoch: 1 [12032/14335 (84%)]\tLoss: 7082.498047\tLR: [8.392857142857143e-06]\n",
            "Train Epoch: 1 [12288/14335 (86%)]\tLoss: 6283.604004\tLR: [8.571428571428573e-06]\n",
            "Train Epoch: 1 [12544/14335 (88%)]\tLoss: 5982.864746\tLR: [8.75e-06]\n",
            "Train Epoch: 1 [12800/14335 (89%)]\tLoss: 6378.943848\tLR: [8.92857142857143e-06]\n",
            "Train Epoch: 1 [13056/14335 (91%)]\tLoss: 7093.236816\tLR: [9.107142857142858e-06]\n",
            "Train Epoch: 1 [13312/14335 (93%)]\tLoss: 6594.398438\tLR: [9.285714285714286e-06]\n",
            "Train Epoch: 1 [13568/14335 (95%)]\tLoss: 6888.040527\tLR: [9.464285714285714e-06]\n",
            "Train Epoch: 1 [13824/14335 (96%)]\tLoss: 5972.112793\tLR: [9.642857142857144e-06]\n",
            "Train Epoch: 1 [14025/14335 (98%)]\tLoss: 6123.748535\tLR: [9.821428571428573e-06]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "saved model with best validaton in epoch1\n",
            "Evaluation ROC Score in epoch 1 is 0.9258639910813824, Best ROC Score is:0.9258639910813824\n",
            "Starting Epoch 2\n",
            "Train Epoch: 2 [0/14335 (0%)]\tLoss: 6356.713379\tLR: [1e-05]\n",
            "Train Epoch: 2 [256/14335 (2%)]\tLoss: 6632.041016\tLR: [1.0178571428571429e-05]\n",
            "Train Epoch: 2 [512/14335 (4%)]\tLoss: 6303.058594\tLR: [1.0357142857142859e-05]\n",
            "Train Epoch: 2 [768/14335 (5%)]\tLoss: 6732.958984\tLR: [1.0535714285714286e-05]\n",
            "Train Epoch: 2 [1024/14335 (7%)]\tLoss: 5696.064453\tLR: [1.0714285714285714e-05]\n",
            "Train Epoch: 2 [1280/14335 (9%)]\tLoss: 5731.204102\tLR: [1.0892857142857144e-05]\n",
            "Train Epoch: 2 [1536/14335 (11%)]\tLoss: 5467.884766\tLR: [1.107142857142857e-05]\n",
            "Train Epoch: 2 [1792/14335 (12%)]\tLoss: 5789.833984\tLR: [1.125e-05]\n",
            "Train Epoch: 2 [2048/14335 (14%)]\tLoss: 5528.989258\tLR: [1.1428571428571429e-05]\n",
            "Train Epoch: 2 [2304/14335 (16%)]\tLoss: 5899.415527\tLR: [1.1607142857142857e-05]\n",
            "Train Epoch: 2 [2560/14335 (18%)]\tLoss: 5628.949219\tLR: [1.1785714285714286e-05]\n",
            "Train Epoch: 2 [2816/14335 (20%)]\tLoss: 5276.360352\tLR: [1.1964285714285716e-05]\n",
            "Train Epoch: 2 [3072/14335 (21%)]\tLoss: 5296.028320\tLR: [1.2142857142857144e-05]\n",
            "Train Epoch: 2 [3328/14335 (23%)]\tLoss: 5935.745117\tLR: [1.2321428571428572e-05]\n",
            "Train Epoch: 2 [3584/14335 (25%)]\tLoss: 5125.703613\tLR: [1.25e-05]\n",
            "Train Epoch: 2 [3840/14335 (27%)]\tLoss: 5517.547363\tLR: [1.2678571428571429e-05]\n",
            "Train Epoch: 2 [4096/14335 (29%)]\tLoss: 4318.708008\tLR: [1.2857142857142857e-05]\n",
            "Train Epoch: 2 [4352/14335 (30%)]\tLoss: 4865.529785\tLR: [1.3035714285714287e-05]\n",
            "Train Epoch: 2 [4608/14335 (32%)]\tLoss: 4849.497070\tLR: [1.3214285714285716e-05]\n",
            "Train Epoch: 2 [4864/14335 (34%)]\tLoss: 5787.775391\tLR: [1.3392857142857144e-05]\n",
            "Train Epoch: 2 [5120/14335 (36%)]\tLoss: 4868.455566\tLR: [1.357142857142857e-05]\n",
            "Train Epoch: 2 [5376/14335 (38%)]\tLoss: 5758.854492\tLR: [1.3750000000000002e-05]\n",
            "Train Epoch: 2 [5632/14335 (39%)]\tLoss: 5402.204102\tLR: [1.392857142857143e-05]\n",
            "Train Epoch: 2 [5888/14335 (41%)]\tLoss: 5122.998047\tLR: [1.4107142857142857e-05]\n",
            "Train Epoch: 2 [6144/14335 (43%)]\tLoss: 4879.980957\tLR: [1.4285714285714285e-05]\n",
            "Train Epoch: 2 [6400/14335 (45%)]\tLoss: 4429.863770\tLR: [1.4464285714285717e-05]\n",
            "Train Epoch: 2 [6656/14335 (46%)]\tLoss: 4639.958984\tLR: [1.4642857142857144e-05]\n",
            "Train Epoch: 2 [6912/14335 (48%)]\tLoss: 4172.627441\tLR: [1.4821428571428572e-05]\n",
            "Train Epoch: 2 [7168/14335 (50%)]\tLoss: 4329.434570\tLR: [1.5e-05]\n",
            "Train Epoch: 2 [7424/14335 (52%)]\tLoss: 4772.753906\tLR: [1.5178571428571429e-05]\n",
            "Train Epoch: 2 [7680/14335 (54%)]\tLoss: 4120.814453\tLR: [1.535714285714286e-05]\n",
            "Train Epoch: 2 [7936/14335 (55%)]\tLoss: 3891.918213\tLR: [1.5535714285714285e-05]\n",
            "Train Epoch: 2 [8192/14335 (57%)]\tLoss: 4825.266602\tLR: [1.5714285714285715e-05]\n",
            "Train Epoch: 2 [8448/14335 (59%)]\tLoss: 4263.654297\tLR: [1.5892857142857142e-05]\n",
            "Train Epoch: 2 [8704/14335 (61%)]\tLoss: 4392.783203\tLR: [1.6071428571428572e-05]\n",
            "Train Epoch: 2 [8960/14335 (62%)]\tLoss: 5037.718750\tLR: [1.6250000000000002e-05]\n",
            "Train Epoch: 2 [9216/14335 (64%)]\tLoss: 4473.526367\tLR: [1.642857142857143e-05]\n",
            "Train Epoch: 2 [9472/14335 (66%)]\tLoss: 4635.808105\tLR: [1.660714285714286e-05]\n",
            "Train Epoch: 2 [9728/14335 (68%)]\tLoss: 5598.446777\tLR: [1.6785714285714285e-05]\n",
            "Train Epoch: 2 [9984/14335 (70%)]\tLoss: 4769.552734\tLR: [1.6964285714285715e-05]\n",
            "Train Epoch: 2 [10240/14335 (71%)]\tLoss: 4051.332031\tLR: [1.7142857142857145e-05]\n",
            "Train Epoch: 2 [10496/14335 (73%)]\tLoss: 4626.085938\tLR: [1.7321428571428572e-05]\n",
            "Train Epoch: 2 [10752/14335 (75%)]\tLoss: 3925.613525\tLR: [1.75e-05]\n",
            "Train Epoch: 2 [11008/14335 (77%)]\tLoss: 4446.304688\tLR: [1.7678571428571432e-05]\n",
            "Train Epoch: 2 [11264/14335 (79%)]\tLoss: 5107.725098\tLR: [1.785714285714286e-05]\n",
            "Train Epoch: 2 [11520/14335 (80%)]\tLoss: 3842.564697\tLR: [1.8035714285714285e-05]\n",
            "Train Epoch: 2 [11776/14335 (82%)]\tLoss: 4250.024414\tLR: [1.8214285714285715e-05]\n",
            "Train Epoch: 2 [12032/14335 (84%)]\tLoss: 4003.899170\tLR: [1.8392857142857145e-05]\n",
            "Train Epoch: 2 [12288/14335 (86%)]\tLoss: 4145.523926\tLR: [1.8571428571428572e-05]\n",
            "Train Epoch: 2 [12544/14335 (88%)]\tLoss: 4610.107422\tLR: [1.8750000000000002e-05]\n",
            "Train Epoch: 2 [12800/14335 (89%)]\tLoss: 3939.845459\tLR: [1.892857142857143e-05]\n",
            "Train Epoch: 2 [13056/14335 (91%)]\tLoss: 3979.439209\tLR: [1.910714285714286e-05]\n",
            "Train Epoch: 2 [13312/14335 (93%)]\tLoss: 3798.717773\tLR: [1.928571428571429e-05]\n",
            "Train Epoch: 2 [13568/14335 (95%)]\tLoss: 3816.152100\tLR: [1.9464285714285715e-05]\n",
            "Train Epoch: 2 [13824/14335 (96%)]\tLoss: 4145.638184\tLR: [1.9642857142857145e-05]\n",
            "Train Epoch: 2 [14025/14335 (98%)]\tLoss: 4531.472168\tLR: [1.982142857142857e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 2 is 0.9074074074074073, Best ROC Score is:0.9258639910813824\n",
            "Starting Epoch 3\n",
            "Train Epoch: 3 [0/14335 (0%)]\tLoss: 4323.699707\tLR: [2e-05]\n",
            "Train Epoch: 3 [256/14335 (2%)]\tLoss: 3998.045898\tLR: [2.017857142857143e-05]\n",
            "Train Epoch: 3 [512/14335 (4%)]\tLoss: 3337.618164\tLR: [2.0357142857142858e-05]\n",
            "Train Epoch: 3 [768/14335 (5%)]\tLoss: 4584.525391\tLR: [2.0535714285714285e-05]\n",
            "Train Epoch: 3 [1024/14335 (7%)]\tLoss: 4019.887939\tLR: [2.0714285714285718e-05]\n",
            "Train Epoch: 3 [1280/14335 (9%)]\tLoss: 3661.104004\tLR: [2.0892857142857145e-05]\n",
            "Train Epoch: 3 [1536/14335 (11%)]\tLoss: 3828.996582\tLR: [2.107142857142857e-05]\n",
            "Train Epoch: 3 [1792/14335 (12%)]\tLoss: 3751.066162\tLR: [2.125e-05]\n",
            "Train Epoch: 3 [2048/14335 (14%)]\tLoss: 3654.069092\tLR: [2.1428571428571428e-05]\n",
            "Train Epoch: 3 [2304/14335 (16%)]\tLoss: 4068.042969\tLR: [2.1607142857142858e-05]\n",
            "Train Epoch: 3 [2560/14335 (18%)]\tLoss: 3375.371826\tLR: [2.1785714285714288e-05]\n",
            "Train Epoch: 3 [2816/14335 (20%)]\tLoss: 3476.422852\tLR: [2.1964285714285715e-05]\n",
            "Train Epoch: 3 [3072/14335 (21%)]\tLoss: 3575.693848\tLR: [2.214285714285714e-05]\n",
            "Train Epoch: 3 [3328/14335 (23%)]\tLoss: 3423.175293\tLR: [2.2321428571428575e-05]\n",
            "Train Epoch: 3 [3584/14335 (25%)]\tLoss: 3673.837891\tLR: [2.25e-05]\n",
            "Train Epoch: 3 [3840/14335 (27%)]\tLoss: 4129.107910\tLR: [2.2678571428571428e-05]\n",
            "Train Epoch: 3 [4096/14335 (29%)]\tLoss: 3905.081299\tLR: [2.2857142857142858e-05]\n",
            "Train Epoch: 3 [4352/14335 (30%)]\tLoss: 3582.268311\tLR: [2.3035714285714285e-05]\n",
            "Train Epoch: 3 [4608/14335 (32%)]\tLoss: 3340.330078\tLR: [2.3214285714285715e-05]\n",
            "Train Epoch: 3 [4864/14335 (34%)]\tLoss: 3210.974365\tLR: [2.3392857142857145e-05]\n",
            "Train Epoch: 3 [5120/14335 (36%)]\tLoss: 3778.816650\tLR: [2.357142857142857e-05]\n",
            "Train Epoch: 3 [5376/14335 (38%)]\tLoss: 3120.114746\tLR: [2.375e-05]\n",
            "Train Epoch: 3 [5632/14335 (39%)]\tLoss: 3465.019775\tLR: [2.392857142857143e-05]\n",
            "Train Epoch: 3 [5888/14335 (41%)]\tLoss: 4283.282715\tLR: [2.4107142857142858e-05]\n",
            "Train Epoch: 3 [6144/14335 (43%)]\tLoss: 3447.445557\tLR: [2.4285714285714288e-05]\n",
            "Train Epoch: 3 [6400/14335 (45%)]\tLoss: 3291.148193\tLR: [2.4464285714285715e-05]\n",
            "Train Epoch: 3 [6656/14335 (46%)]\tLoss: 3796.384766\tLR: [2.4642857142857145e-05]\n",
            "Train Epoch: 3 [6912/14335 (48%)]\tLoss: 3365.622314\tLR: [2.4821428571428575e-05]\n",
            "Train Epoch: 3 [7168/14335 (50%)]\tLoss: 3352.414062\tLR: [2.5e-05]\n",
            "Train Epoch: 3 [7424/14335 (52%)]\tLoss: 3716.816650\tLR: [2.5178571428571428e-05]\n",
            "Train Epoch: 3 [7680/14335 (54%)]\tLoss: 3881.239990\tLR: [2.5357142857142858e-05]\n",
            "Train Epoch: 3 [7936/14335 (55%)]\tLoss: 3573.764160\tLR: [2.5535714285714284e-05]\n",
            "Train Epoch: 3 [8192/14335 (57%)]\tLoss: 3577.323486\tLR: [2.5714285714285714e-05]\n",
            "Train Epoch: 3 [8448/14335 (59%)]\tLoss: 4254.567871\tLR: [2.5892857142857148e-05]\n",
            "Train Epoch: 3 [8704/14335 (61%)]\tLoss: 3808.715332\tLR: [2.6071428571428574e-05]\n",
            "Train Epoch: 3 [8960/14335 (62%)]\tLoss: 3400.263916\tLR: [2.625e-05]\n",
            "Train Epoch: 3 [9216/14335 (64%)]\tLoss: 3828.764893\tLR: [2.642857142857143e-05]\n",
            "Train Epoch: 3 [9472/14335 (66%)]\tLoss: 4205.049316\tLR: [2.6607142857142858e-05]\n",
            "Train Epoch: 3 [9728/14335 (68%)]\tLoss: 3276.291504\tLR: [2.6785714285714288e-05]\n",
            "Train Epoch: 3 [9984/14335 (70%)]\tLoss: 3644.509521\tLR: [2.6964285714285714e-05]\n",
            "Train Epoch: 3 [10240/14335 (71%)]\tLoss: 3805.976807\tLR: [2.714285714285714e-05]\n",
            "Train Epoch: 3 [10496/14335 (73%)]\tLoss: 3551.127686\tLR: [2.732142857142857e-05]\n",
            "Train Epoch: 3 [10752/14335 (75%)]\tLoss: 3391.670410\tLR: [2.7500000000000004e-05]\n",
            "Train Epoch: 3 [11008/14335 (77%)]\tLoss: 3525.473145\tLR: [2.767857142857143e-05]\n",
            "Train Epoch: 3 [11264/14335 (79%)]\tLoss: 3471.260742\tLR: [2.785714285714286e-05]\n",
            "Train Epoch: 3 [11520/14335 (80%)]\tLoss: 3712.666504\tLR: [2.8035714285714288e-05]\n",
            "Train Epoch: 3 [11776/14335 (82%)]\tLoss: 3345.241943\tLR: [2.8214285714285714e-05]\n",
            "Train Epoch: 3 [12032/14335 (84%)]\tLoss: 3245.967285\tLR: [2.8392857142857144e-05]\n",
            "Train Epoch: 3 [12288/14335 (86%)]\tLoss: 3755.895020\tLR: [2.857142857142857e-05]\n",
            "Train Epoch: 3 [12544/14335 (88%)]\tLoss: 3344.479004\tLR: [2.8749999999999997e-05]\n",
            "Train Epoch: 3 [12800/14335 (89%)]\tLoss: 3566.057373\tLR: [2.8928571428571434e-05]\n",
            "Train Epoch: 3 [13056/14335 (91%)]\tLoss: 3493.662842\tLR: [2.910714285714286e-05]\n",
            "Train Epoch: 3 [13312/14335 (93%)]\tLoss: 3663.276611\tLR: [2.9285714285714288e-05]\n",
            "Train Epoch: 3 [13568/14335 (95%)]\tLoss: 3538.619385\tLR: [2.9464285714285718e-05]\n",
            "Train Epoch: 3 [13824/14335 (96%)]\tLoss: 3431.783691\tLR: [2.9642857142857144e-05]\n",
            "Train Epoch: 3 [14025/14335 (98%)]\tLoss: 3391.565186\tLR: [2.982142857142857e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 3 is 0.9085155350978136, Best ROC Score is:0.9258639910813824\n",
            "Starting Epoch 4\n",
            "Train Epoch: 4 [0/14335 (0%)]\tLoss: 3291.637451\tLR: [3e-05]\n",
            "Train Epoch: 4 [256/14335 (2%)]\tLoss: 2867.187012\tLR: [3.0178571428571427e-05]\n",
            "Train Epoch: 4 [512/14335 (4%)]\tLoss: 3161.293945\tLR: [3.0357142857142857e-05]\n",
            "Train Epoch: 4 [768/14335 (5%)]\tLoss: 3361.501953\tLR: [3.053571428571429e-05]\n",
            "Train Epoch: 4 [1024/14335 (7%)]\tLoss: 3519.129639\tLR: [3.071428571428572e-05]\n",
            "Train Epoch: 4 [1280/14335 (9%)]\tLoss: 3449.684082\tLR: [3.089285714285715e-05]\n",
            "Train Epoch: 4 [1536/14335 (11%)]\tLoss: 2844.802490\tLR: [3.107142857142857e-05]\n",
            "Train Epoch: 4 [1792/14335 (12%)]\tLoss: 3489.619141\tLR: [3.125e-05]\n",
            "Train Epoch: 4 [2048/14335 (14%)]\tLoss: 2832.790283\tLR: [3.142857142857143e-05]\n",
            "Train Epoch: 4 [2304/14335 (16%)]\tLoss: 3250.871338\tLR: [3.160714285714286e-05]\n",
            "Train Epoch: 4 [2560/14335 (18%)]\tLoss: 3160.325684\tLR: [3.1785714285714284e-05]\n",
            "Train Epoch: 4 [2816/14335 (20%)]\tLoss: 3538.701904\tLR: [3.1964285714285714e-05]\n",
            "Train Epoch: 4 [3072/14335 (21%)]\tLoss: 3593.585449\tLR: [3.2142857142857144e-05]\n",
            "Train Epoch: 4 [3328/14335 (23%)]\tLoss: 3295.353516\tLR: [3.2321428571428574e-05]\n",
            "Train Epoch: 4 [3584/14335 (25%)]\tLoss: 3057.703125\tLR: [3.2500000000000004e-05]\n",
            "Train Epoch: 4 [3840/14335 (27%)]\tLoss: 3081.974609\tLR: [3.2678571428571434e-05]\n",
            "Train Epoch: 4 [4096/14335 (29%)]\tLoss: 2890.398438\tLR: [3.285714285714286e-05]\n",
            "Train Epoch: 4 [4352/14335 (30%)]\tLoss: 3979.955078\tLR: [3.303571428571429e-05]\n",
            "Train Epoch: 4 [4608/14335 (32%)]\tLoss: 3244.720947\tLR: [3.321428571428572e-05]\n",
            "Train Epoch: 4 [4864/14335 (34%)]\tLoss: 3120.601562\tLR: [3.339285714285714e-05]\n",
            "Train Epoch: 4 [5120/14335 (36%)]\tLoss: 3582.687744\tLR: [3.357142857142857e-05]\n",
            "Train Epoch: 4 [5376/14335 (38%)]\tLoss: 3140.457031\tLR: [3.375000000000001e-05]\n",
            "Train Epoch: 4 [5632/14335 (39%)]\tLoss: 3049.029297\tLR: [3.392857142857143e-05]\n",
            "Train Epoch: 4 [5888/14335 (41%)]\tLoss: 3172.279297\tLR: [3.410714285714286e-05]\n",
            "Train Epoch: 4 [6144/14335 (43%)]\tLoss: 3289.570312\tLR: [3.428571428571429e-05]\n",
            "Train Epoch: 4 [6400/14335 (45%)]\tLoss: 2912.886230\tLR: [3.4464285714285714e-05]\n",
            "Train Epoch: 4 [6656/14335 (46%)]\tLoss: 2828.063477\tLR: [3.4642857142857144e-05]\n",
            "Train Epoch: 4 [6912/14335 (48%)]\tLoss: 3204.482910\tLR: [3.4821428571428574e-05]\n",
            "Train Epoch: 4 [7168/14335 (50%)]\tLoss: 3442.194336\tLR: [3.5e-05]\n",
            "Train Epoch: 4 [7424/14335 (52%)]\tLoss: 3427.858643\tLR: [3.5178571428571434e-05]\n",
            "Train Epoch: 4 [7680/14335 (54%)]\tLoss: 3462.181396\tLR: [3.5357142857142864e-05]\n",
            "Train Epoch: 4 [7936/14335 (55%)]\tLoss: 2873.785645\tLR: [3.553571428571429e-05]\n",
            "Train Epoch: 4 [8192/14335 (57%)]\tLoss: 2979.753174\tLR: [3.571428571428572e-05]\n",
            "Train Epoch: 4 [8448/14335 (59%)]\tLoss: 3159.151367\tLR: [3.589285714285715e-05]\n",
            "Train Epoch: 4 [8704/14335 (61%)]\tLoss: 3085.353516\tLR: [3.607142857142857e-05]\n",
            "Train Epoch: 4 [8960/14335 (62%)]\tLoss: 2918.317627\tLR: [3.625e-05]\n",
            "Train Epoch: 4 [9216/14335 (64%)]\tLoss: 3280.642822\tLR: [3.642857142857143e-05]\n",
            "Train Epoch: 4 [9472/14335 (66%)]\tLoss: 2911.616211\tLR: [3.6607142857142853e-05]\n",
            "Train Epoch: 4 [9728/14335 (68%)]\tLoss: 3383.248779\tLR: [3.678571428571429e-05]\n",
            "Train Epoch: 4 [9984/14335 (70%)]\tLoss: 2991.745117\tLR: [3.696428571428572e-05]\n",
            "Train Epoch: 4 [10240/14335 (71%)]\tLoss: 3530.469238\tLR: [3.7142857142857143e-05]\n",
            "Train Epoch: 4 [10496/14335 (73%)]\tLoss: 3420.325928\tLR: [3.7321428571428573e-05]\n",
            "Train Epoch: 4 [10752/14335 (75%)]\tLoss: 3385.187012\tLR: [3.7500000000000003e-05]\n",
            "Train Epoch: 4 [11008/14335 (77%)]\tLoss: 3028.035400\tLR: [3.767857142857143e-05]\n",
            "Train Epoch: 4 [11264/14335 (79%)]\tLoss: 2894.348633\tLR: [3.785714285714286e-05]\n",
            "Train Epoch: 4 [11520/14335 (80%)]\tLoss: 2930.621094\tLR: [3.803571428571429e-05]\n",
            "Train Epoch: 4 [11776/14335 (82%)]\tLoss: 2907.995850\tLR: [3.821428571428572e-05]\n",
            "Train Epoch: 4 [12032/14335 (84%)]\tLoss: 3380.043213\tLR: [3.839285714285715e-05]\n",
            "Train Epoch: 4 [12288/14335 (86%)]\tLoss: 3028.552246\tLR: [3.857142857142858e-05]\n",
            "Train Epoch: 4 [12544/14335 (88%)]\tLoss: 3321.618408\tLR: [3.875e-05]\n",
            "Train Epoch: 4 [12800/14335 (89%)]\tLoss: 3257.400635\tLR: [3.892857142857143e-05]\n",
            "Train Epoch: 4 [13056/14335 (91%)]\tLoss: 2983.408203\tLR: [3.910714285714286e-05]\n",
            "Train Epoch: 4 [13312/14335 (93%)]\tLoss: 3354.102295\tLR: [3.928571428571429e-05]\n",
            "Train Epoch: 4 [13568/14335 (95%)]\tLoss: 3173.871094\tLR: [3.946428571428571e-05]\n",
            "Train Epoch: 4 [13824/14335 (96%)]\tLoss: 2916.073486\tLR: [3.964285714285714e-05]\n",
            "Train Epoch: 4 [14025/14335 (98%)]\tLoss: 2841.669434\tLR: [3.982142857142857e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 4 is 0.9228243021346468, Best ROC Score is:0.9258639910813824\n",
            "Starting Epoch 5\n",
            "Train Epoch: 5 [0/14335 (0%)]\tLoss: 3114.213867\tLR: [4e-05]\n",
            "Train Epoch: 5 [256/14335 (2%)]\tLoss: 2959.202148\tLR: [4.017857142857143e-05]\n",
            "Train Epoch: 5 [512/14335 (4%)]\tLoss: 3021.262939\tLR: [4.035714285714286e-05]\n",
            "Train Epoch: 5 [768/14335 (5%)]\tLoss: 3237.527832\tLR: [4.0535714285714287e-05]\n",
            "Train Epoch: 5 [1024/14335 (7%)]\tLoss: 3135.566650\tLR: [4.0714285714285717e-05]\n",
            "Train Epoch: 5 [1280/14335 (9%)]\tLoss: 3050.222168\tLR: [4.0892857142857147e-05]\n",
            "Train Epoch: 5 [1536/14335 (11%)]\tLoss: 3240.940430\tLR: [4.107142857142857e-05]\n",
            "Train Epoch: 5 [1792/14335 (12%)]\tLoss: 2821.325439\tLR: [4.125e-05]\n",
            "Train Epoch: 5 [2048/14335 (14%)]\tLoss: 2780.801514\tLR: [4.1428571428571437e-05]\n",
            "Train Epoch: 5 [2304/14335 (16%)]\tLoss: 3106.757568\tLR: [4.160714285714286e-05]\n",
            "Train Epoch: 5 [2560/14335 (18%)]\tLoss: 2949.744385\tLR: [4.178571428571429e-05]\n",
            "Train Epoch: 5 [2816/14335 (20%)]\tLoss: 3179.665283\tLR: [4.196428571428572e-05]\n",
            "Train Epoch: 5 [3072/14335 (21%)]\tLoss: 2996.647949\tLR: [4.214285714285714e-05]\n",
            "Train Epoch: 5 [3328/14335 (23%)]\tLoss: 3121.946777\tLR: [4.232142857142857e-05]\n",
            "Train Epoch: 5 [3584/14335 (25%)]\tLoss: 3243.643799\tLR: [4.25e-05]\n",
            "Train Epoch: 5 [3840/14335 (27%)]\tLoss: 2978.519775\tLR: [4.2678571428571426e-05]\n",
            "Train Epoch: 5 [4096/14335 (29%)]\tLoss: 3140.500732\tLR: [4.2857142857142856e-05]\n",
            "Train Epoch: 5 [4352/14335 (30%)]\tLoss: 3070.643555\tLR: [4.303571428571429e-05]\n",
            "Train Epoch: 5 [4608/14335 (32%)]\tLoss: 2945.746826\tLR: [4.3214285714285716e-05]\n",
            "Train Epoch: 5 [4864/14335 (34%)]\tLoss: 2987.620850\tLR: [4.3392857142857146e-05]\n",
            "Train Epoch: 5 [5120/14335 (36%)]\tLoss: 3190.139648\tLR: [4.3571428571428576e-05]\n",
            "Train Epoch: 5 [5376/14335 (38%)]\tLoss: 2974.720947\tLR: [4.375e-05]\n",
            "Train Epoch: 5 [5632/14335 (39%)]\tLoss: 2814.623779\tLR: [4.392857142857143e-05]\n",
            "Train Epoch: 5 [5888/14335 (41%)]\tLoss: 3280.210449\tLR: [4.410714285714286e-05]\n",
            "Train Epoch: 5 [6144/14335 (43%)]\tLoss: 3122.918701\tLR: [4.428571428571428e-05]\n",
            "Train Epoch: 5 [6400/14335 (45%)]\tLoss: 2835.876465\tLR: [4.446428571428571e-05]\n",
            "Train Epoch: 5 [6656/14335 (46%)]\tLoss: 2924.444580\tLR: [4.464285714285715e-05]\n",
            "Train Epoch: 5 [6912/14335 (48%)]\tLoss: 3605.425781\tLR: [4.482142857142857e-05]\n",
            "Train Epoch: 5 [7168/14335 (50%)]\tLoss: 2751.349854\tLR: [4.5e-05]\n",
            "Train Epoch: 5 [7424/14335 (52%)]\tLoss: 2962.218018\tLR: [4.517857142857143e-05]\n",
            "Train Epoch: 5 [7680/14335 (54%)]\tLoss: 3112.248291\tLR: [4.5357142857142856e-05]\n",
            "Train Epoch: 5 [7936/14335 (55%)]\tLoss: 3028.384521\tLR: [4.5535714285714286e-05]\n",
            "Train Epoch: 5 [8192/14335 (57%)]\tLoss: 3028.544678\tLR: [4.5714285714285716e-05]\n",
            "Train Epoch: 5 [8448/14335 (59%)]\tLoss: 2735.072021\tLR: [4.5892857142857146e-05]\n",
            "Train Epoch: 5 [8704/14335 (61%)]\tLoss: 3328.296631\tLR: [4.607142857142857e-05]\n",
            "Train Epoch: 5 [8960/14335 (62%)]\tLoss: 2608.250977\tLR: [4.6250000000000006e-05]\n",
            "Train Epoch: 5 [9216/14335 (64%)]\tLoss: 2894.333252\tLR: [4.642857142857143e-05]\n",
            "Train Epoch: 5 [9472/14335 (66%)]\tLoss: 2811.144043\tLR: [4.660714285714286e-05]\n",
            "Train Epoch: 5 [9728/14335 (68%)]\tLoss: 2701.520508\tLR: [4.678571428571429e-05]\n",
            "Train Epoch: 5 [9984/14335 (70%)]\tLoss: 3023.656494\tLR: [4.696428571428572e-05]\n",
            "Train Epoch: 5 [10240/14335 (71%)]\tLoss: 2669.715332\tLR: [4.714285714285714e-05]\n",
            "Train Epoch: 5 [10496/14335 (73%)]\tLoss: 3301.750977\tLR: [4.732142857142857e-05]\n",
            "Train Epoch: 5 [10752/14335 (75%)]\tLoss: 3090.117188\tLR: [4.75e-05]\n",
            "Train Epoch: 5 [11008/14335 (77%)]\tLoss: 3113.431152\tLR: [4.767857142857143e-05]\n",
            "Train Epoch: 5 [11264/14335 (79%)]\tLoss: 2839.519043\tLR: [4.785714285714286e-05]\n",
            "Train Epoch: 5 [11520/14335 (80%)]\tLoss: 3033.540039\tLR: [4.803571428571429e-05]\n",
            "Train Epoch: 5 [11776/14335 (82%)]\tLoss: 2714.839844\tLR: [4.8214285714285716e-05]\n",
            "Train Epoch: 5 [12032/14335 (84%)]\tLoss: 2780.611816\tLR: [4.8392857142857146e-05]\n",
            "Train Epoch: 5 [12288/14335 (86%)]\tLoss: 2758.369873\tLR: [4.8571428571428576e-05]\n",
            "Train Epoch: 5 [12544/14335 (88%)]\tLoss: 3564.652100\tLR: [4.875e-05]\n",
            "Train Epoch: 5 [12800/14335 (89%)]\tLoss: 2705.143066\tLR: [4.892857142857143e-05]\n",
            "Train Epoch: 5 [13056/14335 (91%)]\tLoss: 3236.597900\tLR: [4.910714285714286e-05]\n",
            "Train Epoch: 5 [13312/14335 (93%)]\tLoss: 2847.474121\tLR: [4.928571428571429e-05]\n",
            "Train Epoch: 5 [13568/14335 (95%)]\tLoss: 3082.037354\tLR: [4.946428571428572e-05]\n",
            "Train Epoch: 5 [13824/14335 (96%)]\tLoss: 3121.058350\tLR: [4.964285714285715e-05]\n",
            "Train Epoch: 5 [14025/14335 (98%)]\tLoss: 3132.837891\tLR: [4.982142857142857e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "saved model with best validaton in epoch5\n",
            "Evaluation ROC Score in epoch 5 is 0.9312820512820513, Best ROC Score is:0.9312820512820513\n",
            "Starting Epoch 6\n",
            "Train Epoch: 6 [0/14335 (0%)]\tLoss: 3337.271973\tLR: [5e-05]\n",
            "Train Epoch: 6 [256/14335 (2%)]\tLoss: 2615.770020\tLR: [4.998015873015873e-05]\n",
            "Train Epoch: 6 [512/14335 (4%)]\tLoss: 2952.027588\tLR: [4.996031746031746e-05]\n",
            "Train Epoch: 6 [768/14335 (5%)]\tLoss: 2970.507812\tLR: [4.994047619047619e-05]\n",
            "Train Epoch: 6 [1024/14335 (7%)]\tLoss: 2742.068604\tLR: [4.9920634920634924e-05]\n",
            "Train Epoch: 6 [1280/14335 (9%)]\tLoss: 3174.380615\tLR: [4.990079365079365e-05]\n",
            "Train Epoch: 6 [1536/14335 (11%)]\tLoss: 2719.633789\tLR: [4.9880952380952385e-05]\n",
            "Train Epoch: 6 [1792/14335 (12%)]\tLoss: 2808.022461\tLR: [4.986111111111111e-05]\n",
            "Train Epoch: 6 [2048/14335 (14%)]\tLoss: 3134.188477\tLR: [4.9841269841269845e-05]\n",
            "Train Epoch: 6 [2304/14335 (16%)]\tLoss: 2973.812988\tLR: [4.982142857142857e-05]\n",
            "Train Epoch: 6 [2560/14335 (18%)]\tLoss: 3231.096436\tLR: [4.9801587301587306e-05]\n",
            "Train Epoch: 6 [2816/14335 (20%)]\tLoss: 2608.781982\tLR: [4.978174603174603e-05]\n",
            "Train Epoch: 6 [3072/14335 (21%)]\tLoss: 3152.912354\tLR: [4.976190476190477e-05]\n",
            "Train Epoch: 6 [3328/14335 (23%)]\tLoss: 3496.164062\tLR: [4.9742063492063494e-05]\n",
            "Train Epoch: 6 [3584/14335 (25%)]\tLoss: 2809.277588\tLR: [4.972222222222223e-05]\n",
            "Train Epoch: 6 [3840/14335 (27%)]\tLoss: 2836.120850\tLR: [4.9702380952380955e-05]\n",
            "Train Epoch: 6 [4096/14335 (29%)]\tLoss: 3168.796875\tLR: [4.968253968253969e-05]\n",
            "Train Epoch: 6 [4352/14335 (30%)]\tLoss: 2721.423584\tLR: [4.9662698412698415e-05]\n",
            "Train Epoch: 6 [4608/14335 (32%)]\tLoss: 2932.170166\tLR: [4.964285714285715e-05]\n",
            "Train Epoch: 6 [4864/14335 (34%)]\tLoss: 3296.476807\tLR: [4.9623015873015876e-05]\n",
            "Train Epoch: 6 [5120/14335 (36%)]\tLoss: 2873.701416\tLR: [4.960317460317461e-05]\n",
            "Train Epoch: 6 [5376/14335 (38%)]\tLoss: 3154.728027\tLR: [4.958333333333334e-05]\n",
            "Train Epoch: 6 [5632/14335 (39%)]\tLoss: 3305.341309\tLR: [4.956349206349207e-05]\n",
            "Train Epoch: 6 [5888/14335 (41%)]\tLoss: 2959.801025\tLR: [4.95436507936508e-05]\n",
            "Train Epoch: 6 [6144/14335 (43%)]\tLoss: 3197.547852\tLR: [4.9523809523809525e-05]\n",
            "Train Epoch: 6 [6400/14335 (45%)]\tLoss: 3086.756592\tLR: [4.950396825396826e-05]\n",
            "Train Epoch: 6 [6656/14335 (46%)]\tLoss: 2893.479980\tLR: [4.9484126984126985e-05]\n",
            "Train Epoch: 6 [6912/14335 (48%)]\tLoss: 2778.549561\tLR: [4.946428571428572e-05]\n",
            "Train Epoch: 6 [7168/14335 (50%)]\tLoss: 3283.678467\tLR: [4.9444444444444446e-05]\n",
            "Train Epoch: 6 [7424/14335 (52%)]\tLoss: 2937.881348\tLR: [4.942460317460318e-05]\n",
            "Train Epoch: 6 [7680/14335 (54%)]\tLoss: 3161.802734\tLR: [4.940476190476191e-05]\n",
            "Train Epoch: 6 [7936/14335 (55%)]\tLoss: 2837.166748\tLR: [4.938492063492064e-05]\n",
            "Train Epoch: 6 [8192/14335 (57%)]\tLoss: 3350.876465\tLR: [4.936507936507937e-05]\n",
            "Train Epoch: 6 [8448/14335 (59%)]\tLoss: 2998.471436\tLR: [4.93452380952381e-05]\n",
            "Train Epoch: 6 [8704/14335 (61%)]\tLoss: 2677.254150\tLR: [4.932539682539683e-05]\n",
            "Train Epoch: 6 [8960/14335 (62%)]\tLoss: 3234.057373\tLR: [4.930555555555556e-05]\n",
            "Train Epoch: 6 [9216/14335 (64%)]\tLoss: 3060.435303\tLR: [4.928571428571429e-05]\n",
            "Train Epoch: 6 [9472/14335 (66%)]\tLoss: 3061.688965\tLR: [4.926587301587302e-05]\n",
            "Train Epoch: 6 [9728/14335 (68%)]\tLoss: 3355.783691\tLR: [4.924603174603175e-05]\n",
            "Train Epoch: 6 [9984/14335 (70%)]\tLoss: 2698.465576\tLR: [4.9226190476190484e-05]\n",
            "Train Epoch: 6 [10240/14335 (71%)]\tLoss: 2956.964355\tLR: [4.9206349206349204e-05]\n",
            "Train Epoch: 6 [10496/14335 (73%)]\tLoss: 2850.162354\tLR: [4.918650793650794e-05]\n",
            "Train Epoch: 6 [10752/14335 (75%)]\tLoss: 2780.374268\tLR: [4.9166666666666665e-05]\n",
            "Train Epoch: 6 [11008/14335 (77%)]\tLoss: 3026.122070\tLR: [4.91468253968254e-05]\n",
            "Train Epoch: 6 [11264/14335 (79%)]\tLoss: 2863.136230\tLR: [4.9126984126984125e-05]\n",
            "Train Epoch: 6 [11520/14335 (80%)]\tLoss: 3194.706787\tLR: [4.910714285714286e-05]\n",
            "Train Epoch: 6 [11776/14335 (82%)]\tLoss: 2994.975830\tLR: [4.9087301587301586e-05]\n",
            "Train Epoch: 6 [12032/14335 (84%)]\tLoss: 2901.395264\tLR: [4.906746031746032e-05]\n",
            "Train Epoch: 6 [12288/14335 (86%)]\tLoss: 2871.124512\tLR: [4.904761904761905e-05]\n",
            "Train Epoch: 6 [12544/14335 (88%)]\tLoss: 3261.867432\tLR: [4.902777777777778e-05]\n",
            "Train Epoch: 6 [12800/14335 (89%)]\tLoss: 3361.321289\tLR: [4.900793650793651e-05]\n",
            "Train Epoch: 6 [13056/14335 (91%)]\tLoss: 2983.175049\tLR: [4.898809523809524e-05]\n",
            "Train Epoch: 6 [13312/14335 (93%)]\tLoss: 2879.830811\tLR: [4.896825396825397e-05]\n",
            "Train Epoch: 6 [13568/14335 (95%)]\tLoss: 2844.692627\tLR: [4.89484126984127e-05]\n",
            "Train Epoch: 6 [13824/14335 (96%)]\tLoss: 3318.237061\tLR: [4.892857142857143e-05]\n",
            "Train Epoch: 6 [14025/14335 (98%)]\tLoss: 3034.078613\tLR: [4.8908730158730156e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 6 is 0.8895582329317269, Best ROC Score is:0.9312820512820513\n",
            "Starting Epoch 7\n",
            "Train Epoch: 7 [0/14335 (0%)]\tLoss: 3092.643066\tLR: [4.888888888888889e-05]\n",
            "Train Epoch: 7 [256/14335 (2%)]\tLoss: 3012.073486\tLR: [4.886904761904762e-05]\n",
            "Train Epoch: 7 [512/14335 (4%)]\tLoss: 2854.208252\tLR: [4.884920634920635e-05]\n",
            "Train Epoch: 7 [768/14335 (5%)]\tLoss: 2874.594727\tLR: [4.882936507936508e-05]\n",
            "Train Epoch: 7 [1024/14335 (7%)]\tLoss: 2864.786865\tLR: [4.880952380952381e-05]\n",
            "Train Epoch: 7 [1280/14335 (9%)]\tLoss: 3018.335938\tLR: [4.878968253968254e-05]\n",
            "Train Epoch: 7 [1536/14335 (11%)]\tLoss: 3169.993652\tLR: [4.876984126984127e-05]\n",
            "Train Epoch: 7 [1792/14335 (12%)]\tLoss: 2734.884521\tLR: [4.875e-05]\n",
            "Train Epoch: 7 [2048/14335 (14%)]\tLoss: 3003.312500\tLR: [4.873015873015873e-05]\n",
            "Train Epoch: 7 [2304/14335 (16%)]\tLoss: 3089.384033\tLR: [4.871031746031746e-05]\n",
            "Train Epoch: 7 [2560/14335 (18%)]\tLoss: 2834.841064\tLR: [4.8690476190476194e-05]\n",
            "Train Epoch: 7 [2816/14335 (20%)]\tLoss: 3423.325195\tLR: [4.867063492063492e-05]\n",
            "Train Epoch: 7 [3072/14335 (21%)]\tLoss: 3139.365234\tLR: [4.8650793650793654e-05]\n",
            "Train Epoch: 7 [3328/14335 (23%)]\tLoss: 2795.700684\tLR: [4.863095238095238e-05]\n",
            "Train Epoch: 7 [3584/14335 (25%)]\tLoss: 3098.441406\tLR: [4.8611111111111115e-05]\n",
            "Train Epoch: 7 [3840/14335 (27%)]\tLoss: 2914.281738\tLR: [4.859126984126984e-05]\n",
            "Train Epoch: 7 [4096/14335 (29%)]\tLoss: 3088.477783\tLR: [4.8571428571428576e-05]\n",
            "Train Epoch: 7 [4352/14335 (30%)]\tLoss: 2657.265381\tLR: [4.85515873015873e-05]\n",
            "Train Epoch: 7 [4608/14335 (32%)]\tLoss: 3330.479004\tLR: [4.853174603174604e-05]\n",
            "Train Epoch: 7 [4864/14335 (34%)]\tLoss: 3117.627441\tLR: [4.8511904761904764e-05]\n",
            "Train Epoch: 7 [5120/14335 (36%)]\tLoss: 2994.658447\tLR: [4.84920634920635e-05]\n",
            "Train Epoch: 7 [5376/14335 (38%)]\tLoss: 3181.013916\tLR: [4.8472222222222224e-05]\n",
            "Train Epoch: 7 [5632/14335 (39%)]\tLoss: 3095.940918\tLR: [4.845238095238095e-05]\n",
            "Train Epoch: 7 [5888/14335 (41%)]\tLoss: 2927.470703\tLR: [4.8432539682539685e-05]\n",
            "Train Epoch: 7 [6144/14335 (43%)]\tLoss: 3131.385254\tLR: [4.841269841269841e-05]\n",
            "Train Epoch: 7 [6400/14335 (45%)]\tLoss: 2914.729492\tLR: [4.8392857142857146e-05]\n",
            "Train Epoch: 7 [6656/14335 (46%)]\tLoss: 3051.591797\tLR: [4.837301587301587e-05]\n",
            "Train Epoch: 7 [6912/14335 (48%)]\tLoss: 2874.603760\tLR: [4.835317460317461e-05]\n",
            "Train Epoch: 7 [7168/14335 (50%)]\tLoss: 3027.985840\tLR: [4.8333333333333334e-05]\n",
            "Train Epoch: 7 [7424/14335 (52%)]\tLoss: 3398.170898\tLR: [4.831349206349207e-05]\n",
            "Train Epoch: 7 [7680/14335 (54%)]\tLoss: 3124.771729\tLR: [4.8293650793650794e-05]\n",
            "Train Epoch: 7 [7936/14335 (55%)]\tLoss: 2911.963867\tLR: [4.827380952380953e-05]\n",
            "Train Epoch: 7 [8192/14335 (57%)]\tLoss: 3282.716553\tLR: [4.8253968253968255e-05]\n",
            "Train Epoch: 7 [8448/14335 (59%)]\tLoss: 2895.394775\tLR: [4.823412698412699e-05]\n",
            "Train Epoch: 7 [8704/14335 (61%)]\tLoss: 3144.811523\tLR: [4.8214285714285716e-05]\n",
            "Train Epoch: 7 [8960/14335 (62%)]\tLoss: 2732.619385\tLR: [4.819444444444445e-05]\n",
            "Train Epoch: 7 [9216/14335 (64%)]\tLoss: 3081.853027\tLR: [4.817460317460318e-05]\n",
            "Train Epoch: 7 [9472/14335 (66%)]\tLoss: 3395.678955\tLR: [4.815476190476191e-05]\n",
            "Train Epoch: 7 [9728/14335 (68%)]\tLoss: 3172.898438\tLR: [4.813492063492064e-05]\n",
            "Train Epoch: 7 [9984/14335 (70%)]\tLoss: 2787.526123\tLR: [4.811507936507937e-05]\n",
            "Train Epoch: 7 [10240/14335 (71%)]\tLoss: 3106.811035\tLR: [4.80952380952381e-05]\n",
            "Train Epoch: 7 [10496/14335 (73%)]\tLoss: 3080.748535\tLR: [4.807539682539683e-05]\n",
            "Train Epoch: 7 [10752/14335 (75%)]\tLoss: 2978.772461\tLR: [4.805555555555556e-05]\n",
            "Train Epoch: 7 [11008/14335 (77%)]\tLoss: 2807.463867\tLR: [4.803571428571429e-05]\n",
            "Train Epoch: 7 [11264/14335 (79%)]\tLoss: 2791.416016\tLR: [4.801587301587302e-05]\n",
            "Train Epoch: 7 [11520/14335 (80%)]\tLoss: 3142.623291\tLR: [4.799603174603175e-05]\n",
            "Train Epoch: 7 [11776/14335 (82%)]\tLoss: 2725.192627\tLR: [4.797619047619048e-05]\n",
            "Train Epoch: 7 [12032/14335 (84%)]\tLoss: 3046.050781\tLR: [4.795634920634921e-05]\n",
            "Train Epoch: 7 [12288/14335 (86%)]\tLoss: 3139.270508\tLR: [4.793650793650794e-05]\n",
            "Train Epoch: 7 [12544/14335 (88%)]\tLoss: 3095.075195\tLR: [4.791666666666667e-05]\n",
            "Train Epoch: 7 [12800/14335 (89%)]\tLoss: 2904.302734\tLR: [4.78968253968254e-05]\n",
            "Train Epoch: 7 [13056/14335 (91%)]\tLoss: 3392.991699\tLR: [4.787698412698413e-05]\n",
            "Train Epoch: 7 [13312/14335 (93%)]\tLoss: 2890.031982\tLR: [4.785714285714286e-05]\n",
            "Train Epoch: 7 [13568/14335 (95%)]\tLoss: 3185.670898\tLR: [4.783730158730159e-05]\n",
            "Train Epoch: 7 [13824/14335 (96%)]\tLoss: 2933.793213\tLR: [4.781746031746032e-05]\n",
            "Train Epoch: 7 [14025/14335 (98%)]\tLoss: 2689.970703\tLR: [4.779761904761905e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 7 is 0.8502347417840376, Best ROC Score is:0.9312820512820513\n",
            "Starting Epoch 8\n",
            "Train Epoch: 8 [0/14335 (0%)]\tLoss: 3161.211670\tLR: [4.7777777777777784e-05]\n",
            "Train Epoch: 8 [256/14335 (2%)]\tLoss: 2964.578613\tLR: [4.775793650793651e-05]\n",
            "Train Epoch: 8 [512/14335 (4%)]\tLoss: 3244.860352\tLR: [4.7738095238095245e-05]\n",
            "Train Epoch: 8 [768/14335 (5%)]\tLoss: 3336.238770\tLR: [4.771825396825397e-05]\n",
            "Train Epoch: 8 [1024/14335 (7%)]\tLoss: 3155.208008\tLR: [4.7698412698412706e-05]\n",
            "Train Epoch: 8 [1280/14335 (9%)]\tLoss: 2847.442871\tLR: [4.767857142857143e-05]\n",
            "Train Epoch: 8 [1536/14335 (11%)]\tLoss: 3198.382324\tLR: [4.7658730158730166e-05]\n",
            "Train Epoch: 8 [1792/14335 (12%)]\tLoss: 3118.668945\tLR: [4.7638888888888887e-05]\n",
            "Train Epoch: 8 [2048/14335 (14%)]\tLoss: 3152.780273\tLR: [4.761904761904762e-05]\n",
            "Train Epoch: 8 [2304/14335 (16%)]\tLoss: 2883.123779\tLR: [4.759920634920635e-05]\n",
            "Train Epoch: 8 [2560/14335 (18%)]\tLoss: 2921.061279\tLR: [4.757936507936508e-05]\n",
            "Train Epoch: 8 [2816/14335 (20%)]\tLoss: 3023.565430\tLR: [4.755952380952381e-05]\n",
            "Train Epoch: 8 [3072/14335 (21%)]\tLoss: 2966.879639\tLR: [4.753968253968254e-05]\n",
            "Train Epoch: 8 [3328/14335 (23%)]\tLoss: 2795.121338\tLR: [4.751984126984127e-05]\n",
            "Train Epoch: 8 [3584/14335 (25%)]\tLoss: 3474.596924\tLR: [4.75e-05]\n",
            "Train Epoch: 8 [3840/14335 (27%)]\tLoss: 2692.149902\tLR: [4.748015873015873e-05]\n",
            "Train Epoch: 8 [4096/14335 (29%)]\tLoss: 3133.824951\tLR: [4.746031746031746e-05]\n",
            "Train Epoch: 8 [4352/14335 (30%)]\tLoss: 3187.032715\tLR: [4.744047619047619e-05]\n",
            "Train Epoch: 8 [4608/14335 (32%)]\tLoss: 2944.946533\tLR: [4.7420634920634924e-05]\n",
            "Train Epoch: 8 [4864/14335 (34%)]\tLoss: 3069.974854\tLR: [4.740079365079365e-05]\n",
            "Train Epoch: 8 [5120/14335 (36%)]\tLoss: 3400.154053\tLR: [4.738095238095238e-05]\n",
            "Train Epoch: 8 [5376/14335 (38%)]\tLoss: 3033.728027\tLR: [4.736111111111111e-05]\n",
            "Train Epoch: 8 [5632/14335 (39%)]\tLoss: 2766.187012\tLR: [4.734126984126984e-05]\n",
            "Train Epoch: 8 [5888/14335 (41%)]\tLoss: 3296.676270\tLR: [4.732142857142857e-05]\n",
            "Train Epoch: 8 [6144/14335 (43%)]\tLoss: 3173.157471\tLR: [4.73015873015873e-05]\n",
            "Train Epoch: 8 [6400/14335 (45%)]\tLoss: 2516.987061\tLR: [4.728174603174603e-05]\n",
            "Train Epoch: 8 [6656/14335 (46%)]\tLoss: 3263.630859\tLR: [4.726190476190476e-05]\n",
            "Train Epoch: 8 [6912/14335 (48%)]\tLoss: 2835.771729\tLR: [4.7242063492063494e-05]\n",
            "Train Epoch: 8 [7168/14335 (50%)]\tLoss: 2990.481689\tLR: [4.722222222222222e-05]\n",
            "Train Epoch: 8 [7424/14335 (52%)]\tLoss: 3136.656494\tLR: [4.7202380952380955e-05]\n",
            "Train Epoch: 8 [7680/14335 (54%)]\tLoss: 3024.042969\tLR: [4.718253968253968e-05]\n",
            "Train Epoch: 8 [7936/14335 (55%)]\tLoss: 3453.381104\tLR: [4.7162698412698416e-05]\n",
            "Train Epoch: 8 [8192/14335 (57%)]\tLoss: 3327.741699\tLR: [4.714285714285714e-05]\n",
            "Train Epoch: 8 [8448/14335 (59%)]\tLoss: 2847.832275\tLR: [4.7123015873015876e-05]\n",
            "Train Epoch: 8 [8704/14335 (61%)]\tLoss: 2649.931885\tLR: [4.71031746031746e-05]\n",
            "Train Epoch: 8 [8960/14335 (62%)]\tLoss: 2979.418701\tLR: [4.708333333333334e-05]\n",
            "Train Epoch: 8 [9216/14335 (64%)]\tLoss: 3124.078613\tLR: [4.7063492063492064e-05]\n",
            "Train Epoch: 8 [9472/14335 (66%)]\tLoss: 3061.736328\tLR: [4.70436507936508e-05]\n",
            "Train Epoch: 8 [9728/14335 (68%)]\tLoss: 2889.863281\tLR: [4.7023809523809525e-05]\n",
            "Train Epoch: 8 [9984/14335 (70%)]\tLoss: 3179.911621\tLR: [4.700396825396826e-05]\n",
            "Train Epoch: 8 [10240/14335 (71%)]\tLoss: 3133.311035\tLR: [4.6984126984126986e-05]\n",
            "Train Epoch: 8 [10496/14335 (73%)]\tLoss: 2614.303467\tLR: [4.696428571428572e-05]\n",
            "Train Epoch: 8 [10752/14335 (75%)]\tLoss: 2572.519043\tLR: [4.6944444444444446e-05]\n",
            "Train Epoch: 8 [11008/14335 (77%)]\tLoss: 2895.237305\tLR: [4.692460317460317e-05]\n",
            "Train Epoch: 8 [11264/14335 (79%)]\tLoss: 3367.224609\tLR: [4.690476190476191e-05]\n",
            "Train Epoch: 8 [11520/14335 (80%)]\tLoss: 3218.857422\tLR: [4.6884920634920634e-05]\n",
            "Train Epoch: 8 [11776/14335 (82%)]\tLoss: 2846.264160\tLR: [4.686507936507937e-05]\n",
            "Train Epoch: 8 [12032/14335 (84%)]\tLoss: 2828.459473\tLR: [4.6845238095238095e-05]\n",
            "Train Epoch: 8 [12288/14335 (86%)]\tLoss: 2793.089355\tLR: [4.682539682539683e-05]\n",
            "Train Epoch: 8 [12544/14335 (88%)]\tLoss: 2933.418701\tLR: [4.6805555555555556e-05]\n",
            "Train Epoch: 8 [12800/14335 (89%)]\tLoss: 2732.566162\tLR: [4.678571428571429e-05]\n",
            "Train Epoch: 8 [13056/14335 (91%)]\tLoss: 3295.634521\tLR: [4.6765873015873016e-05]\n",
            "Train Epoch: 8 [13312/14335 (93%)]\tLoss: 2860.287109\tLR: [4.674603174603175e-05]\n",
            "Train Epoch: 8 [13568/14335 (95%)]\tLoss: 2844.491211\tLR: [4.672619047619048e-05]\n",
            "Train Epoch: 8 [13824/14335 (96%)]\tLoss: 3135.831299\tLR: [4.670634920634921e-05]\n",
            "Train Epoch: 8 [14025/14335 (98%)]\tLoss: 3099.332031\tLR: [4.668650793650794e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 8 is 0.8671373555840821, Best ROC Score is:0.9312820512820513\n",
            "Starting Epoch 9\n",
            "Train Epoch: 9 [0/14335 (0%)]\tLoss: 2798.752197\tLR: [4.666666666666667e-05]\n",
            "Train Epoch: 9 [256/14335 (2%)]\tLoss: 2972.562744\tLR: [4.66468253968254e-05]\n",
            "Train Epoch: 9 [512/14335 (4%)]\tLoss: 2649.642822\tLR: [4.662698412698413e-05]\n",
            "Train Epoch: 9 [768/14335 (5%)]\tLoss: 3031.160400\tLR: [4.660714285714286e-05]\n",
            "Train Epoch: 9 [1024/14335 (7%)]\tLoss: 3291.464600\tLR: [4.658730158730159e-05]\n",
            "Train Epoch: 9 [1280/14335 (9%)]\tLoss: 2897.614746\tLR: [4.656746031746032e-05]\n",
            "Train Epoch: 9 [1536/14335 (11%)]\tLoss: 2775.685547\tLR: [4.6547619047619054e-05]\n",
            "Train Epoch: 9 [1792/14335 (12%)]\tLoss: 3094.391602\tLR: [4.652777777777778e-05]\n",
            "Train Epoch: 9 [2048/14335 (14%)]\tLoss: 2673.252197\tLR: [4.6507936507936515e-05]\n",
            "Train Epoch: 9 [2304/14335 (16%)]\tLoss: 3248.989014\tLR: [4.648809523809524e-05]\n",
            "Train Epoch: 9 [2560/14335 (18%)]\tLoss: 3238.625488\tLR: [4.646825396825397e-05]\n",
            "Train Epoch: 9 [2816/14335 (20%)]\tLoss: 3115.878906\tLR: [4.64484126984127e-05]\n",
            "Train Epoch: 9 [3072/14335 (21%)]\tLoss: 2818.199463\tLR: [4.642857142857143e-05]\n",
            "Train Epoch: 9 [3328/14335 (23%)]\tLoss: 3109.308350\tLR: [4.640873015873016e-05]\n",
            "Train Epoch: 9 [3584/14335 (25%)]\tLoss: 3097.539307\tLR: [4.638888888888889e-05]\n",
            "Train Epoch: 9 [3840/14335 (27%)]\tLoss: 3153.069336\tLR: [4.6369047619047624e-05]\n",
            "Train Epoch: 9 [4096/14335 (29%)]\tLoss: 2863.932373\tLR: [4.634920634920635e-05]\n",
            "Train Epoch: 9 [4352/14335 (30%)]\tLoss: 3164.688477\tLR: [4.6329365079365085e-05]\n",
            "Train Epoch: 9 [4608/14335 (32%)]\tLoss: 3282.773682\tLR: [4.630952380952381e-05]\n",
            "Train Epoch: 9 [4864/14335 (34%)]\tLoss: 2790.670410\tLR: [4.6289682539682545e-05]\n",
            "Train Epoch: 9 [5120/14335 (36%)]\tLoss: 2889.245361\tLR: [4.626984126984127e-05]\n",
            "Train Epoch: 9 [5376/14335 (38%)]\tLoss: 2847.588135\tLR: [4.6250000000000006e-05]\n",
            "Train Epoch: 9 [5632/14335 (39%)]\tLoss: 2948.673584\tLR: [4.623015873015873e-05]\n",
            "Train Epoch: 9 [5888/14335 (41%)]\tLoss: 2810.113037\tLR: [4.621031746031747e-05]\n",
            "Train Epoch: 9 [6144/14335 (43%)]\tLoss: 2878.550293\tLR: [4.6190476190476194e-05]\n",
            "Train Epoch: 9 [6400/14335 (45%)]\tLoss: 3035.110352\tLR: [4.617063492063493e-05]\n",
            "Train Epoch: 9 [6656/14335 (46%)]\tLoss: 3238.731934\tLR: [4.6150793650793655e-05]\n",
            "Train Epoch: 9 [6912/14335 (48%)]\tLoss: 2895.869385\tLR: [4.613095238095239e-05]\n",
            "Train Epoch: 9 [7168/14335 (50%)]\tLoss: 2974.552979\tLR: [4.6111111111111115e-05]\n",
            "Train Epoch: 9 [7424/14335 (52%)]\tLoss: 3429.674805\tLR: [4.609126984126984e-05]\n",
            "Train Epoch: 9 [7680/14335 (54%)]\tLoss: 3416.436035\tLR: [4.607142857142857e-05]\n",
            "Train Epoch: 9 [7936/14335 (55%)]\tLoss: 3204.455322\tLR: [4.60515873015873e-05]\n",
            "Train Epoch: 9 [8192/14335 (57%)]\tLoss: 2796.310791\tLR: [4.603174603174603e-05]\n",
            "Train Epoch: 9 [8448/14335 (59%)]\tLoss: 3153.833740\tLR: [4.6011904761904764e-05]\n",
            "Train Epoch: 9 [8704/14335 (61%)]\tLoss: 3071.110107\tLR: [4.599206349206349e-05]\n",
            "Train Epoch: 9 [8960/14335 (62%)]\tLoss: 2732.492676\tLR: [4.5972222222222225e-05]\n",
            "Train Epoch: 9 [9216/14335 (64%)]\tLoss: 2707.874512\tLR: [4.595238095238095e-05]\n",
            "Train Epoch: 9 [9472/14335 (66%)]\tLoss: 3001.525146\tLR: [4.5932539682539685e-05]\n",
            "Train Epoch: 9 [9728/14335 (68%)]\tLoss: 3227.305176\tLR: [4.591269841269841e-05]\n",
            "Train Epoch: 9 [9984/14335 (70%)]\tLoss: 2756.895020\tLR: [4.5892857142857146e-05]\n",
            "Train Epoch: 9 [10240/14335 (71%)]\tLoss: 2833.616455\tLR: [4.587301587301587e-05]\n",
            "Train Epoch: 9 [10496/14335 (73%)]\tLoss: 2924.905518\tLR: [4.58531746031746e-05]\n",
            "Train Epoch: 9 [10752/14335 (75%)]\tLoss: 2693.190918\tLR: [4.5833333333333334e-05]\n",
            "Train Epoch: 9 [11008/14335 (77%)]\tLoss: 2962.891357\tLR: [4.581349206349206e-05]\n",
            "Train Epoch: 9 [11264/14335 (79%)]\tLoss: 2656.507080\tLR: [4.5793650793650795e-05]\n",
            "Train Epoch: 9 [11520/14335 (80%)]\tLoss: 2922.915527\tLR: [4.577380952380952e-05]\n",
            "Train Epoch: 9 [11776/14335 (82%)]\tLoss: 2914.596680\tLR: [4.5753968253968255e-05]\n",
            "Train Epoch: 9 [12032/14335 (84%)]\tLoss: 3202.258545\tLR: [4.573412698412698e-05]\n",
            "Train Epoch: 9 [12288/14335 (86%)]\tLoss: 3077.961914\tLR: [4.5714285714285716e-05]\n",
            "Train Epoch: 9 [12544/14335 (88%)]\tLoss: 3489.240234\tLR: [4.569444444444444e-05]\n",
            "Train Epoch: 9 [12800/14335 (89%)]\tLoss: 2927.269043\tLR: [4.567460317460318e-05]\n",
            "Train Epoch: 9 [13056/14335 (91%)]\tLoss: 3067.496094\tLR: [4.5654761904761904e-05]\n",
            "Train Epoch: 9 [13312/14335 (93%)]\tLoss: 2919.509521\tLR: [4.563492063492064e-05]\n",
            "Train Epoch: 9 [13568/14335 (95%)]\tLoss: 2750.166016\tLR: [4.5615079365079365e-05]\n",
            "Train Epoch: 9 [13824/14335 (96%)]\tLoss: 3264.116699\tLR: [4.55952380952381e-05]\n",
            "Train Epoch: 9 [14025/14335 (98%)]\tLoss: 2804.501953\tLR: [4.5575396825396825e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 9 is 0.8158803222094361, Best ROC Score is:0.9312820512820513\n",
            "Starting Epoch 10\n",
            "Train Epoch: 10 [0/14335 (0%)]\tLoss: 3266.313477\tLR: [4.555555555555556e-05]\n",
            "Train Epoch: 10 [256/14335 (2%)]\tLoss: 3023.053711\tLR: [4.5535714285714286e-05]\n",
            "Train Epoch: 10 [512/14335 (4%)]\tLoss: 3342.562012\tLR: [4.551587301587302e-05]\n",
            "Train Epoch: 10 [768/14335 (5%)]\tLoss: 2894.702148\tLR: [4.549603174603175e-05]\n",
            "Train Epoch: 10 [1024/14335 (7%)]\tLoss: 3039.643555\tLR: [4.547619047619048e-05]\n",
            "Train Epoch: 10 [1280/14335 (9%)]\tLoss: 2582.468750\tLR: [4.545634920634921e-05]\n",
            "Train Epoch: 10 [1536/14335 (11%)]\tLoss: 3168.117676\tLR: [4.543650793650794e-05]\n",
            "Train Epoch: 10 [1792/14335 (12%)]\tLoss: 2879.171875\tLR: [4.541666666666667e-05]\n",
            "Train Epoch: 10 [2048/14335 (14%)]\tLoss: 3309.161865\tLR: [4.5396825396825395e-05]\n",
            "Train Epoch: 10 [2304/14335 (16%)]\tLoss: 3099.600098\tLR: [4.537698412698413e-05]\n",
            "Train Epoch: 10 [2560/14335 (18%)]\tLoss: 2774.485840\tLR: [4.5357142857142856e-05]\n",
            "Train Epoch: 10 [2816/14335 (20%)]\tLoss: 2897.000244\tLR: [4.533730158730159e-05]\n",
            "Train Epoch: 10 [3072/14335 (21%)]\tLoss: 2870.788330\tLR: [4.531746031746032e-05]\n",
            "Train Epoch: 10 [3328/14335 (23%)]\tLoss: 2899.080078\tLR: [4.529761904761905e-05]\n",
            "Train Epoch: 10 [3584/14335 (25%)]\tLoss: 3100.202148\tLR: [4.527777777777778e-05]\n",
            "Train Epoch: 10 [3840/14335 (27%)]\tLoss: 3072.685791\tLR: [4.525793650793651e-05]\n",
            "Train Epoch: 10 [4096/14335 (29%)]\tLoss: 3049.097900\tLR: [4.523809523809524e-05]\n",
            "Train Epoch: 10 [4352/14335 (30%)]\tLoss: 2970.478760\tLR: [4.521825396825397e-05]\n",
            "Train Epoch: 10 [4608/14335 (32%)]\tLoss: 2792.171387\tLR: [4.51984126984127e-05]\n",
            "Train Epoch: 10 [4864/14335 (34%)]\tLoss: 2684.133545\tLR: [4.517857142857143e-05]\n",
            "Train Epoch: 10 [5120/14335 (36%)]\tLoss: 2788.531250\tLR: [4.515873015873016e-05]\n",
            "Train Epoch: 10 [5376/14335 (38%)]\tLoss: 3076.339600\tLR: [4.5138888888888894e-05]\n",
            "Train Epoch: 10 [5632/14335 (39%)]\tLoss: 3432.363037\tLR: [4.511904761904762e-05]\n",
            "Train Epoch: 10 [5888/14335 (41%)]\tLoss: 2593.698242\tLR: [4.5099206349206354e-05]\n",
            "Train Epoch: 10 [6144/14335 (43%)]\tLoss: 2688.784912\tLR: [4.507936507936508e-05]\n",
            "Train Epoch: 10 [6400/14335 (45%)]\tLoss: 2694.116211\tLR: [4.5059523809523815e-05]\n",
            "Train Epoch: 10 [6656/14335 (46%)]\tLoss: 3413.853516\tLR: [4.503968253968254e-05]\n",
            "Train Epoch: 10 [6912/14335 (48%)]\tLoss: 3236.519775\tLR: [4.5019841269841276e-05]\n",
            "Train Epoch: 10 [7168/14335 (50%)]\tLoss: 2622.842773\tLR: [4.5e-05]\n",
            "Train Epoch: 10 [7424/14335 (52%)]\tLoss: 2827.212158\tLR: [4.4980158730158737e-05]\n",
            "Train Epoch: 10 [7680/14335 (54%)]\tLoss: 2545.748047\tLR: [4.4960317460317464e-05]\n",
            "Train Epoch: 10 [7936/14335 (55%)]\tLoss: 3054.866943\tLR: [4.494047619047619e-05]\n",
            "Train Epoch: 10 [8192/14335 (57%)]\tLoss: 2898.578369\tLR: [4.4920634920634924e-05]\n",
            "Train Epoch: 10 [8448/14335 (59%)]\tLoss: 3260.780518\tLR: [4.490079365079365e-05]\n",
            "Train Epoch: 10 [8704/14335 (61%)]\tLoss: 2890.888428\tLR: [4.4880952380952385e-05]\n",
            "Train Epoch: 10 [8960/14335 (62%)]\tLoss: 2854.751953\tLR: [4.486111111111111e-05]\n",
            "Train Epoch: 10 [9216/14335 (64%)]\tLoss: 3101.495117\tLR: [4.4841269841269846e-05]\n",
            "Train Epoch: 10 [9472/14335 (66%)]\tLoss: 3351.956299\tLR: [4.482142857142857e-05]\n",
            "Train Epoch: 10 [9728/14335 (68%)]\tLoss: 3063.849121\tLR: [4.4801587301587307e-05]\n",
            "Train Epoch: 10 [9984/14335 (70%)]\tLoss: 3171.510986\tLR: [4.4781746031746034e-05]\n",
            "Train Epoch: 10 [10240/14335 (71%)]\tLoss: 3201.389648\tLR: [4.476190476190477e-05]\n",
            "Train Epoch: 10 [10496/14335 (73%)]\tLoss: 3071.437012\tLR: [4.4742063492063494e-05]\n",
            "Train Epoch: 10 [10752/14335 (75%)]\tLoss: 3063.854004\tLR: [4.472222222222223e-05]\n",
            "Train Epoch: 10 [11008/14335 (77%)]\tLoss: 3187.446289\tLR: [4.4702380952380955e-05]\n",
            "Train Epoch: 10 [11264/14335 (79%)]\tLoss: 3187.070801\tLR: [4.468253968253969e-05]\n",
            "Train Epoch: 10 [11520/14335 (80%)]\tLoss: 3355.830078\tLR: [4.4662698412698416e-05]\n",
            "Train Epoch: 10 [11776/14335 (82%)]\tLoss: 2899.287842\tLR: [4.464285714285715e-05]\n",
            "Train Epoch: 10 [12032/14335 (84%)]\tLoss: 2859.093506\tLR: [4.4623015873015877e-05]\n",
            "Train Epoch: 10 [12288/14335 (86%)]\tLoss: 2977.997314\tLR: [4.460317460317461e-05]\n",
            "Train Epoch: 10 [12544/14335 (88%)]\tLoss: 2937.455811\tLR: [4.458333333333334e-05]\n",
            "Train Epoch: 10 [12800/14335 (89%)]\tLoss: 2776.749268\tLR: [4.456349206349207e-05]\n",
            "Train Epoch: 10 [13056/14335 (91%)]\tLoss: 3302.758789\tLR: [4.45436507936508e-05]\n",
            "Train Epoch: 10 [13312/14335 (93%)]\tLoss: 2679.725830\tLR: [4.4523809523809525e-05]\n",
            "Train Epoch: 10 [13568/14335 (95%)]\tLoss: 3391.644775\tLR: [4.450396825396825e-05]\n",
            "Train Epoch: 10 [13824/14335 (96%)]\tLoss: 2828.584717\tLR: [4.4484126984126986e-05]\n",
            "Train Epoch: 10 [14025/14335 (98%)]\tLoss: 3175.276367\tLR: [4.446428571428571e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 10 is 0.8110367892976589, Best ROC Score is:0.9312820512820513\n",
            "Starting Epoch 11\n",
            "Train Epoch: 11 [0/14335 (0%)]\tLoss: 2780.646484\tLR: [4.4444444444444447e-05]\n",
            "Train Epoch: 11 [256/14335 (2%)]\tLoss: 3066.449707\tLR: [4.4424603174603174e-05]\n",
            "Train Epoch: 11 [512/14335 (4%)]\tLoss: 2735.091309\tLR: [4.440476190476191e-05]\n",
            "Train Epoch: 11 [768/14335 (5%)]\tLoss: 3220.842285\tLR: [4.4384920634920634e-05]\n",
            "Train Epoch: 11 [1024/14335 (7%)]\tLoss: 3163.301758\tLR: [4.436507936507937e-05]\n",
            "Train Epoch: 11 [1280/14335 (9%)]\tLoss: 3004.824463\tLR: [4.4345238095238095e-05]\n",
            "Train Epoch: 11 [1536/14335 (11%)]\tLoss: 3008.481445\tLR: [4.432539682539683e-05]\n",
            "Train Epoch: 11 [1792/14335 (12%)]\tLoss: 2984.716064\tLR: [4.4305555555555556e-05]\n",
            "Train Epoch: 11 [2048/14335 (14%)]\tLoss: 3326.370605\tLR: [4.428571428571428e-05]\n",
            "Train Epoch: 11 [2304/14335 (16%)]\tLoss: 2927.568359\tLR: [4.4265873015873017e-05]\n",
            "Train Epoch: 11 [2560/14335 (18%)]\tLoss: 3127.349365\tLR: [4.4246031746031744e-05]\n",
            "Train Epoch: 11 [2816/14335 (20%)]\tLoss: 2828.091553\tLR: [4.422619047619048e-05]\n",
            "Train Epoch: 11 [3072/14335 (21%)]\tLoss: 3064.508789\tLR: [4.4206349206349204e-05]\n",
            "Train Epoch: 11 [3328/14335 (23%)]\tLoss: 2865.645264\tLR: [4.418650793650794e-05]\n",
            "Train Epoch: 11 [3584/14335 (25%)]\tLoss: 2767.489258\tLR: [4.4166666666666665e-05]\n",
            "Train Epoch: 11 [3840/14335 (27%)]\tLoss: 3188.721680\tLR: [4.41468253968254e-05]\n",
            "Train Epoch: 11 [4096/14335 (29%)]\tLoss: 2536.359863\tLR: [4.4126984126984126e-05]\n",
            "Train Epoch: 11 [4352/14335 (30%)]\tLoss: 2999.076904\tLR: [4.410714285714286e-05]\n",
            "Train Epoch: 11 [4608/14335 (32%)]\tLoss: 3239.866455\tLR: [4.4087301587301587e-05]\n",
            "Train Epoch: 11 [4864/14335 (34%)]\tLoss: 3055.952393\tLR: [4.406746031746032e-05]\n",
            "Train Epoch: 11 [5120/14335 (36%)]\tLoss: 2994.249512\tLR: [4.404761904761905e-05]\n",
            "Train Epoch: 11 [5376/14335 (38%)]\tLoss: 3260.100342\tLR: [4.402777777777778e-05]\n",
            "Train Epoch: 11 [5632/14335 (39%)]\tLoss: 3019.668213\tLR: [4.400793650793651e-05]\n",
            "Train Epoch: 11 [5888/14335 (41%)]\tLoss: 2951.504395\tLR: [4.398809523809524e-05]\n",
            "Train Epoch: 11 [6144/14335 (43%)]\tLoss: 3612.227295\tLR: [4.396825396825397e-05]\n",
            "Train Epoch: 11 [6400/14335 (45%)]\tLoss: 2904.365479\tLR: [4.39484126984127e-05]\n",
            "Train Epoch: 11 [6656/14335 (46%)]\tLoss: 2950.618896\tLR: [4.392857142857143e-05]\n",
            "Train Epoch: 11 [6912/14335 (48%)]\tLoss: 2506.672363\tLR: [4.390873015873016e-05]\n",
            "Train Epoch: 11 [7168/14335 (50%)]\tLoss: 2970.786377\tLR: [4.388888888888889e-05]\n",
            "Train Epoch: 11 [7424/14335 (52%)]\tLoss: 3174.104736\tLR: [4.3869047619047624e-05]\n",
            "Train Epoch: 11 [7680/14335 (54%)]\tLoss: 2749.670410\tLR: [4.384920634920635e-05]\n",
            "Train Epoch: 11 [7936/14335 (55%)]\tLoss: 2725.068604\tLR: [4.382936507936508e-05]\n",
            "Train Epoch: 11 [8192/14335 (57%)]\tLoss: 2923.831055\tLR: [4.380952380952381e-05]\n",
            "Train Epoch: 11 [8448/14335 (59%)]\tLoss: 2774.519775\tLR: [4.378968253968254e-05]\n",
            "Train Epoch: 11 [8704/14335 (61%)]\tLoss: 3285.763916\tLR: [4.376984126984127e-05]\n",
            "Train Epoch: 11 [8960/14335 (62%)]\tLoss: 2728.323730\tLR: [4.375e-05]\n",
            "Train Epoch: 11 [9216/14335 (64%)]\tLoss: 2719.916260\tLR: [4.373015873015873e-05]\n",
            "Train Epoch: 11 [9472/14335 (66%)]\tLoss: 3302.513916\tLR: [4.371031746031746e-05]\n",
            "Train Epoch: 11 [9728/14335 (68%)]\tLoss: 3015.237793\tLR: [4.3690476190476194e-05]\n",
            "Train Epoch: 11 [9984/14335 (70%)]\tLoss: 2504.232666\tLR: [4.367063492063492e-05]\n",
            "Train Epoch: 11 [10240/14335 (71%)]\tLoss: 3117.476562\tLR: [4.3650793650793655e-05]\n",
            "Train Epoch: 11 [10496/14335 (73%)]\tLoss: 3060.387451\tLR: [4.363095238095238e-05]\n",
            "Train Epoch: 11 [10752/14335 (75%)]\tLoss: 3114.178955\tLR: [4.3611111111111116e-05]\n",
            "Train Epoch: 11 [11008/14335 (77%)]\tLoss: 2791.771973\tLR: [4.359126984126984e-05]\n",
            "Train Epoch: 11 [11264/14335 (79%)]\tLoss: 3455.797119\tLR: [4.3571428571428576e-05]\n",
            "Train Epoch: 11 [11520/14335 (80%)]\tLoss: 3118.297363\tLR: [4.35515873015873e-05]\n",
            "Train Epoch: 11 [11776/14335 (82%)]\tLoss: 3602.662598\tLR: [4.353174603174604e-05]\n",
            "Train Epoch: 11 [12032/14335 (84%)]\tLoss: 2937.870361\tLR: [4.3511904761904764e-05]\n",
            "Train Epoch: 11 [12288/14335 (86%)]\tLoss: 2937.710938\tLR: [4.34920634920635e-05]\n",
            "Train Epoch: 11 [12544/14335 (88%)]\tLoss: 3375.262207\tLR: [4.3472222222222225e-05]\n",
            "Train Epoch: 11 [12800/14335 (89%)]\tLoss: 2865.154541\tLR: [4.345238095238096e-05]\n",
            "Train Epoch: 11 [13056/14335 (91%)]\tLoss: 3261.448975\tLR: [4.3432539682539686e-05]\n",
            "Train Epoch: 11 [13312/14335 (93%)]\tLoss: 3000.109375\tLR: [4.341269841269842e-05]\n",
            "Train Epoch: 11 [13568/14335 (95%)]\tLoss: 3058.342285\tLR: [4.3392857142857146e-05]\n",
            "Train Epoch: 11 [13824/14335 (96%)]\tLoss: 3103.487793\tLR: [4.337301587301587e-05]\n",
            "Train Epoch: 11 [14025/14335 (98%)]\tLoss: 3005.552734\tLR: [4.335317460317461e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 11 is 0.8215384615384616, Best ROC Score is:0.9312820512820513\n",
            "Starting Epoch 12\n",
            "Train Epoch: 12 [0/14335 (0%)]\tLoss: 2757.525391\tLR: [4.3333333333333334e-05]\n",
            "Train Epoch: 12 [256/14335 (2%)]\tLoss: 3159.117676\tLR: [4.331349206349207e-05]\n",
            "Train Epoch: 12 [512/14335 (4%)]\tLoss: 2943.613770\tLR: [4.3293650793650795e-05]\n",
            "Train Epoch: 12 [768/14335 (5%)]\tLoss: 3070.690430\tLR: [4.327380952380953e-05]\n",
            "Train Epoch: 12 [1024/14335 (7%)]\tLoss: 3539.017090\tLR: [4.3253968253968256e-05]\n",
            "Train Epoch: 12 [1280/14335 (9%)]\tLoss: 3162.903809\tLR: [4.323412698412699e-05]\n",
            "Train Epoch: 12 [1536/14335 (11%)]\tLoss: 2807.728271\tLR: [4.3214285714285716e-05]\n",
            "Train Epoch: 12 [1792/14335 (12%)]\tLoss: 3284.802246\tLR: [4.319444444444445e-05]\n",
            "Train Epoch: 12 [2048/14335 (14%)]\tLoss: 2909.683105\tLR: [4.317460317460318e-05]\n",
            "Train Epoch: 12 [2304/14335 (16%)]\tLoss: 2711.784180\tLR: [4.315476190476191e-05]\n",
            "Train Epoch: 12 [2560/14335 (18%)]\tLoss: 2907.159668\tLR: [4.313492063492064e-05]\n",
            "Train Epoch: 12 [2816/14335 (20%)]\tLoss: 2633.758301\tLR: [4.311507936507937e-05]\n",
            "Train Epoch: 12 [3072/14335 (21%)]\tLoss: 2723.513916\tLR: [4.30952380952381e-05]\n",
            "Train Epoch: 12 [3328/14335 (23%)]\tLoss: 3127.100586\tLR: [4.307539682539683e-05]\n",
            "Train Epoch: 12 [3584/14335 (25%)]\tLoss: 2523.451172\tLR: [4.305555555555556e-05]\n",
            "Train Epoch: 12 [3840/14335 (27%)]\tLoss: 2854.501465\tLR: [4.303571428571429e-05]\n",
            "Train Epoch: 12 [4096/14335 (29%)]\tLoss: 2866.953369\tLR: [4.301587301587302e-05]\n",
            "Train Epoch: 12 [4352/14335 (30%)]\tLoss: 3152.202637\tLR: [4.2996031746031754e-05]\n",
            "Train Epoch: 12 [4608/14335 (32%)]\tLoss: 2947.062012\tLR: [4.297619047619048e-05]\n",
            "Train Epoch: 12 [4864/14335 (34%)]\tLoss: 2913.623047\tLR: [4.295634920634921e-05]\n",
            "Train Epoch: 12 [5120/14335 (36%)]\tLoss: 2783.959961\tLR: [4.2936507936507935e-05]\n",
            "Train Epoch: 12 [5376/14335 (38%)]\tLoss: 3185.922119\tLR: [4.291666666666667e-05]\n",
            "Train Epoch: 12 [5632/14335 (39%)]\tLoss: 3049.623779\tLR: [4.2896825396825396e-05]\n",
            "Train Epoch: 12 [5888/14335 (41%)]\tLoss: 2925.138428\tLR: [4.287698412698413e-05]\n",
            "Train Epoch: 12 [6144/14335 (43%)]\tLoss: 3230.296143\tLR: [4.2857142857142856e-05]\n",
            "Train Epoch: 12 [6400/14335 (45%)]\tLoss: 3156.883301\tLR: [4.283730158730159e-05]\n",
            "Train Epoch: 12 [6656/14335 (46%)]\tLoss: 2767.032715\tLR: [4.281746031746032e-05]\n",
            "Train Epoch: 12 [6912/14335 (48%)]\tLoss: 2840.308838\tLR: [4.279761904761905e-05]\n",
            "Train Epoch: 12 [7168/14335 (50%)]\tLoss: 3068.364746\tLR: [4.277777777777778e-05]\n",
            "Train Epoch: 12 [7424/14335 (52%)]\tLoss: 3195.955811\tLR: [4.2757936507936505e-05]\n",
            "Train Epoch: 12 [7680/14335 (54%)]\tLoss: 2709.902832\tLR: [4.273809523809524e-05]\n",
            "Train Epoch: 12 [7936/14335 (55%)]\tLoss: 3055.773193\tLR: [4.2718253968253966e-05]\n",
            "Train Epoch: 12 [8192/14335 (57%)]\tLoss: 2890.566162\tLR: [4.26984126984127e-05]\n",
            "Train Epoch: 12 [8448/14335 (59%)]\tLoss: 2602.391113\tLR: [4.2678571428571426e-05]\n",
            "Train Epoch: 12 [8704/14335 (61%)]\tLoss: 2672.799561\tLR: [4.265873015873016e-05]\n",
            "Train Epoch: 12 [8960/14335 (62%)]\tLoss: 3038.880127\tLR: [4.263888888888889e-05]\n",
            "Train Epoch: 12 [9216/14335 (64%)]\tLoss: 3185.730225\tLR: [4.261904761904762e-05]\n",
            "Train Epoch: 12 [9472/14335 (66%)]\tLoss: 3426.064697\tLR: [4.259920634920635e-05]\n",
            "Train Epoch: 12 [9728/14335 (68%)]\tLoss: 3214.077637\tLR: [4.257936507936508e-05]\n",
            "Train Epoch: 12 [9984/14335 (70%)]\tLoss: 2721.200195\tLR: [4.255952380952381e-05]\n",
            "Train Epoch: 12 [10240/14335 (71%)]\tLoss: 2964.362793\tLR: [4.253968253968254e-05]\n",
            "Train Epoch: 12 [10496/14335 (73%)]\tLoss: 3277.626953\tLR: [4.251984126984127e-05]\n",
            "Train Epoch: 12 [10752/14335 (75%)]\tLoss: 3086.015625\tLR: [4.25e-05]\n",
            "Train Epoch: 12 [11008/14335 (77%)]\tLoss: 3628.818115\tLR: [4.248015873015873e-05]\n",
            "Train Epoch: 12 [11264/14335 (79%)]\tLoss: 2832.269531\tLR: [4.2460317460317464e-05]\n",
            "Train Epoch: 12 [11520/14335 (80%)]\tLoss: 3186.120850\tLR: [4.244047619047619e-05]\n",
            "Train Epoch: 12 [11776/14335 (82%)]\tLoss: 2938.741699\tLR: [4.2420634920634925e-05]\n",
            "Train Epoch: 12 [12032/14335 (84%)]\tLoss: 3045.509766\tLR: [4.240079365079365e-05]\n",
            "Train Epoch: 12 [12288/14335 (86%)]\tLoss: 3148.901611\tLR: [4.2380952380952385e-05]\n",
            "Train Epoch: 12 [12544/14335 (88%)]\tLoss: 3324.763428\tLR: [4.236111111111111e-05]\n",
            "Train Epoch: 12 [12800/14335 (89%)]\tLoss: 2433.048828\tLR: [4.2341269841269846e-05]\n",
            "Train Epoch: 12 [13056/14335 (91%)]\tLoss: 2951.988037\tLR: [4.232142857142857e-05]\n",
            "Train Epoch: 12 [13312/14335 (93%)]\tLoss: 3332.199951\tLR: [4.23015873015873e-05]\n",
            "Train Epoch: 12 [13568/14335 (95%)]\tLoss: 3182.027588\tLR: [4.2281746031746034e-05]\n",
            "Train Epoch: 12 [13824/14335 (96%)]\tLoss: 3142.051758\tLR: [4.226190476190476e-05]\n",
            "Train Epoch: 12 [14025/14335 (98%)]\tLoss: 3201.155762\tLR: [4.2242063492063495e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 12 is 0.8358024691358025, Best ROC Score is:0.9312820512820513\n",
            "Starting Epoch 13\n",
            "Train Epoch: 13 [0/14335 (0%)]\tLoss: 3011.123047\tLR: [4.222222222222222e-05]\n",
            "Train Epoch: 13 [256/14335 (2%)]\tLoss: 3073.749756\tLR: [4.2202380952380955e-05]\n",
            "Train Epoch: 13 [512/14335 (4%)]\tLoss: 2849.677734\tLR: [4.218253968253968e-05]\n",
            "Train Epoch: 13 [768/14335 (5%)]\tLoss: 2941.521484\tLR: [4.2162698412698416e-05]\n",
            "Train Epoch: 13 [1024/14335 (7%)]\tLoss: 2948.201172\tLR: [4.214285714285714e-05]\n",
            "Train Epoch: 13 [1280/14335 (9%)]\tLoss: 3060.525879\tLR: [4.212301587301588e-05]\n",
            "Train Epoch: 13 [1536/14335 (11%)]\tLoss: 3047.584717\tLR: [4.2103174603174604e-05]\n",
            "Train Epoch: 13 [1792/14335 (12%)]\tLoss: 3215.983154\tLR: [4.208333333333334e-05]\n",
            "Train Epoch: 13 [2048/14335 (14%)]\tLoss: 3070.219482\tLR: [4.2063492063492065e-05]\n",
            "Train Epoch: 13 [2304/14335 (16%)]\tLoss: 2974.002686\tLR: [4.20436507936508e-05]\n",
            "Train Epoch: 13 [2560/14335 (18%)]\tLoss: 3395.769287\tLR: [4.2023809523809525e-05]\n",
            "Train Epoch: 13 [2816/14335 (20%)]\tLoss: 3119.915527\tLR: [4.200396825396826e-05]\n",
            "Train Epoch: 13 [3072/14335 (21%)]\tLoss: 2674.838623\tLR: [4.1984126984126986e-05]\n",
            "Train Epoch: 13 [3328/14335 (23%)]\tLoss: 2724.054688\tLR: [4.196428571428572e-05]\n",
            "Train Epoch: 13 [3584/14335 (25%)]\tLoss: 2851.627197\tLR: [4.194444444444445e-05]\n",
            "Train Epoch: 13 [3840/14335 (27%)]\tLoss: 3082.437500\tLR: [4.192460317460318e-05]\n",
            "Train Epoch: 13 [4096/14335 (29%)]\tLoss: 3064.903809\tLR: [4.190476190476191e-05]\n",
            "Train Epoch: 13 [4352/14335 (30%)]\tLoss: 2855.329834\tLR: [4.188492063492064e-05]\n",
            "Train Epoch: 13 [4608/14335 (32%)]\tLoss: 3064.812256\tLR: [4.186507936507937e-05]\n",
            "Train Epoch: 13 [4864/14335 (34%)]\tLoss: 2912.802979\tLR: [4.1845238095238095e-05]\n",
            "Train Epoch: 13 [5120/14335 (36%)]\tLoss: 2805.822510\tLR: [4.182539682539683e-05]\n",
            "Train Epoch: 13 [5376/14335 (38%)]\tLoss: 2723.600830\tLR: [4.1805555555555556e-05]\n",
            "Train Epoch: 13 [5632/14335 (39%)]\tLoss: 2711.374756\tLR: [4.178571428571429e-05]\n",
            "Train Epoch: 13 [5888/14335 (41%)]\tLoss: 3408.026123\tLR: [4.176587301587302e-05]\n",
            "Train Epoch: 13 [6144/14335 (43%)]\tLoss: 2896.594238\tLR: [4.174603174603175e-05]\n",
            "Train Epoch: 13 [6400/14335 (45%)]\tLoss: 2715.463623\tLR: [4.172619047619048e-05]\n",
            "Train Epoch: 13 [6656/14335 (46%)]\tLoss: 2932.577393\tLR: [4.170634920634921e-05]\n",
            "Train Epoch: 13 [6912/14335 (48%)]\tLoss: 2798.034424\tLR: [4.168650793650794e-05]\n",
            "Train Epoch: 13 [7168/14335 (50%)]\tLoss: 3112.592041\tLR: [4.166666666666667e-05]\n",
            "Train Epoch: 13 [7424/14335 (52%)]\tLoss: 3185.828857\tLR: [4.16468253968254e-05]\n",
            "Train Epoch: 13 [7680/14335 (54%)]\tLoss: 3120.257812\tLR: [4.162698412698413e-05]\n",
            "Train Epoch: 13 [7936/14335 (55%)]\tLoss: 3071.913086\tLR: [4.160714285714286e-05]\n",
            "Train Epoch: 13 [8192/14335 (57%)]\tLoss: 2987.065674\tLR: [4.1587301587301594e-05]\n",
            "Train Epoch: 13 [8448/14335 (59%)]\tLoss: 3247.587891\tLR: [4.156746031746032e-05]\n",
            "Train Epoch: 13 [8704/14335 (61%)]\tLoss: 3088.752197\tLR: [4.1547619047619054e-05]\n",
            "Train Epoch: 13 [8960/14335 (62%)]\tLoss: 3006.383545\tLR: [4.152777777777778e-05]\n",
            "Train Epoch: 13 [9216/14335 (64%)]\tLoss: 3231.716064\tLR: [4.1507936507936515e-05]\n",
            "Train Epoch: 13 [9472/14335 (66%)]\tLoss: 3006.823242\tLR: [4.148809523809524e-05]\n",
            "Train Epoch: 13 [9728/14335 (68%)]\tLoss: 3392.763916\tLR: [4.1468253968253976e-05]\n",
            "Train Epoch: 13 [9984/14335 (70%)]\tLoss: 3048.394531\tLR: [4.14484126984127e-05]\n",
            "Train Epoch: 13 [10240/14335 (71%)]\tLoss: 2999.900635\tLR: [4.1428571428571437e-05]\n",
            "Train Epoch: 13 [10496/14335 (73%)]\tLoss: 2785.206055\tLR: [4.1408730158730164e-05]\n",
            "Train Epoch: 13 [10752/14335 (75%)]\tLoss: 2843.887451\tLR: [4.138888888888889e-05]\n",
            "Train Epoch: 13 [11008/14335 (77%)]\tLoss: 3208.303467\tLR: [4.136904761904762e-05]\n",
            "Train Epoch: 13 [11264/14335 (79%)]\tLoss: 3411.745605\tLR: [4.134920634920635e-05]\n",
            "Train Epoch: 13 [11520/14335 (80%)]\tLoss: 3027.028076\tLR: [4.132936507936508e-05]\n",
            "Train Epoch: 13 [11776/14335 (82%)]\tLoss: 3091.801514\tLR: [4.130952380952381e-05]\n",
            "Train Epoch: 13 [12032/14335 (84%)]\tLoss: 2845.224854\tLR: [4.128968253968254e-05]\n",
            "Train Epoch: 13 [12288/14335 (86%)]\tLoss: 2919.599121\tLR: [4.126984126984127e-05]\n",
            "Train Epoch: 13 [12544/14335 (88%)]\tLoss: 2809.221436\tLR: [4.125e-05]\n",
            "Train Epoch: 13 [12800/14335 (89%)]\tLoss: 2893.082520\tLR: [4.123015873015873e-05]\n",
            "Train Epoch: 13 [13056/14335 (91%)]\tLoss: 3227.481445\tLR: [4.121031746031746e-05]\n",
            "Train Epoch: 13 [13312/14335 (93%)]\tLoss: 3161.278076\tLR: [4.119047619047619e-05]\n",
            "Train Epoch: 13 [13568/14335 (95%)]\tLoss: 2967.929199\tLR: [4.117063492063492e-05]\n",
            "Train Epoch: 13 [13824/14335 (96%)]\tLoss: 2953.669922\tLR: [4.115079365079365e-05]\n",
            "Train Epoch: 13 [14025/14335 (98%)]\tLoss: 3199.061523\tLR: [4.113095238095238e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 13 is 0.8143812709030099, Best ROC Score is:0.9312820512820513\n",
            "Starting Epoch 14\n",
            "Train Epoch: 14 [0/14335 (0%)]\tLoss: 2724.658691\tLR: [4.111111111111111e-05]\n",
            "Train Epoch: 14 [256/14335 (2%)]\tLoss: 3173.158447\tLR: [4.109126984126984e-05]\n",
            "Train Epoch: 14 [512/14335 (4%)]\tLoss: 3412.577148\tLR: [4.107142857142857e-05]\n",
            "Train Epoch: 14 [768/14335 (5%)]\tLoss: 2859.687500\tLR: [4.1051587301587304e-05]\n",
            "Train Epoch: 14 [1024/14335 (7%)]\tLoss: 2967.067139\tLR: [4.103174603174603e-05]\n",
            "Train Epoch: 14 [1280/14335 (9%)]\tLoss: 2935.737793\tLR: [4.1011904761904764e-05]\n",
            "Train Epoch: 14 [1536/14335 (11%)]\tLoss: 3120.784912\tLR: [4.099206349206349e-05]\n",
            "Train Epoch: 14 [1792/14335 (12%)]\tLoss: 2881.331543\tLR: [4.0972222222222225e-05]\n",
            "Train Epoch: 14 [2048/14335 (14%)]\tLoss: 3307.956787\tLR: [4.095238095238095e-05]\n",
            "Train Epoch: 14 [2304/14335 (16%)]\tLoss: 2828.611328\tLR: [4.0932539682539686e-05]\n",
            "Train Epoch: 14 [2560/14335 (18%)]\tLoss: 2940.159912\tLR: [4.091269841269841e-05]\n",
            "Train Epoch: 14 [2816/14335 (20%)]\tLoss: 3290.502441\tLR: [4.0892857142857147e-05]\n",
            "Train Epoch: 14 [3072/14335 (21%)]\tLoss: 2931.629883\tLR: [4.0873015873015874e-05]\n",
            "Train Epoch: 14 [3328/14335 (23%)]\tLoss: 3148.614746\tLR: [4.085317460317461e-05]\n",
            "Train Epoch: 14 [3584/14335 (25%)]\tLoss: 2956.768555\tLR: [4.0833333333333334e-05]\n",
            "Train Epoch: 14 [3840/14335 (27%)]\tLoss: 3211.145996\tLR: [4.081349206349207e-05]\n",
            "Train Epoch: 14 [4096/14335 (29%)]\tLoss: 2961.140381\tLR: [4.0793650793650795e-05]\n",
            "Train Epoch: 14 [4352/14335 (30%)]\tLoss: 2861.950684\tLR: [4.077380952380952e-05]\n",
            "Train Epoch: 14 [4608/14335 (32%)]\tLoss: 2944.641602\tLR: [4.0753968253968256e-05]\n",
            "Train Epoch: 14 [4864/14335 (34%)]\tLoss: 3067.455811\tLR: [4.073412698412698e-05]\n",
            "Train Epoch: 14 [5120/14335 (36%)]\tLoss: 2881.809570\tLR: [4.0714285714285717e-05]\n",
            "Train Epoch: 14 [5376/14335 (38%)]\tLoss: 3067.575684\tLR: [4.0694444444444444e-05]\n",
            "Train Epoch: 14 [5632/14335 (39%)]\tLoss: 2967.650146\tLR: [4.067460317460318e-05]\n",
            "Train Epoch: 14 [5888/14335 (41%)]\tLoss: 3143.546143\tLR: [4.0654761904761904e-05]\n",
            "Train Epoch: 14 [6144/14335 (43%)]\tLoss: 2871.420410\tLR: [4.063492063492064e-05]\n",
            "Train Epoch: 14 [6400/14335 (45%)]\tLoss: 2995.580322\tLR: [4.0615079365079365e-05]\n",
            "Train Epoch: 14 [6656/14335 (46%)]\tLoss: 2898.532959\tLR: [4.05952380952381e-05]\n",
            "Train Epoch: 14 [6912/14335 (48%)]\tLoss: 3391.798096\tLR: [4.0575396825396826e-05]\n",
            "Train Epoch: 14 [7168/14335 (50%)]\tLoss: 3287.259033\tLR: [4.055555555555556e-05]\n",
            "Train Epoch: 14 [7424/14335 (52%)]\tLoss: 3013.141357\tLR: [4.0535714285714287e-05]\n",
            "Train Epoch: 14 [7680/14335 (54%)]\tLoss: 2693.504639\tLR: [4.051587301587302e-05]\n",
            "Train Epoch: 14 [7936/14335 (55%)]\tLoss: 3240.199219\tLR: [4.049603174603175e-05]\n",
            "Train Epoch: 14 [8192/14335 (57%)]\tLoss: 2888.849121\tLR: [4.047619047619048e-05]\n",
            "Train Epoch: 14 [8448/14335 (59%)]\tLoss: 3001.116455\tLR: [4.045634920634921e-05]\n",
            "Train Epoch: 14 [8704/14335 (61%)]\tLoss: 2761.960938\tLR: [4.043650793650794e-05]\n",
            "Train Epoch: 14 [8960/14335 (62%)]\tLoss: 3073.713379\tLR: [4.041666666666667e-05]\n",
            "Train Epoch: 14 [9216/14335 (64%)]\tLoss: 2946.122070\tLR: [4.03968253968254e-05]\n",
            "Train Epoch: 14 [9472/14335 (66%)]\tLoss: 3127.604248\tLR: [4.037698412698413e-05]\n",
            "Train Epoch: 14 [9728/14335 (68%)]\tLoss: 2919.691650\tLR: [4.035714285714286e-05]\n",
            "Train Epoch: 14 [9984/14335 (70%)]\tLoss: 2435.431396\tLR: [4.033730158730159e-05]\n",
            "Train Epoch: 14 [10240/14335 (71%)]\tLoss: 3238.319092\tLR: [4.031746031746032e-05]\n",
            "Train Epoch: 14 [10496/14335 (73%)]\tLoss: 2596.816650\tLR: [4.029761904761905e-05]\n",
            "Train Epoch: 14 [10752/14335 (75%)]\tLoss: 2762.462158\tLR: [4.027777777777778e-05]\n",
            "Train Epoch: 14 [11008/14335 (77%)]\tLoss: 2953.475830\tLR: [4.025793650793651e-05]\n",
            "Train Epoch: 14 [11264/14335 (79%)]\tLoss: 3232.456055\tLR: [4.023809523809524e-05]\n",
            "Train Epoch: 14 [11520/14335 (80%)]\tLoss: 2697.950684\tLR: [4.021825396825397e-05]\n",
            "Train Epoch: 14 [11776/14335 (82%)]\tLoss: 3476.616699\tLR: [4.01984126984127e-05]\n",
            "Train Epoch: 14 [12032/14335 (84%)]\tLoss: 3173.415283\tLR: [4.017857142857143e-05]\n",
            "Train Epoch: 14 [12288/14335 (86%)]\tLoss: 3124.873047\tLR: [4.015873015873016e-05]\n",
            "Train Epoch: 14 [12544/14335 (88%)]\tLoss: 3047.910400\tLR: [4.0138888888888894e-05]\n",
            "Train Epoch: 14 [12800/14335 (89%)]\tLoss: 2428.621582\tLR: [4.011904761904762e-05]\n",
            "Train Epoch: 14 [13056/14335 (91%)]\tLoss: 2912.660889\tLR: [4.0099206349206355e-05]\n",
            "Train Epoch: 14 [13312/14335 (93%)]\tLoss: 3060.291504\tLR: [4.007936507936508e-05]\n",
            "Train Epoch: 14 [13568/14335 (95%)]\tLoss: 3534.299316\tLR: [4.0059523809523816e-05]\n",
            "Train Epoch: 14 [13824/14335 (96%)]\tLoss: 3284.771729\tLR: [4.003968253968254e-05]\n",
            "Train Epoch: 14 [14025/14335 (98%)]\tLoss: 3291.286377\tLR: [4.0019841269841276e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 14 is 0.8928571428571428, Best ROC Score is:0.9312820512820513\n",
            "Starting Epoch 15\n",
            "Train Epoch: 15 [0/14335 (0%)]\tLoss: 2630.377686\tLR: [4e-05]\n",
            "Train Epoch: 15 [256/14335 (2%)]\tLoss: 3346.743652\tLR: [3.998015873015874e-05]\n",
            "Train Epoch: 15 [512/14335 (4%)]\tLoss: 3264.908447\tLR: [3.9960317460317464e-05]\n",
            "Train Epoch: 15 [768/14335 (5%)]\tLoss: 3472.752686\tLR: [3.99404761904762e-05]\n",
            "Train Epoch: 15 [1024/14335 (7%)]\tLoss: 3060.135498\tLR: [3.9920634920634925e-05]\n",
            "Train Epoch: 15 [1280/14335 (9%)]\tLoss: 3069.770508\tLR: [3.990079365079366e-05]\n",
            "Train Epoch: 15 [1536/14335 (11%)]\tLoss: 3125.716064\tLR: [3.9880952380952386e-05]\n",
            "Train Epoch: 15 [1792/14335 (12%)]\tLoss: 2882.837646\tLR: [3.986111111111111e-05]\n",
            "Train Epoch: 15 [2048/14335 (14%)]\tLoss: 2849.165527\tLR: [3.984126984126984e-05]\n",
            "Train Epoch: 15 [2304/14335 (16%)]\tLoss: 2942.076660\tLR: [3.982142857142857e-05]\n",
            "Train Epoch: 15 [2560/14335 (18%)]\tLoss: 2987.207520\tLR: [3.98015873015873e-05]\n",
            "Train Epoch: 15 [2816/14335 (20%)]\tLoss: 2852.966064\tLR: [3.9781746031746034e-05]\n",
            "Train Epoch: 15 [3072/14335 (21%)]\tLoss: 3060.691406\tLR: [3.976190476190476e-05]\n",
            "Train Epoch: 15 [3328/14335 (23%)]\tLoss: 3038.938965\tLR: [3.9742063492063495e-05]\n",
            "Train Epoch: 15 [3584/14335 (25%)]\tLoss: 2830.484131\tLR: [3.972222222222222e-05]\n",
            "Train Epoch: 15 [3840/14335 (27%)]\tLoss: 3000.136719\tLR: [3.970238095238095e-05]\n",
            "Train Epoch: 15 [4096/14335 (29%)]\tLoss: 3020.849609\tLR: [3.968253968253968e-05]\n",
            "Train Epoch: 15 [4352/14335 (30%)]\tLoss: 2908.745117\tLR: [3.966269841269841e-05]\n",
            "Train Epoch: 15 [4608/14335 (32%)]\tLoss: 3268.993164\tLR: [3.964285714285714e-05]\n",
            "Train Epoch: 15 [4864/14335 (34%)]\tLoss: 2624.634766\tLR: [3.962301587301587e-05]\n",
            "Train Epoch: 15 [5120/14335 (36%)]\tLoss: 3260.087402\tLR: [3.9603174603174604e-05]\n",
            "Train Epoch: 15 [5376/14335 (38%)]\tLoss: 3152.968994\tLR: [3.958333333333333e-05]\n",
            "Train Epoch: 15 [5632/14335 (39%)]\tLoss: 2734.366699\tLR: [3.9563492063492065e-05]\n",
            "Train Epoch: 15 [5888/14335 (41%)]\tLoss: 2987.246094\tLR: [3.954365079365079e-05]\n",
            "Train Epoch: 15 [6144/14335 (43%)]\tLoss: 3010.498047\tLR: [3.9523809523809526e-05]\n",
            "Train Epoch: 15 [6400/14335 (45%)]\tLoss: 3119.259766\tLR: [3.950396825396825e-05]\n",
            "Train Epoch: 15 [6656/14335 (46%)]\tLoss: 2973.129639\tLR: [3.9484126984126986e-05]\n",
            "Train Epoch: 15 [6912/14335 (48%)]\tLoss: 3111.114746\tLR: [3.946428571428571e-05]\n",
            "Train Epoch: 15 [7168/14335 (50%)]\tLoss: 3147.963135\tLR: [3.944444444444445e-05]\n",
            "Train Epoch: 15 [7424/14335 (52%)]\tLoss: 3031.589111\tLR: [3.9424603174603174e-05]\n",
            "Train Epoch: 15 [7680/14335 (54%)]\tLoss: 2847.841064\tLR: [3.940476190476191e-05]\n",
            "Train Epoch: 15 [7936/14335 (55%)]\tLoss: 3186.702637\tLR: [3.9384920634920635e-05]\n",
            "Train Epoch: 15 [8192/14335 (57%)]\tLoss: 3309.589844\tLR: [3.936507936507937e-05]\n",
            "Train Epoch: 15 [8448/14335 (59%)]\tLoss: 2871.380615\tLR: [3.9345238095238096e-05]\n",
            "Train Epoch: 15 [8704/14335 (61%)]\tLoss: 2598.620850\tLR: [3.932539682539683e-05]\n",
            "Train Epoch: 15 [8960/14335 (62%)]\tLoss: 2882.075195\tLR: [3.9305555555555556e-05]\n",
            "Train Epoch: 15 [9216/14335 (64%)]\tLoss: 2907.933838\tLR: [3.928571428571429e-05]\n",
            "Train Epoch: 15 [9472/14335 (66%)]\tLoss: 3102.803955\tLR: [3.926587301587302e-05]\n",
            "Train Epoch: 15 [9728/14335 (68%)]\tLoss: 3182.050781\tLR: [3.9246031746031744e-05]\n",
            "Train Epoch: 15 [9984/14335 (70%)]\tLoss: 3103.574463\tLR: [3.922619047619048e-05]\n",
            "Train Epoch: 15 [10240/14335 (71%)]\tLoss: 3195.500488\tLR: [3.9206349206349205e-05]\n",
            "Train Epoch: 15 [10496/14335 (73%)]\tLoss: 3057.498291\tLR: [3.918650793650794e-05]\n",
            "Train Epoch: 15 [10752/14335 (75%)]\tLoss: 2761.318848\tLR: [3.9166666666666665e-05]\n",
            "Train Epoch: 15 [11008/14335 (77%)]\tLoss: 2844.806885\tLR: [3.91468253968254e-05]\n",
            "Train Epoch: 15 [11264/14335 (79%)]\tLoss: 2813.817383\tLR: [3.9126984126984126e-05]\n",
            "Train Epoch: 15 [11520/14335 (80%)]\tLoss: 3044.478027\tLR: [3.910714285714286e-05]\n",
            "Train Epoch: 15 [11776/14335 (82%)]\tLoss: 3009.974365\tLR: [3.908730158730159e-05]\n",
            "Train Epoch: 15 [12032/14335 (84%)]\tLoss: 3034.913818\tLR: [3.906746031746032e-05]\n",
            "Train Epoch: 15 [12288/14335 (86%)]\tLoss: 2649.904785\tLR: [3.904761904761905e-05]\n",
            "Train Epoch: 15 [12544/14335 (88%)]\tLoss: 3115.713623\tLR: [3.902777777777778e-05]\n",
            "Train Epoch: 15 [12800/14335 (89%)]\tLoss: 2933.746582\tLR: [3.900793650793651e-05]\n",
            "Train Epoch: 15 [13056/14335 (91%)]\tLoss: 2883.332764\tLR: [3.898809523809524e-05]\n",
            "Train Epoch: 15 [13312/14335 (93%)]\tLoss: 3092.747803\tLR: [3.896825396825397e-05]\n",
            "Train Epoch: 15 [13568/14335 (95%)]\tLoss: 2898.859131\tLR: [3.89484126984127e-05]\n",
            "Train Epoch: 15 [13824/14335 (96%)]\tLoss: 3434.216309\tLR: [3.892857142857143e-05]\n",
            "Train Epoch: 15 [14025/14335 (98%)]\tLoss: 2680.626221\tLR: [3.8908730158730164e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 15 is 0.8746478873239437, Best ROC Score is:0.9312820512820513\n",
            "Starting Epoch 16\n",
            "Train Epoch: 16 [0/14335 (0%)]\tLoss: 2946.568848\tLR: [3.888888888888889e-05]\n",
            "Train Epoch: 16 [256/14335 (2%)]\tLoss: 3017.817139\tLR: [3.8869047619047625e-05]\n",
            "Train Epoch: 16 [512/14335 (4%)]\tLoss: 3031.009033\tLR: [3.884920634920635e-05]\n",
            "Train Epoch: 16 [768/14335 (5%)]\tLoss: 2737.114746\tLR: [3.8829365079365085e-05]\n",
            "Train Epoch: 16 [1024/14335 (7%)]\tLoss: 3023.102051\tLR: [3.880952380952381e-05]\n",
            "Train Epoch: 16 [1280/14335 (9%)]\tLoss: 3175.818359\tLR: [3.878968253968254e-05]\n",
            "Train Epoch: 16 [1536/14335 (11%)]\tLoss: 3134.712891\tLR: [3.876984126984127e-05]\n",
            "Train Epoch: 16 [1792/14335 (12%)]\tLoss: 2580.553955\tLR: [3.875e-05]\n",
            "Train Epoch: 16 [2048/14335 (14%)]\tLoss: 2925.372803\tLR: [3.8730158730158734e-05]\n",
            "Train Epoch: 16 [2304/14335 (16%)]\tLoss: 2969.562988\tLR: [3.871031746031746e-05]\n",
            "Train Epoch: 16 [2560/14335 (18%)]\tLoss: 2943.638916\tLR: [3.8690476190476195e-05]\n",
            "Train Epoch: 16 [2816/14335 (20%)]\tLoss: 2837.840820\tLR: [3.867063492063492e-05]\n",
            "Train Epoch: 16 [3072/14335 (21%)]\tLoss: 2812.509277\tLR: [3.8650793650793655e-05]\n",
            "Train Epoch: 16 [3328/14335 (23%)]\tLoss: 3170.725342\tLR: [3.863095238095238e-05]\n",
            "Train Epoch: 16 [3584/14335 (25%)]\tLoss: 3260.408203\tLR: [3.8611111111111116e-05]\n",
            "Train Epoch: 16 [3840/14335 (27%)]\tLoss: 3302.957031\tLR: [3.859126984126984e-05]\n",
            "Train Epoch: 16 [4096/14335 (29%)]\tLoss: 2907.575195\tLR: [3.857142857142858e-05]\n",
            "Train Epoch: 16 [4352/14335 (30%)]\tLoss: 3044.371338\tLR: [3.8551587301587304e-05]\n",
            "Train Epoch: 16 [4608/14335 (32%)]\tLoss: 3243.315186\tLR: [3.853174603174604e-05]\n",
            "Train Epoch: 16 [4864/14335 (34%)]\tLoss: 2971.929443\tLR: [3.8511904761904765e-05]\n",
            "Train Epoch: 16 [5120/14335 (36%)]\tLoss: 3266.693359\tLR: [3.84920634920635e-05]\n",
            "Train Epoch: 16 [5376/14335 (38%)]\tLoss: 3056.743652\tLR: [3.8472222222222225e-05]\n",
            "Train Epoch: 16 [5632/14335 (39%)]\tLoss: 2939.104004\tLR: [3.845238095238096e-05]\n",
            "Train Epoch: 16 [5888/14335 (41%)]\tLoss: 3128.642578\tLR: [3.8432539682539686e-05]\n",
            "Train Epoch: 16 [6144/14335 (43%)]\tLoss: 3018.252930\tLR: [3.841269841269842e-05]\n",
            "Train Epoch: 16 [6400/14335 (45%)]\tLoss: 2879.919434\tLR: [3.839285714285715e-05]\n",
            "Train Epoch: 16 [6656/14335 (46%)]\tLoss: 3234.739502\tLR: [3.837301587301588e-05]\n",
            "Train Epoch: 16 [6912/14335 (48%)]\tLoss: 3106.682129\tLR: [3.835317460317461e-05]\n",
            "Train Epoch: 16 [7168/14335 (50%)]\tLoss: 3413.067383\tLR: [3.8333333333333334e-05]\n",
            "Train Epoch: 16 [7424/14335 (52%)]\tLoss: 2893.690430\tLR: [3.831349206349207e-05]\n",
            "Train Epoch: 16 [7680/14335 (54%)]\tLoss: 2887.425781\tLR: [3.8293650793650795e-05]\n",
            "Train Epoch: 16 [7936/14335 (55%)]\tLoss: 2847.146484\tLR: [3.827380952380952e-05]\n",
            "Train Epoch: 16 [8192/14335 (57%)]\tLoss: 3007.932373\tLR: [3.8253968253968256e-05]\n",
            "Train Epoch: 16 [8448/14335 (59%)]\tLoss: 3113.927246\tLR: [3.823412698412698e-05]\n",
            "Train Epoch: 16 [8704/14335 (61%)]\tLoss: 3087.077881\tLR: [3.821428571428572e-05]\n",
            "Train Epoch: 16 [8960/14335 (62%)]\tLoss: 3227.081543\tLR: [3.8194444444444444e-05]\n",
            "Train Epoch: 16 [9216/14335 (64%)]\tLoss: 2951.457275\tLR: [3.817460317460317e-05]\n",
            "Train Epoch: 16 [9472/14335 (66%)]\tLoss: 2674.798584\tLR: [3.8154761904761904e-05]\n",
            "Train Epoch: 16 [9728/14335 (68%)]\tLoss: 3118.396484\tLR: [3.813492063492063e-05]\n",
            "Train Epoch: 16 [9984/14335 (70%)]\tLoss: 3143.122314\tLR: [3.8115079365079365e-05]\n",
            "Train Epoch: 16 [10240/14335 (71%)]\tLoss: 2426.070312\tLR: [3.809523809523809e-05]\n",
            "Train Epoch: 16 [10496/14335 (73%)]\tLoss: 2787.661621\tLR: [3.8075396825396826e-05]\n",
            "Train Epoch: 16 [10752/14335 (75%)]\tLoss: 2440.869629\tLR: [3.805555555555555e-05]\n",
            "Train Epoch: 16 [11008/14335 (77%)]\tLoss: 2851.148926\tLR: [3.803571428571429e-05]\n",
            "Train Epoch: 16 [11264/14335 (79%)]\tLoss: 3337.505615\tLR: [3.8015873015873014e-05]\n",
            "Train Epoch: 16 [11520/14335 (80%)]\tLoss: 3013.905273\tLR: [3.799603174603175e-05]\n",
            "Train Epoch: 16 [11776/14335 (82%)]\tLoss: 2762.197998\tLR: [3.7976190476190474e-05]\n",
            "Train Epoch: 16 [12032/14335 (84%)]\tLoss: 2991.088867\tLR: [3.795634920634921e-05]\n",
            "Train Epoch: 16 [12288/14335 (86%)]\tLoss: 3151.600586\tLR: [3.7936507936507935e-05]\n",
            "Train Epoch: 16 [12544/14335 (88%)]\tLoss: 2964.000244\tLR: [3.791666666666667e-05]\n",
            "Train Epoch: 16 [12800/14335 (89%)]\tLoss: 3110.382568\tLR: [3.7896825396825396e-05]\n",
            "Train Epoch: 16 [13056/14335 (91%)]\tLoss: 3256.844727\tLR: [3.787698412698413e-05]\n",
            "Train Epoch: 16 [13312/14335 (93%)]\tLoss: 3118.448242\tLR: [3.785714285714286e-05]\n",
            "Train Epoch: 16 [13568/14335 (95%)]\tLoss: 3363.757080\tLR: [3.783730158730159e-05]\n",
            "Train Epoch: 16 [13824/14335 (96%)]\tLoss: 2901.322998\tLR: [3.781746031746032e-05]\n",
            "Train Epoch: 16 [14025/14335 (98%)]\tLoss: 2660.467285\tLR: [3.779761904761905e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 16 is 0.85962441314554, Best ROC Score is:0.9312820512820513\n",
            "Starting Epoch 17\n",
            "Train Epoch: 17 [0/14335 (0%)]\tLoss: 3040.336182\tLR: [3.777777777777778e-05]\n",
            "Train Epoch: 17 [256/14335 (2%)]\tLoss: 2971.105225\tLR: [3.775793650793651e-05]\n",
            "Train Epoch: 17 [512/14335 (4%)]\tLoss: 3187.364746\tLR: [3.773809523809524e-05]\n",
            "Train Epoch: 17 [768/14335 (5%)]\tLoss: 3017.438477\tLR: [3.7718253968253966e-05]\n",
            "Train Epoch: 17 [1024/14335 (7%)]\tLoss: 2783.258789\tLR: [3.76984126984127e-05]\n",
            "Train Epoch: 17 [1280/14335 (9%)]\tLoss: 3107.637207\tLR: [3.767857142857143e-05]\n",
            "Train Epoch: 17 [1536/14335 (11%)]\tLoss: 3016.272705\tLR: [3.765873015873016e-05]\n",
            "Train Epoch: 17 [1792/14335 (12%)]\tLoss: 2896.797852\tLR: [3.763888888888889e-05]\n",
            "Train Epoch: 17 [2048/14335 (14%)]\tLoss: 3114.459229\tLR: [3.761904761904762e-05]\n",
            "Train Epoch: 17 [2304/14335 (16%)]\tLoss: 3452.694580\tLR: [3.759920634920635e-05]\n",
            "Train Epoch: 17 [2560/14335 (18%)]\tLoss: 3058.522217\tLR: [3.757936507936508e-05]\n",
            "Train Epoch: 17 [2816/14335 (20%)]\tLoss: 3024.529785\tLR: [3.755952380952381e-05]\n",
            "Train Epoch: 17 [3072/14335 (21%)]\tLoss: 2887.782227\tLR: [3.753968253968254e-05]\n",
            "Train Epoch: 17 [3328/14335 (23%)]\tLoss: 2721.681152\tLR: [3.751984126984127e-05]\n",
            "Train Epoch: 17 [3584/14335 (25%)]\tLoss: 3072.605713\tLR: [3.7500000000000003e-05]\n",
            "Train Epoch: 17 [3840/14335 (27%)]\tLoss: 2695.701416\tLR: [3.748015873015873e-05]\n",
            "Train Epoch: 17 [4096/14335 (29%)]\tLoss: 3139.881592\tLR: [3.7460317460317464e-05]\n",
            "Train Epoch: 17 [4352/14335 (30%)]\tLoss: 3150.753418\tLR: [3.744047619047619e-05]\n",
            "Train Epoch: 17 [4608/14335 (32%)]\tLoss: 3515.155273\tLR: [3.7420634920634925e-05]\n",
            "Train Epoch: 17 [4864/14335 (34%)]\tLoss: 2977.235107\tLR: [3.740079365079365e-05]\n",
            "Train Epoch: 17 [5120/14335 (36%)]\tLoss: 2960.014160\tLR: [3.7380952380952386e-05]\n",
            "Train Epoch: 17 [5376/14335 (38%)]\tLoss: 2850.487061\tLR: [3.736111111111111e-05]\n",
            "Train Epoch: 17 [5632/14335 (39%)]\tLoss: 3122.025635\tLR: [3.7341269841269846e-05]\n",
            "Train Epoch: 17 [5888/14335 (41%)]\tLoss: 2981.606689\tLR: [3.7321428571428573e-05]\n",
            "Train Epoch: 17 [6144/14335 (43%)]\tLoss: 3061.789062\tLR: [3.730158730158731e-05]\n",
            "Train Epoch: 17 [6400/14335 (45%)]\tLoss: 2894.316406\tLR: [3.7281746031746034e-05]\n",
            "Train Epoch: 17 [6656/14335 (46%)]\tLoss: 3203.968994\tLR: [3.726190476190476e-05]\n",
            "Train Epoch: 17 [6912/14335 (48%)]\tLoss: 2799.650391\tLR: [3.7242063492063495e-05]\n",
            "Train Epoch: 17 [7168/14335 (50%)]\tLoss: 2945.975586\tLR: [3.722222222222222e-05]\n",
            "Train Epoch: 17 [7424/14335 (52%)]\tLoss: 3040.456055\tLR: [3.7202380952380956e-05]\n",
            "Train Epoch: 17 [7680/14335 (54%)]\tLoss: 2989.367188\tLR: [3.718253968253968e-05]\n",
            "Train Epoch: 17 [7936/14335 (55%)]\tLoss: 3001.707520\tLR: [3.7162698412698416e-05]\n",
            "Train Epoch: 17 [8192/14335 (57%)]\tLoss: 2850.695801\tLR: [3.7142857142857143e-05]\n",
            "Train Epoch: 17 [8448/14335 (59%)]\tLoss: 3167.644531\tLR: [3.712301587301588e-05]\n",
            "Train Epoch: 17 [8704/14335 (61%)]\tLoss: 2696.032471\tLR: [3.7103174603174604e-05]\n",
            "Train Epoch: 17 [8960/14335 (62%)]\tLoss: 3359.264648\tLR: [3.708333333333334e-05]\n",
            "Train Epoch: 17 [9216/14335 (64%)]\tLoss: 2670.185791\tLR: [3.7063492063492065e-05]\n",
            "Train Epoch: 17 [9472/14335 (66%)]\tLoss: 3477.261230\tLR: [3.70436507936508e-05]\n",
            "Train Epoch: 17 [9728/14335 (68%)]\tLoss: 2564.416748\tLR: [3.7023809523809526e-05]\n",
            "Train Epoch: 17 [9984/14335 (70%)]\tLoss: 3042.615479\tLR: [3.700396825396826e-05]\n",
            "Train Epoch: 17 [10240/14335 (71%)]\tLoss: 3113.033936\tLR: [3.6984126984126986e-05]\n",
            "Train Epoch: 17 [10496/14335 (73%)]\tLoss: 3300.395508\tLR: [3.696428571428572e-05]\n",
            "Train Epoch: 17 [10752/14335 (75%)]\tLoss: 2970.371582\tLR: [3.694444444444445e-05]\n",
            "Train Epoch: 17 [11008/14335 (77%)]\tLoss: 2398.960938\tLR: [3.692460317460318e-05]\n",
            "Train Epoch: 17 [11264/14335 (79%)]\tLoss: 3081.055420\tLR: [3.690476190476191e-05]\n",
            "Train Epoch: 17 [11520/14335 (80%)]\tLoss: 2850.932861\tLR: [3.688492063492064e-05]\n",
            "Train Epoch: 17 [11776/14335 (82%)]\tLoss: 3037.895508\tLR: [3.686507936507937e-05]\n",
            "Train Epoch: 17 [12032/14335 (84%)]\tLoss: 3257.529053\tLR: [3.68452380952381e-05]\n",
            "Train Epoch: 17 [12288/14335 (86%)]\tLoss: 2890.101807\tLR: [3.682539682539683e-05]\n",
            "Train Epoch: 17 [12544/14335 (88%)]\tLoss: 3099.083496\tLR: [3.6805555555555556e-05]\n",
            "Train Epoch: 17 [12800/14335 (89%)]\tLoss: 2651.910400\tLR: [3.678571428571429e-05]\n",
            "Train Epoch: 17 [13056/14335 (91%)]\tLoss: 2918.821045\tLR: [3.676587301587302e-05]\n",
            "Train Epoch: 17 [13312/14335 (93%)]\tLoss: 2996.391602\tLR: [3.674603174603175e-05]\n",
            "Train Epoch: 17 [13568/14335 (95%)]\tLoss: 2853.253418\tLR: [3.672619047619048e-05]\n",
            "Train Epoch: 17 [13824/14335 (96%)]\tLoss: 3079.984863\tLR: [3.6706349206349205e-05]\n",
            "Train Epoch: 17 [14025/14335 (98%)]\tLoss: 2837.740967\tLR: [3.668650793650794e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 17 is 0.8833333333333333, Best ROC Score is:0.9312820512820513\n",
            "Starting Epoch 18\n",
            "Train Epoch: 18 [0/14335 (0%)]\tLoss: 2995.155029\tLR: [3.6666666666666666e-05]\n",
            "Train Epoch: 18 [256/14335 (2%)]\tLoss: 3158.145020\tLR: [3.664682539682539e-05]\n",
            "Train Epoch: 18 [512/14335 (4%)]\tLoss: 3363.488770\tLR: [3.6626984126984126e-05]\n",
            "Train Epoch: 18 [768/14335 (5%)]\tLoss: 2596.682373\tLR: [3.6607142857142853e-05]\n",
            "Train Epoch: 18 [1024/14335 (7%)]\tLoss: 3185.230713\tLR: [3.658730158730159e-05]\n",
            "Train Epoch: 18 [1280/14335 (9%)]\tLoss: 2928.496338\tLR: [3.6567460317460314e-05]\n",
            "Train Epoch: 18 [1536/14335 (11%)]\tLoss: 2821.855225\tLR: [3.654761904761905e-05]\n",
            "Train Epoch: 18 [1792/14335 (12%)]\tLoss: 2654.335938\tLR: [3.6527777777777775e-05]\n",
            "Train Epoch: 18 [2048/14335 (14%)]\tLoss: 3221.673096\tLR: [3.650793650793651e-05]\n",
            "Train Epoch: 18 [2304/14335 (16%)]\tLoss: 2475.596191\tLR: [3.6488095238095236e-05]\n",
            "Train Epoch: 18 [2560/14335 (18%)]\tLoss: 3164.414551\tLR: [3.646825396825397e-05]\n",
            "Train Epoch: 18 [2816/14335 (20%)]\tLoss: 2980.774170\tLR: [3.6448412698412696e-05]\n",
            "Train Epoch: 18 [3072/14335 (21%)]\tLoss: 3274.325195\tLR: [3.642857142857143e-05]\n",
            "Train Epoch: 18 [3328/14335 (23%)]\tLoss: 2923.510986\tLR: [3.640873015873016e-05]\n",
            "Train Epoch: 18 [3584/14335 (25%)]\tLoss: 2855.078857\tLR: [3.638888888888889e-05]\n",
            "Train Epoch: 18 [3840/14335 (27%)]\tLoss: 2812.965088\tLR: [3.636904761904762e-05]\n",
            "Train Epoch: 18 [4096/14335 (29%)]\tLoss: 3115.085938\tLR: [3.634920634920635e-05]\n",
            "Train Epoch: 18 [4352/14335 (30%)]\tLoss: 2945.453613\tLR: [3.632936507936508e-05]\n",
            "Train Epoch: 18 [4608/14335 (32%)]\tLoss: 3018.565186\tLR: [3.630952380952381e-05]\n",
            "Train Epoch: 18 [4864/14335 (34%)]\tLoss: 3166.689209\tLR: [3.628968253968254e-05]\n",
            "Train Epoch: 18 [5120/14335 (36%)]\tLoss: 3030.205811\tLR: [3.626984126984127e-05]\n",
            "Train Epoch: 18 [5376/14335 (38%)]\tLoss: 2976.914062\tLR: [3.625e-05]\n",
            "Train Epoch: 18 [5632/14335 (39%)]\tLoss: 3000.581299\tLR: [3.6230158730158734e-05]\n",
            "Train Epoch: 18 [5888/14335 (41%)]\tLoss: 2887.197510\tLR: [3.621031746031746e-05]\n",
            "Train Epoch: 18 [6144/14335 (43%)]\tLoss: 3215.965332\tLR: [3.619047619047619e-05]\n",
            "Train Epoch: 18 [6400/14335 (45%)]\tLoss: 3224.393311\tLR: [3.617063492063492e-05]\n",
            "Train Epoch: 18 [6656/14335 (46%)]\tLoss: 2936.834473\tLR: [3.615079365079365e-05]\n",
            "Train Epoch: 18 [6912/14335 (48%)]\tLoss: 3317.574219\tLR: [3.613095238095238e-05]\n",
            "Train Epoch: 18 [7168/14335 (50%)]\tLoss: 2741.855957\tLR: [3.611111111111111e-05]\n",
            "Train Epoch: 18 [7424/14335 (52%)]\tLoss: 2964.090576\tLR: [3.609126984126984e-05]\n",
            "Train Epoch: 18 [7680/14335 (54%)]\tLoss: 2949.291992\tLR: [3.607142857142857e-05]\n",
            "Train Epoch: 18 [7936/14335 (55%)]\tLoss: 3376.033936\tLR: [3.6051587301587304e-05]\n",
            "Train Epoch: 18 [8192/14335 (57%)]\tLoss: 2995.680420\tLR: [3.603174603174603e-05]\n",
            "Train Epoch: 18 [8448/14335 (59%)]\tLoss: 3062.458984\tLR: [3.6011904761904765e-05]\n",
            "Train Epoch: 18 [8704/14335 (61%)]\tLoss: 3171.509033\tLR: [3.599206349206349e-05]\n",
            "Train Epoch: 18 [8960/14335 (62%)]\tLoss: 2880.052490\tLR: [3.5972222222222225e-05]\n",
            "Train Epoch: 18 [9216/14335 (64%)]\tLoss: 3234.277588\tLR: [3.595238095238095e-05]\n",
            "Train Epoch: 18 [9472/14335 (66%)]\tLoss: 3205.898682\tLR: [3.5932539682539686e-05]\n",
            "Train Epoch: 18 [9728/14335 (68%)]\tLoss: 3090.760742\tLR: [3.591269841269841e-05]\n",
            "Train Epoch: 18 [9984/14335 (70%)]\tLoss: 2877.785156\tLR: [3.589285714285715e-05]\n",
            "Train Epoch: 18 [10240/14335 (71%)]\tLoss: 3393.477295\tLR: [3.5873015873015874e-05]\n",
            "Train Epoch: 18 [10496/14335 (73%)]\tLoss: 3082.941650\tLR: [3.585317460317461e-05]\n",
            "Train Epoch: 18 [10752/14335 (75%)]\tLoss: 2875.645020\tLR: [3.5833333333333335e-05]\n",
            "Train Epoch: 18 [11008/14335 (77%)]\tLoss: 3046.975830\tLR: [3.581349206349207e-05]\n",
            "Train Epoch: 18 [11264/14335 (79%)]\tLoss: 2760.096924\tLR: [3.5793650793650795e-05]\n",
            "Train Epoch: 18 [11520/14335 (80%)]\tLoss: 3030.961182\tLR: [3.577380952380953e-05]\n",
            "Train Epoch: 18 [11776/14335 (82%)]\tLoss: 2698.523926\tLR: [3.5753968253968256e-05]\n",
            "Train Epoch: 18 [12032/14335 (84%)]\tLoss: 2733.757568\tLR: [3.573412698412698e-05]\n",
            "Train Epoch: 18 [12288/14335 (86%)]\tLoss: 2987.120850\tLR: [3.571428571428572e-05]\n",
            "Train Epoch: 18 [12544/14335 (88%)]\tLoss: 3179.122314\tLR: [3.5694444444444444e-05]\n",
            "Train Epoch: 18 [12800/14335 (89%)]\tLoss: 3358.200928\tLR: [3.567460317460318e-05]\n",
            "Train Epoch: 18 [13056/14335 (91%)]\tLoss: 2849.585205\tLR: [3.5654761904761905e-05]\n",
            "Train Epoch: 18 [13312/14335 (93%)]\tLoss: 2864.081787\tLR: [3.563492063492064e-05]\n",
            "Train Epoch: 18 [13568/14335 (95%)]\tLoss: 3244.031250\tLR: [3.5615079365079365e-05]\n",
            "Train Epoch: 18 [13824/14335 (96%)]\tLoss: 2786.988281\tLR: [3.55952380952381e-05]\n",
            "Train Epoch: 18 [14025/14335 (98%)]\tLoss: 3248.151367\tLR: [3.5575396825396826e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 18 is 0.9188722669735327, Best ROC Score is:0.9312820512820513\n",
            "Starting Epoch 19\n",
            "Train Epoch: 19 [0/14335 (0%)]\tLoss: 3248.670166\tLR: [3.555555555555556e-05]\n",
            "Train Epoch: 19 [256/14335 (2%)]\tLoss: 3099.093750\tLR: [3.553571428571429e-05]\n",
            "Train Epoch: 19 [512/14335 (4%)]\tLoss: 2559.689209\tLR: [3.551587301587302e-05]\n",
            "Train Epoch: 19 [768/14335 (5%)]\tLoss: 2893.895996\tLR: [3.549603174603175e-05]\n",
            "Train Epoch: 19 [1024/14335 (7%)]\tLoss: 2901.640381\tLR: [3.547619047619048e-05]\n",
            "Train Epoch: 19 [1280/14335 (9%)]\tLoss: 3069.665283\tLR: [3.545634920634921e-05]\n",
            "Train Epoch: 19 [1536/14335 (11%)]\tLoss: 3306.428223\tLR: [3.543650793650794e-05]\n",
            "Train Epoch: 19 [1792/14335 (12%)]\tLoss: 3419.364014\tLR: [3.541666666666667e-05]\n",
            "Train Epoch: 19 [2048/14335 (14%)]\tLoss: 3356.242676\tLR: [3.53968253968254e-05]\n",
            "Train Epoch: 19 [2304/14335 (16%)]\tLoss: 2548.644043\tLR: [3.537698412698413e-05]\n",
            "Train Epoch: 19 [2560/14335 (18%)]\tLoss: 2709.959717\tLR: [3.5357142857142864e-05]\n",
            "Train Epoch: 19 [2816/14335 (20%)]\tLoss: 2729.734375\tLR: [3.533730158730159e-05]\n",
            "Train Epoch: 19 [3072/14335 (21%)]\tLoss: 2929.019287\tLR: [3.5317460317460324e-05]\n",
            "Train Epoch: 19 [3328/14335 (23%)]\tLoss: 3086.416260\tLR: [3.529761904761905e-05]\n",
            "Train Epoch: 19 [3584/14335 (25%)]\tLoss: 3230.697266\tLR: [3.527777777777778e-05]\n",
            "Train Epoch: 19 [3840/14335 (27%)]\tLoss: 3111.692627\tLR: [3.525793650793651e-05]\n",
            "Train Epoch: 19 [4096/14335 (29%)]\tLoss: 3011.606934\tLR: [3.523809523809524e-05]\n",
            "Train Epoch: 19 [4352/14335 (30%)]\tLoss: 2611.216797\tLR: [3.521825396825397e-05]\n",
            "Train Epoch: 19 [4608/14335 (32%)]\tLoss: 2900.324463\tLR: [3.51984126984127e-05]\n",
            "Train Epoch: 19 [4864/14335 (34%)]\tLoss: 3242.607178\tLR: [3.5178571428571434e-05]\n",
            "Train Epoch: 19 [5120/14335 (36%)]\tLoss: 3331.492432\tLR: [3.515873015873016e-05]\n",
            "Train Epoch: 19 [5376/14335 (38%)]\tLoss: 2909.631348\tLR: [3.513888888888889e-05]\n",
            "Train Epoch: 19 [5632/14335 (39%)]\tLoss: 3097.678955\tLR: [3.511904761904762e-05]\n",
            "Train Epoch: 19 [5888/14335 (41%)]\tLoss: 3093.290283\tLR: [3.509920634920635e-05]\n",
            "Train Epoch: 19 [6144/14335 (43%)]\tLoss: 3134.749268\tLR: [3.5079365079365075e-05]\n",
            "Train Epoch: 19 [6400/14335 (45%)]\tLoss: 3064.924805\tLR: [3.505952380952381e-05]\n",
            "Train Epoch: 19 [6656/14335 (46%)]\tLoss: 2813.911621\tLR: [3.5039682539682536e-05]\n",
            "Train Epoch: 19 [6912/14335 (48%)]\tLoss: 3261.226562\tLR: [3.501984126984127e-05]\n",
            "Train Epoch: 19 [7168/14335 (50%)]\tLoss: 2642.324951\tLR: [3.5e-05]\n",
            "Train Epoch: 19 [7424/14335 (52%)]\tLoss: 3067.878662\tLR: [3.498015873015873e-05]\n",
            "Train Epoch: 19 [7680/14335 (54%)]\tLoss: 2827.504150\tLR: [3.496031746031746e-05]\n",
            "Train Epoch: 19 [7936/14335 (55%)]\tLoss: 3119.516846\tLR: [3.494047619047619e-05]\n",
            "Train Epoch: 19 [8192/14335 (57%)]\tLoss: 2804.393311\tLR: [3.492063492063492e-05]\n",
            "Train Epoch: 19 [8448/14335 (59%)]\tLoss: 3240.924072\tLR: [3.490079365079365e-05]\n",
            "Train Epoch: 19 [8704/14335 (61%)]\tLoss: 3084.485107\tLR: [3.488095238095238e-05]\n",
            "Train Epoch: 19 [8960/14335 (62%)]\tLoss: 2800.493896\tLR: [3.486111111111111e-05]\n",
            "Train Epoch: 19 [9216/14335 (64%)]\tLoss: 2979.120605\tLR: [3.484126984126984e-05]\n",
            "Train Epoch: 19 [9472/14335 (66%)]\tLoss: 3053.688477\tLR: [3.4821428571428574e-05]\n",
            "Train Epoch: 19 [9728/14335 (68%)]\tLoss: 2744.944336\tLR: [3.48015873015873e-05]\n",
            "Train Epoch: 19 [9984/14335 (70%)]\tLoss: 3529.055420\tLR: [3.4781746031746034e-05]\n",
            "Train Epoch: 19 [10240/14335 (71%)]\tLoss: 2627.363770\tLR: [3.476190476190476e-05]\n",
            "Train Epoch: 19 [10496/14335 (73%)]\tLoss: 3325.339600\tLR: [3.4742063492063495e-05]\n",
            "Train Epoch: 19 [10752/14335 (75%)]\tLoss: 3204.911133\tLR: [3.472222222222222e-05]\n",
            "Train Epoch: 19 [11008/14335 (77%)]\tLoss: 2854.728271\tLR: [3.4702380952380956e-05]\n",
            "Train Epoch: 19 [11264/14335 (79%)]\tLoss: 3143.829834\tLR: [3.468253968253968e-05]\n",
            "Train Epoch: 19 [11520/14335 (80%)]\tLoss: 3223.477051\tLR: [3.466269841269842e-05]\n",
            "Train Epoch: 19 [11776/14335 (82%)]\tLoss: 3347.385010\tLR: [3.4642857142857144e-05]\n",
            "Train Epoch: 19 [12032/14335 (84%)]\tLoss: 2945.943359\tLR: [3.462301587301587e-05]\n",
            "Train Epoch: 19 [12288/14335 (86%)]\tLoss: 2569.866943\tLR: [3.4603174603174604e-05]\n",
            "Train Epoch: 19 [12544/14335 (88%)]\tLoss: 2820.783203\tLR: [3.458333333333333e-05]\n",
            "Train Epoch: 19 [12800/14335 (89%)]\tLoss: 3045.382812\tLR: [3.4563492063492065e-05]\n",
            "Train Epoch: 19 [13056/14335 (91%)]\tLoss: 2907.565186\tLR: [3.454365079365079e-05]\n",
            "Train Epoch: 19 [13312/14335 (93%)]\tLoss: 2909.358643\tLR: [3.4523809523809526e-05]\n",
            "Train Epoch: 19 [13568/14335 (95%)]\tLoss: 2795.786865\tLR: [3.450396825396825e-05]\n",
            "Train Epoch: 19 [13824/14335 (96%)]\tLoss: 2953.175293\tLR: [3.448412698412699e-05]\n",
            "Train Epoch: 19 [14025/14335 (98%)]\tLoss: 2963.170166\tLR: [3.4464285714285714e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 19 is 0.8838235294117648, Best ROC Score is:0.9312820512820513\n",
            "Starting Epoch 20\n",
            "Train Epoch: 20 [0/14335 (0%)]\tLoss: 3150.569092\tLR: [3.444444444444445e-05]\n",
            "Train Epoch: 20 [256/14335 (2%)]\tLoss: 2532.852539\tLR: [3.4424603174603174e-05]\n",
            "Train Epoch: 20 [512/14335 (4%)]\tLoss: 2961.615723\tLR: [3.440476190476191e-05]\n",
            "Train Epoch: 20 [768/14335 (5%)]\tLoss: 2689.875488\tLR: [3.4384920634920635e-05]\n",
            "Train Epoch: 20 [1024/14335 (7%)]\tLoss: 3382.369141\tLR: [3.436507936507937e-05]\n",
            "Train Epoch: 20 [1280/14335 (9%)]\tLoss: 2868.958496\tLR: [3.4345238095238096e-05]\n",
            "Train Epoch: 20 [1536/14335 (11%)]\tLoss: 3513.087891\tLR: [3.432539682539683e-05]\n",
            "Train Epoch: 20 [1792/14335 (12%)]\tLoss: 3192.279785\tLR: [3.430555555555556e-05]\n",
            "Train Epoch: 20 [2048/14335 (14%)]\tLoss: 3118.986328\tLR: [3.428571428571429e-05]\n",
            "Train Epoch: 20 [2304/14335 (16%)]\tLoss: 2949.013672\tLR: [3.426587301587302e-05]\n",
            "Train Epoch: 20 [2560/14335 (18%)]\tLoss: 3257.157227\tLR: [3.424603174603175e-05]\n",
            "Train Epoch: 20 [2816/14335 (20%)]\tLoss: 2953.275879\tLR: [3.422619047619048e-05]\n",
            "Train Epoch: 20 [3072/14335 (21%)]\tLoss: 3098.101074\tLR: [3.420634920634921e-05]\n",
            "Train Epoch: 20 [3328/14335 (23%)]\tLoss: 3056.489014\tLR: [3.418650793650794e-05]\n",
            "Train Epoch: 20 [3584/14335 (25%)]\tLoss: 3332.079590\tLR: [3.4166666666666666e-05]\n",
            "Train Epoch: 20 [3840/14335 (27%)]\tLoss: 3140.622559\tLR: [3.41468253968254e-05]\n",
            "Train Epoch: 20 [4096/14335 (29%)]\tLoss: 2741.501953\tLR: [3.412698412698413e-05]\n",
            "Train Epoch: 20 [4352/14335 (30%)]\tLoss: 3126.947998\tLR: [3.410714285714286e-05]\n",
            "Train Epoch: 20 [4608/14335 (32%)]\tLoss: 2860.603271\tLR: [3.408730158730159e-05]\n",
            "Train Epoch: 20 [4864/14335 (34%)]\tLoss: 3225.849854\tLR: [3.406746031746032e-05]\n",
            "Train Epoch: 20 [5120/14335 (36%)]\tLoss: 3169.157959\tLR: [3.404761904761905e-05]\n",
            "Train Epoch: 20 [5376/14335 (38%)]\tLoss: 3004.145264\tLR: [3.402777777777778e-05]\n",
            "Train Epoch: 20 [5632/14335 (39%)]\tLoss: 2751.811768\tLR: [3.400793650793651e-05]\n",
            "Train Epoch: 20 [5888/14335 (41%)]\tLoss: 3364.691162\tLR: [3.398809523809524e-05]\n",
            "Train Epoch: 20 [6144/14335 (43%)]\tLoss: 2905.563477\tLR: [3.396825396825397e-05]\n",
            "Train Epoch: 20 [6400/14335 (45%)]\tLoss: 2828.291992\tLR: [3.3948412698412703e-05]\n",
            "Train Epoch: 20 [6656/14335 (46%)]\tLoss: 3096.868896\tLR: [3.392857142857143e-05]\n",
            "Train Epoch: 20 [6912/14335 (48%)]\tLoss: 2911.009766\tLR: [3.3908730158730164e-05]\n",
            "Train Epoch: 20 [7168/14335 (50%)]\tLoss: 2603.856201\tLR: [3.388888888888889e-05]\n",
            "Train Epoch: 20 [7424/14335 (52%)]\tLoss: 3309.062012\tLR: [3.3869047619047625e-05]\n",
            "Train Epoch: 20 [7680/14335 (54%)]\tLoss: 2776.152344\tLR: [3.384920634920635e-05]\n",
            "Train Epoch: 20 [7936/14335 (55%)]\tLoss: 2721.989258\tLR: [3.3829365079365086e-05]\n",
            "Train Epoch: 20 [8192/14335 (57%)]\tLoss: 3022.380371\tLR: [3.380952380952381e-05]\n",
            "Train Epoch: 20 [8448/14335 (59%)]\tLoss: 2697.358154\tLR: [3.3789682539682546e-05]\n",
            "Train Epoch: 20 [8704/14335 (61%)]\tLoss: 3022.985840\tLR: [3.3769841269841273e-05]\n",
            "Train Epoch: 20 [8960/14335 (62%)]\tLoss: 3178.563721\tLR: [3.375000000000001e-05]\n",
            "Train Epoch: 20 [9216/14335 (64%)]\tLoss: 2868.607422\tLR: [3.3730158730158734e-05]\n",
            "Train Epoch: 20 [9472/14335 (66%)]\tLoss: 3139.836670\tLR: [3.371031746031746e-05]\n",
            "Train Epoch: 20 [9728/14335 (68%)]\tLoss: 2990.524414\tLR: [3.3690476190476195e-05]\n",
            "Train Epoch: 20 [9984/14335 (70%)]\tLoss: 3037.323975\tLR: [3.367063492063492e-05]\n",
            "Train Epoch: 20 [10240/14335 (71%)]\tLoss: 2550.696777\tLR: [3.3650793650793656e-05]\n",
            "Train Epoch: 20 [10496/14335 (73%)]\tLoss: 2616.659668\tLR: [3.363095238095238e-05]\n",
            "Train Epoch: 20 [10752/14335 (75%)]\tLoss: 3173.870605\tLR: [3.3611111111111116e-05]\n",
            "Train Epoch: 20 [11008/14335 (77%)]\tLoss: 3088.566162\tLR: [3.3591269841269843e-05]\n",
            "Train Epoch: 20 [11264/14335 (79%)]\tLoss: 2998.521729\tLR: [3.357142857142857e-05]\n",
            "Train Epoch: 20 [11520/14335 (80%)]\tLoss: 3034.111816\tLR: [3.35515873015873e-05]\n",
            "Train Epoch: 20 [11776/14335 (82%)]\tLoss: 2834.800049\tLR: [3.353174603174603e-05]\n",
            "Train Epoch: 20 [12032/14335 (84%)]\tLoss: 3300.844971\tLR: [3.351190476190476e-05]\n",
            "Train Epoch: 20 [12288/14335 (86%)]\tLoss: 2724.958984\tLR: [3.349206349206349e-05]\n",
            "Train Epoch: 20 [12544/14335 (88%)]\tLoss: 2712.108398\tLR: [3.347222222222222e-05]\n",
            "Train Epoch: 20 [12800/14335 (89%)]\tLoss: 2991.964600\tLR: [3.345238095238095e-05]\n",
            "Train Epoch: 20 [13056/14335 (91%)]\tLoss: 2989.419678\tLR: [3.343253968253968e-05]\n",
            "Train Epoch: 20 [13312/14335 (93%)]\tLoss: 3304.624512\tLR: [3.3412698412698413e-05]\n",
            "Train Epoch: 20 [13568/14335 (95%)]\tLoss: 3048.903320\tLR: [3.339285714285714e-05]\n",
            "Train Epoch: 20 [13824/14335 (96%)]\tLoss: 3334.957520\tLR: [3.3373015873015874e-05]\n",
            "Train Epoch: 20 [14025/14335 (98%)]\tLoss: 3073.019043\tLR: [3.33531746031746e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 20 is 0.9216944801026958, Best ROC Score is:0.9312820512820513\n",
            "Starting Epoch 21\n",
            "Train Epoch: 21 [0/14335 (0%)]\tLoss: 3114.552490\tLR: [3.3333333333333335e-05]\n",
            "Train Epoch: 21 [256/14335 (2%)]\tLoss: 2739.166748\tLR: [3.331349206349206e-05]\n",
            "Train Epoch: 21 [512/14335 (4%)]\tLoss: 3255.043701\tLR: [3.3293650793650796e-05]\n",
            "Train Epoch: 21 [768/14335 (5%)]\tLoss: 3259.215088\tLR: [3.327380952380952e-05]\n",
            "Train Epoch: 21 [1024/14335 (7%)]\tLoss: 3128.073730\tLR: [3.3253968253968256e-05]\n",
            "Train Epoch: 21 [1280/14335 (9%)]\tLoss: 3147.021484\tLR: [3.3234126984126983e-05]\n",
            "Train Epoch: 21 [1536/14335 (11%)]\tLoss: 2731.853027\tLR: [3.321428571428572e-05]\n",
            "Train Epoch: 21 [1792/14335 (12%)]\tLoss: 3025.752441\tLR: [3.3194444444444444e-05]\n",
            "Train Epoch: 21 [2048/14335 (14%)]\tLoss: 2643.671631\tLR: [3.317460317460318e-05]\n",
            "Train Epoch: 21 [2304/14335 (16%)]\tLoss: 3444.459717\tLR: [3.3154761904761905e-05]\n",
            "Train Epoch: 21 [2560/14335 (18%)]\tLoss: 2929.981445\tLR: [3.313492063492064e-05]\n",
            "Train Epoch: 21 [2816/14335 (20%)]\tLoss: 3167.796875\tLR: [3.3115079365079366e-05]\n",
            "Train Epoch: 21 [3072/14335 (21%)]\tLoss: 2556.634521\tLR: [3.309523809523809e-05]\n",
            "Train Epoch: 21 [3328/14335 (23%)]\tLoss: 2699.663086\tLR: [3.3075396825396826e-05]\n",
            "Train Epoch: 21 [3584/14335 (25%)]\tLoss: 2852.507568\tLR: [3.3055555555555553e-05]\n",
            "Train Epoch: 21 [3840/14335 (27%)]\tLoss: 3201.825928\tLR: [3.303571428571429e-05]\n",
            "Train Epoch: 21 [4096/14335 (29%)]\tLoss: 2848.681641\tLR: [3.3015873015873014e-05]\n",
            "Train Epoch: 21 [4352/14335 (30%)]\tLoss: 2549.270020\tLR: [3.299603174603175e-05]\n",
            "Train Epoch: 21 [4608/14335 (32%)]\tLoss: 2797.185547\tLR: [3.2976190476190475e-05]\n",
            "Train Epoch: 21 [4864/14335 (34%)]\tLoss: 2880.729980\tLR: [3.295634920634921e-05]\n",
            "Train Epoch: 21 [5120/14335 (36%)]\tLoss: 2822.895752\tLR: [3.2936507936507936e-05]\n",
            "Train Epoch: 21 [5376/14335 (38%)]\tLoss: 2876.371338\tLR: [3.291666666666667e-05]\n",
            "Train Epoch: 21 [5632/14335 (39%)]\tLoss: 3185.026367\tLR: [3.2896825396825396e-05]\n",
            "Train Epoch: 21 [5888/14335 (41%)]\tLoss: 2891.626465\tLR: [3.287698412698413e-05]\n",
            "Train Epoch: 21 [6144/14335 (43%)]\tLoss: 2978.978760\tLR: [3.285714285714286e-05]\n",
            "Train Epoch: 21 [6400/14335 (45%)]\tLoss: 3320.871826\tLR: [3.283730158730159e-05]\n",
            "Train Epoch: 21 [6656/14335 (46%)]\tLoss: 3124.294678\tLR: [3.281746031746032e-05]\n",
            "Train Epoch: 21 [6912/14335 (48%)]\tLoss: 3137.225342\tLR: [3.279761904761905e-05]\n",
            "Train Epoch: 21 [7168/14335 (50%)]\tLoss: 3055.557129\tLR: [3.277777777777778e-05]\n",
            "Train Epoch: 21 [7424/14335 (52%)]\tLoss: 2898.637695\tLR: [3.275793650793651e-05]\n",
            "Train Epoch: 21 [7680/14335 (54%)]\tLoss: 3170.093750\tLR: [3.273809523809524e-05]\n",
            "Train Epoch: 21 [7936/14335 (55%)]\tLoss: 2966.588867\tLR: [3.271825396825397e-05]\n",
            "Train Epoch: 21 [8192/14335 (57%)]\tLoss: 2896.301514\tLR: [3.26984126984127e-05]\n",
            "Train Epoch: 21 [8448/14335 (59%)]\tLoss: 3131.000244\tLR: [3.2678571428571434e-05]\n",
            "Train Epoch: 21 [8704/14335 (61%)]\tLoss: 2873.773438\tLR: [3.265873015873016e-05]\n",
            "Train Epoch: 21 [8960/14335 (62%)]\tLoss: 3254.809570\tLR: [3.263888888888889e-05]\n",
            "Train Epoch: 21 [9216/14335 (64%)]\tLoss: 3140.185791\tLR: [3.261904761904762e-05]\n",
            "Train Epoch: 21 [9472/14335 (66%)]\tLoss: 3312.768311\tLR: [3.259920634920635e-05]\n",
            "Train Epoch: 21 [9728/14335 (68%)]\tLoss: 2703.940918\tLR: [3.257936507936508e-05]\n",
            "Train Epoch: 21 [9984/14335 (70%)]\tLoss: 3165.819336\tLR: [3.255952380952381e-05]\n",
            "Train Epoch: 21 [10240/14335 (71%)]\tLoss: 3154.140625\tLR: [3.253968253968254e-05]\n",
            "Train Epoch: 21 [10496/14335 (73%)]\tLoss: 2666.107422\tLR: [3.251984126984127e-05]\n",
            "Train Epoch: 21 [10752/14335 (75%)]\tLoss: 3001.540771\tLR: [3.2500000000000004e-05]\n",
            "Train Epoch: 21 [11008/14335 (77%)]\tLoss: 3417.636963\tLR: [3.248015873015873e-05]\n",
            "Train Epoch: 21 [11264/14335 (79%)]\tLoss: 2685.274902\tLR: [3.2460317460317465e-05]\n",
            "Train Epoch: 21 [11520/14335 (80%)]\tLoss: 2986.993652\tLR: [3.244047619047619e-05]\n",
            "Train Epoch: 21 [11776/14335 (82%)]\tLoss: 3370.788818\tLR: [3.2420634920634925e-05]\n",
            "Train Epoch: 21 [12032/14335 (84%)]\tLoss: 3042.474609\tLR: [3.240079365079365e-05]\n",
            "Train Epoch: 21 [12288/14335 (86%)]\tLoss: 3239.016846\tLR: [3.2380952380952386e-05]\n",
            "Train Epoch: 21 [12544/14335 (88%)]\tLoss: 3004.355469\tLR: [3.236111111111111e-05]\n",
            "Train Epoch: 21 [12800/14335 (89%)]\tLoss: 2705.182129\tLR: [3.234126984126985e-05]\n",
            "Train Epoch: 21 [13056/14335 (91%)]\tLoss: 2929.267334\tLR: [3.2321428571428574e-05]\n",
            "Train Epoch: 21 [13312/14335 (93%)]\tLoss: 3047.070557\tLR: [3.230158730158731e-05]\n",
            "Train Epoch: 21 [13568/14335 (95%)]\tLoss: 3155.399414\tLR: [3.2281746031746035e-05]\n",
            "Train Epoch: 21 [13824/14335 (96%)]\tLoss: 3214.676270\tLR: [3.226190476190477e-05]\n",
            "Train Epoch: 21 [14025/14335 (98%)]\tLoss: 2913.348389\tLR: [3.2242063492063495e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 21 is 0.913961038961039, Best ROC Score is:0.9312820512820513\n",
            "Starting Epoch 22\n",
            "Train Epoch: 22 [0/14335 (0%)]\tLoss: 3183.731689\tLR: [3.222222222222223e-05]\n",
            "Train Epoch: 22 [256/14335 (2%)]\tLoss: 3284.201172\tLR: [3.2202380952380956e-05]\n",
            "Train Epoch: 22 [512/14335 (4%)]\tLoss: 2887.732666\tLR: [3.218253968253968e-05]\n",
            "Train Epoch: 22 [768/14335 (5%)]\tLoss: 2878.705078\tLR: [3.216269841269842e-05]\n",
            "Train Epoch: 22 [1024/14335 (7%)]\tLoss: 2886.089844\tLR: [3.2142857142857144e-05]\n",
            "Train Epoch: 22 [1280/14335 (9%)]\tLoss: 2949.190430\tLR: [3.212301587301588e-05]\n",
            "Train Epoch: 22 [1536/14335 (11%)]\tLoss: 2997.539795\tLR: [3.2103174603174605e-05]\n",
            "Train Epoch: 22 [1792/14335 (12%)]\tLoss: 3387.450684\tLR: [3.208333333333334e-05]\n",
            "Train Epoch: 22 [2048/14335 (14%)]\tLoss: 2988.748779\tLR: [3.2063492063492065e-05]\n",
            "Train Epoch: 22 [2304/14335 (16%)]\tLoss: 3448.314941\tLR: [3.20436507936508e-05]\n",
            "Train Epoch: 22 [2560/14335 (18%)]\tLoss: 3158.738281\tLR: [3.202380952380952e-05]\n",
            "Train Epoch: 22 [2816/14335 (20%)]\tLoss: 2880.861816\tLR: [3.200396825396825e-05]\n",
            "Train Epoch: 22 [3072/14335 (21%)]\tLoss: 2675.345215\tLR: [3.198412698412698e-05]\n",
            "Train Epoch: 22 [3328/14335 (23%)]\tLoss: 3000.084473\tLR: [3.1964285714285714e-05]\n",
            "Train Epoch: 22 [3584/14335 (25%)]\tLoss: 3208.218018\tLR: [3.194444444444444e-05]\n",
            "Train Epoch: 22 [3840/14335 (27%)]\tLoss: 2907.327637\tLR: [3.1924603174603175e-05]\n",
            "Train Epoch: 22 [4096/14335 (29%)]\tLoss: 3232.332031\tLR: [3.19047619047619e-05]\n",
            "Train Epoch: 22 [4352/14335 (30%)]\tLoss: 2860.972900\tLR: [3.1884920634920635e-05]\n",
            "Train Epoch: 22 [4608/14335 (32%)]\tLoss: 3184.418213\tLR: [3.186507936507936e-05]\n",
            "Train Epoch: 22 [4864/14335 (34%)]\tLoss: 2959.811768\tLR: [3.1845238095238096e-05]\n",
            "Train Epoch: 22 [5120/14335 (36%)]\tLoss: 3224.884277\tLR: [3.182539682539682e-05]\n",
            "Train Epoch: 22 [5376/14335 (38%)]\tLoss: 3197.615234\tLR: [3.180555555555556e-05]\n",
            "Train Epoch: 22 [5632/14335 (39%)]\tLoss: 3089.251221\tLR: [3.1785714285714284e-05]\n",
            "Train Epoch: 22 [5888/14335 (41%)]\tLoss: 3065.729736\tLR: [3.176587301587302e-05]\n",
            "Train Epoch: 22 [6144/14335 (43%)]\tLoss: 2764.328125\tLR: [3.1746031746031745e-05]\n",
            "Train Epoch: 22 [6400/14335 (45%)]\tLoss: 3121.864014\tLR: [3.172619047619048e-05]\n",
            "Train Epoch: 22 [6656/14335 (46%)]\tLoss: 2697.211670\tLR: [3.1706349206349205e-05]\n",
            "Train Epoch: 22 [6912/14335 (48%)]\tLoss: 3028.268799\tLR: [3.168650793650794e-05]\n",
            "Train Epoch: 22 [7168/14335 (50%)]\tLoss: 2902.627441\tLR: [3.1666666666666666e-05]\n",
            "Train Epoch: 22 [7424/14335 (52%)]\tLoss: 2655.101318\tLR: [3.16468253968254e-05]\n",
            "Train Epoch: 22 [7680/14335 (54%)]\tLoss: 2859.345459\tLR: [3.162698412698413e-05]\n",
            "Train Epoch: 22 [7936/14335 (55%)]\tLoss: 2892.131104\tLR: [3.160714285714286e-05]\n",
            "Train Epoch: 22 [8192/14335 (57%)]\tLoss: 2267.828369\tLR: [3.158730158730159e-05]\n",
            "Train Epoch: 22 [8448/14335 (59%)]\tLoss: 2943.831787\tLR: [3.1567460317460315e-05]\n",
            "Train Epoch: 22 [8704/14335 (61%)]\tLoss: 2637.417969\tLR: [3.154761904761905e-05]\n",
            "Train Epoch: 22 [8960/14335 (62%)]\tLoss: 3108.030762\tLR: [3.1527777777777775e-05]\n",
            "Train Epoch: 22 [9216/14335 (64%)]\tLoss: 3328.041016\tLR: [3.150793650793651e-05]\n",
            "Train Epoch: 22 [9472/14335 (66%)]\tLoss: 2902.866943\tLR: [3.1488095238095236e-05]\n",
            "Train Epoch: 22 [9728/14335 (68%)]\tLoss: 2954.834229\tLR: [3.146825396825397e-05]\n",
            "Train Epoch: 22 [9984/14335 (70%)]\tLoss: 2728.788330\tLR: [3.14484126984127e-05]\n",
            "Train Epoch: 22 [10240/14335 (71%)]\tLoss: 2981.277100\tLR: [3.142857142857143e-05]\n",
            "Train Epoch: 22 [10496/14335 (73%)]\tLoss: 3249.337891\tLR: [3.140873015873016e-05]\n",
            "Train Epoch: 22 [10752/14335 (75%)]\tLoss: 2916.589111\tLR: [3.138888888888889e-05]\n",
            "Train Epoch: 22 [11008/14335 (77%)]\tLoss: 3102.175781\tLR: [3.136904761904762e-05]\n",
            "Train Epoch: 22 [11264/14335 (79%)]\tLoss: 3304.580078\tLR: [3.134920634920635e-05]\n",
            "Train Epoch: 22 [11520/14335 (80%)]\tLoss: 3132.427246\tLR: [3.132936507936508e-05]\n",
            "Train Epoch: 22 [11776/14335 (82%)]\tLoss: 2674.878174\tLR: [3.130952380952381e-05]\n",
            "Train Epoch: 22 [12032/14335 (84%)]\tLoss: 3011.624268\tLR: [3.128968253968254e-05]\n",
            "Train Epoch: 22 [12288/14335 (86%)]\tLoss: 2942.157959\tLR: [3.1269841269841274e-05]\n",
            "Train Epoch: 22 [12544/14335 (88%)]\tLoss: 2974.608887\tLR: [3.125e-05]\n",
            "Train Epoch: 22 [12800/14335 (89%)]\tLoss: 3123.206299\tLR: [3.1230158730158734e-05]\n",
            "Train Epoch: 22 [13056/14335 (91%)]\tLoss: 3135.615234\tLR: [3.121031746031746e-05]\n",
            "Train Epoch: 22 [13312/14335 (93%)]\tLoss: 2946.743896\tLR: [3.1190476190476195e-05]\n",
            "Train Epoch: 22 [13568/14335 (95%)]\tLoss: 2921.159424\tLR: [3.117063492063492e-05]\n",
            "Train Epoch: 22 [13824/14335 (96%)]\tLoss: 3165.105713\tLR: [3.1150793650793656e-05]\n",
            "Train Epoch: 22 [14025/14335 (98%)]\tLoss: 2707.832031\tLR: [3.113095238095238e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 22 is 0.8623188405797102, Best ROC Score is:0.9312820512820513\n",
            "Starting Epoch 23\n",
            "Train Epoch: 23 [0/14335 (0%)]\tLoss: 2731.177246\tLR: [3.111111111111111e-05]\n",
            "Train Epoch: 23 [256/14335 (2%)]\tLoss: 2671.409668\tLR: [3.1091269841269844e-05]\n",
            "Train Epoch: 23 [512/14335 (4%)]\tLoss: 3165.028564\tLR: [3.107142857142857e-05]\n",
            "Train Epoch: 23 [768/14335 (5%)]\tLoss: 3113.487793\tLR: [3.1051587301587304e-05]\n",
            "Train Epoch: 23 [1024/14335 (7%)]\tLoss: 3351.907227\tLR: [3.103174603174603e-05]\n",
            "Train Epoch: 23 [1280/14335 (9%)]\tLoss: 2977.676025\tLR: [3.1011904761904765e-05]\n",
            "Train Epoch: 23 [1536/14335 (11%)]\tLoss: 2954.951660\tLR: [3.099206349206349e-05]\n",
            "Train Epoch: 23 [1792/14335 (12%)]\tLoss: 3040.870850\tLR: [3.0972222222222226e-05]\n",
            "Train Epoch: 23 [2048/14335 (14%)]\tLoss: 3163.895264\tLR: [3.095238095238095e-05]\n",
            "Train Epoch: 23 [2304/14335 (16%)]\tLoss: 2991.922119\tLR: [3.093253968253969e-05]\n",
            "Train Epoch: 23 [2560/14335 (18%)]\tLoss: 2970.871338\tLR: [3.0912698412698414e-05]\n",
            "Train Epoch: 23 [2816/14335 (20%)]\tLoss: 3056.271484\tLR: [3.089285714285715e-05]\n",
            "Train Epoch: 23 [3072/14335 (21%)]\tLoss: 3000.087891\tLR: [3.0873015873015874e-05]\n",
            "Train Epoch: 23 [3328/14335 (23%)]\tLoss: 3171.757080\tLR: [3.085317460317461e-05]\n",
            "Train Epoch: 23 [3584/14335 (25%)]\tLoss: 2982.031738\tLR: [3.0833333333333335e-05]\n",
            "Train Epoch: 23 [3840/14335 (27%)]\tLoss: 3145.657715\tLR: [3.081349206349207e-05]\n",
            "Train Epoch: 23 [4096/14335 (29%)]\tLoss: 3406.238281\tLR: [3.0793650793650796e-05]\n",
            "Train Epoch: 23 [4352/14335 (30%)]\tLoss: 3123.272461\tLR: [3.077380952380953e-05]\n",
            "Train Epoch: 23 [4608/14335 (32%)]\tLoss: 2954.939941\tLR: [3.075396825396826e-05]\n",
            "Train Epoch: 23 [4864/14335 (34%)]\tLoss: 3017.965820\tLR: [3.073412698412699e-05]\n",
            "Train Epoch: 23 [5120/14335 (36%)]\tLoss: 3102.912109\tLR: [3.071428571428572e-05]\n",
            "Train Epoch: 23 [5376/14335 (38%)]\tLoss: 3030.806396\tLR: [3.069444444444445e-05]\n",
            "Train Epoch: 23 [5632/14335 (39%)]\tLoss: 2987.490479\tLR: [3.067460317460318e-05]\n",
            "Train Epoch: 23 [5888/14335 (41%)]\tLoss: 2817.312500\tLR: [3.0654761904761905e-05]\n",
            "Train Epoch: 23 [6144/14335 (43%)]\tLoss: 2970.627441\tLR: [3.063492063492064e-05]\n",
            "Train Epoch: 23 [6400/14335 (45%)]\tLoss: 3003.417969\tLR: [3.0615079365079366e-05]\n",
            "Train Epoch: 23 [6656/14335 (46%)]\tLoss: 2498.568359\tLR: [3.05952380952381e-05]\n",
            "Train Epoch: 23 [6912/14335 (48%)]\tLoss: 3033.585938\tLR: [3.057539682539683e-05]\n",
            "Train Epoch: 23 [7168/14335 (50%)]\tLoss: 2679.297119\tLR: [3.055555555555556e-05]\n",
            "Train Epoch: 23 [7424/14335 (52%)]\tLoss: 3221.700195\tLR: [3.053571428571429e-05]\n",
            "Train Epoch: 23 [7680/14335 (54%)]\tLoss: 3066.892578\tLR: [3.051587301587302e-05]\n",
            "Train Epoch: 23 [7936/14335 (55%)]\tLoss: 2782.920410\tLR: [3.049603174603175e-05]\n",
            "Train Epoch: 23 [8192/14335 (57%)]\tLoss: 2844.391357\tLR: [3.0476190476190482e-05]\n",
            "Train Epoch: 23 [8448/14335 (59%)]\tLoss: 2860.094727\tLR: [3.0456349206349206e-05]\n",
            "Train Epoch: 23 [8704/14335 (61%)]\tLoss: 3066.842285\tLR: [3.0436507936507936e-05]\n",
            "Train Epoch: 23 [8960/14335 (62%)]\tLoss: 2615.658691\tLR: [3.0416666666666666e-05]\n",
            "Train Epoch: 23 [9216/14335 (64%)]\tLoss: 2921.344482\tLR: [3.0396825396825397e-05]\n",
            "Train Epoch: 23 [9472/14335 (66%)]\tLoss: 2998.692383\tLR: [3.0376984126984127e-05]\n",
            "Train Epoch: 23 [9728/14335 (68%)]\tLoss: 3455.414307\tLR: [3.0357142857142857e-05]\n",
            "Train Epoch: 23 [9984/14335 (70%)]\tLoss: 3511.324463\tLR: [3.0337301587301588e-05]\n",
            "Train Epoch: 23 [10240/14335 (71%)]\tLoss: 2985.290039\tLR: [3.0317460317460318e-05]\n",
            "Train Epoch: 23 [10496/14335 (73%)]\tLoss: 2906.317139\tLR: [3.029761904761905e-05]\n",
            "Train Epoch: 23 [10752/14335 (75%)]\tLoss: 2677.710449\tLR: [3.0277777777777776e-05]\n",
            "Train Epoch: 23 [11008/14335 (77%)]\tLoss: 3036.651367\tLR: [3.0257936507936506e-05]\n",
            "Train Epoch: 23 [11264/14335 (79%)]\tLoss: 2840.821533\tLR: [3.0238095238095236e-05]\n",
            "Train Epoch: 23 [11520/14335 (80%)]\tLoss: 2853.641113\tLR: [3.0218253968253967e-05]\n",
            "Train Epoch: 23 [11776/14335 (82%)]\tLoss: 2975.971680\tLR: [3.0198412698412697e-05]\n",
            "Train Epoch: 23 [12032/14335 (84%)]\tLoss: 3083.123779\tLR: [3.0178571428571427e-05]\n",
            "Train Epoch: 23 [12288/14335 (86%)]\tLoss: 2880.930420\tLR: [3.0158730158730158e-05]\n",
            "Train Epoch: 23 [12544/14335 (88%)]\tLoss: 2858.797119\tLR: [3.0138888888888888e-05]\n",
            "Train Epoch: 23 [12800/14335 (89%)]\tLoss: 3031.340088\tLR: [3.011904761904762e-05]\n",
            "Train Epoch: 23 [13056/14335 (91%)]\tLoss: 3092.767822\tLR: [3.009920634920635e-05]\n",
            "Train Epoch: 23 [13312/14335 (93%)]\tLoss: 3291.972168\tLR: [3.007936507936508e-05]\n",
            "Train Epoch: 23 [13568/14335 (95%)]\tLoss: 2785.396240\tLR: [3.005952380952381e-05]\n",
            "Train Epoch: 23 [13824/14335 (96%)]\tLoss: 2717.319824\tLR: [3.003968253968254e-05]\n",
            "Train Epoch: 23 [14025/14335 (98%)]\tLoss: 3240.849854\tLR: [3.001984126984127e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "saved model with best validaton in epoch23\n",
            "Evaluation ROC Score in epoch 23 is 0.9615036231884058, Best ROC Score is:0.9615036231884058\n",
            "Starting Epoch 24\n",
            "Train Epoch: 24 [0/14335 (0%)]\tLoss: 3003.016113\tLR: [3e-05]\n",
            "Train Epoch: 24 [256/14335 (2%)]\tLoss: 3484.128174\tLR: [2.998015873015873e-05]\n",
            "Train Epoch: 24 [512/14335 (4%)]\tLoss: 3333.045166\tLR: [2.996031746031746e-05]\n",
            "Train Epoch: 24 [768/14335 (5%)]\tLoss: 2733.222168\tLR: [2.9940476190476192e-05]\n",
            "Train Epoch: 24 [1024/14335 (7%)]\tLoss: 2966.686035\tLR: [2.9920634920634922e-05]\n",
            "Train Epoch: 24 [1280/14335 (9%)]\tLoss: 2971.790283\tLR: [2.9900793650793653e-05]\n",
            "Train Epoch: 24 [1536/14335 (11%)]\tLoss: 3160.488037\tLR: [2.9880952380952383e-05]\n",
            "Train Epoch: 24 [1792/14335 (12%)]\tLoss: 3054.478516\tLR: [2.9861111111111113e-05]\n",
            "Train Epoch: 24 [2048/14335 (14%)]\tLoss: 2893.321777\tLR: [2.9841269841269844e-05]\n",
            "Train Epoch: 24 [2304/14335 (16%)]\tLoss: 3298.727783\tLR: [2.982142857142857e-05]\n",
            "Train Epoch: 24 [2560/14335 (18%)]\tLoss: 2860.765381\tLR: [2.98015873015873e-05]\n",
            "Train Epoch: 24 [2816/14335 (20%)]\tLoss: 3526.549316\tLR: [2.978174603174603e-05]\n",
            "Train Epoch: 24 [3072/14335 (21%)]\tLoss: 2612.240967\tLR: [2.9761904761904762e-05]\n",
            "Train Epoch: 24 [3328/14335 (23%)]\tLoss: 2934.645020\tLR: [2.9742063492063492e-05]\n",
            "Train Epoch: 24 [3584/14335 (25%)]\tLoss: 3011.717041\tLR: [2.9722222222222223e-05]\n",
            "Train Epoch: 24 [3840/14335 (27%)]\tLoss: 2741.661377\tLR: [2.9702380952380953e-05]\n",
            "Train Epoch: 24 [4096/14335 (29%)]\tLoss: 2501.501465\tLR: [2.9682539682539683e-05]\n",
            "Train Epoch: 24 [4352/14335 (30%)]\tLoss: 3108.447266\tLR: [2.9662698412698414e-05]\n",
            "Train Epoch: 24 [4608/14335 (32%)]\tLoss: 2993.863525\tLR: [2.9642857142857144e-05]\n",
            "Train Epoch: 24 [4864/14335 (34%)]\tLoss: 2910.170166\tLR: [2.9623015873015875e-05]\n",
            "Train Epoch: 24 [5120/14335 (36%)]\tLoss: 3123.962646\tLR: [2.9603174603174605e-05]\n",
            "Train Epoch: 24 [5376/14335 (38%)]\tLoss: 2832.510498\tLR: [2.9583333333333335e-05]\n",
            "Train Epoch: 24 [5632/14335 (39%)]\tLoss: 3133.237061\tLR: [2.9563492063492066e-05]\n",
            "Train Epoch: 24 [5888/14335 (41%)]\tLoss: 3183.697266\tLR: [2.9543650793650796e-05]\n",
            "Train Epoch: 24 [6144/14335 (43%)]\tLoss: 2659.708008\tLR: [2.9523809523809526e-05]\n",
            "Train Epoch: 24 [6400/14335 (45%)]\tLoss: 2952.203369\tLR: [2.9503968253968257e-05]\n",
            "Train Epoch: 24 [6656/14335 (46%)]\tLoss: 2802.686523\tLR: [2.9484126984126987e-05]\n",
            "Train Epoch: 24 [6912/14335 (48%)]\tLoss: 3213.653809\tLR: [2.9464285714285718e-05]\n",
            "Train Epoch: 24 [7168/14335 (50%)]\tLoss: 2701.926025\tLR: [2.9444444444444448e-05]\n",
            "Train Epoch: 24 [7424/14335 (52%)]\tLoss: 2804.010498\tLR: [2.9424603174603178e-05]\n",
            "Train Epoch: 24 [7680/14335 (54%)]\tLoss: 3333.822266\tLR: [2.940476190476191e-05]\n",
            "Train Epoch: 24 [7936/14335 (55%)]\tLoss: 2915.933838\tLR: [2.938492063492064e-05]\n",
            "Train Epoch: 24 [8192/14335 (57%)]\tLoss: 3109.852051\tLR: [2.9365079365079366e-05]\n",
            "Train Epoch: 24 [8448/14335 (59%)]\tLoss: 3203.640869\tLR: [2.9345238095238096e-05]\n",
            "Train Epoch: 24 [8704/14335 (61%)]\tLoss: 2908.704590\tLR: [2.9325396825396827e-05]\n",
            "Train Epoch: 24 [8960/14335 (62%)]\tLoss: 2864.703369\tLR: [2.9305555555555557e-05]\n",
            "Train Epoch: 24 [9216/14335 (64%)]\tLoss: 2978.112305\tLR: [2.9285714285714288e-05]\n",
            "Train Epoch: 24 [9472/14335 (66%)]\tLoss: 2893.769043\tLR: [2.9265873015873018e-05]\n",
            "Train Epoch: 24 [9728/14335 (68%)]\tLoss: 3154.819580\tLR: [2.9246031746031748e-05]\n",
            "Train Epoch: 24 [9984/14335 (70%)]\tLoss: 3143.278564\tLR: [2.922619047619048e-05]\n",
            "Train Epoch: 24 [10240/14335 (71%)]\tLoss: 2765.585205\tLR: [2.920634920634921e-05]\n",
            "Train Epoch: 24 [10496/14335 (73%)]\tLoss: 3259.935547\tLR: [2.918650793650794e-05]\n",
            "Train Epoch: 24 [10752/14335 (75%)]\tLoss: 2919.280029\tLR: [2.916666666666667e-05]\n",
            "Train Epoch: 24 [11008/14335 (77%)]\tLoss: 2875.398193\tLR: [2.91468253968254e-05]\n",
            "Train Epoch: 24 [11264/14335 (79%)]\tLoss: 2577.951172\tLR: [2.912698412698413e-05]\n",
            "Train Epoch: 24 [11520/14335 (80%)]\tLoss: 3003.031006\tLR: [2.910714285714286e-05]\n",
            "Train Epoch: 24 [11776/14335 (82%)]\tLoss: 2669.189453\tLR: [2.908730158730159e-05]\n",
            "Train Epoch: 24 [12032/14335 (84%)]\tLoss: 2831.324463\tLR: [2.906746031746032e-05]\n",
            "Train Epoch: 24 [12288/14335 (86%)]\tLoss: 3312.849854\tLR: [2.9047619047619052e-05]\n",
            "Train Epoch: 24 [12544/14335 (88%)]\tLoss: 2754.370361\tLR: [2.9027777777777782e-05]\n",
            "Train Epoch: 24 [12800/14335 (89%)]\tLoss: 2721.113770\tLR: [2.9007936507936513e-05]\n",
            "Train Epoch: 24 [13056/14335 (91%)]\tLoss: 3460.511230\tLR: [2.8988095238095243e-05]\n",
            "Train Epoch: 24 [13312/14335 (93%)]\tLoss: 3447.832520\tLR: [2.8968253968253974e-05]\n",
            "Train Epoch: 24 [13568/14335 (95%)]\tLoss: 3054.058838\tLR: [2.8948412698412704e-05]\n",
            "Train Epoch: 24 [13824/14335 (96%)]\tLoss: 3076.239990\tLR: [2.8928571428571434e-05]\n",
            "Train Epoch: 24 [14025/14335 (98%)]\tLoss: 3029.048340\tLR: [2.890873015873016e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 24 is 0.9188722669735327, Best ROC Score is:0.9615036231884058\n",
            "Starting Epoch 25\n",
            "Train Epoch: 25 [0/14335 (0%)]\tLoss: 2662.775391\tLR: [2.8888888888888888e-05]\n",
            "Train Epoch: 25 [256/14335 (2%)]\tLoss: 3040.760986\tLR: [2.886904761904762e-05]\n",
            "Train Epoch: 25 [512/14335 (4%)]\tLoss: 2968.391846\tLR: [2.884920634920635e-05]\n",
            "Train Epoch: 25 [768/14335 (5%)]\tLoss: 2766.774414\tLR: [2.882936507936508e-05]\n",
            "Train Epoch: 25 [1024/14335 (7%)]\tLoss: 3121.570068\tLR: [2.880952380952381e-05]\n",
            "Train Epoch: 25 [1280/14335 (9%)]\tLoss: 2764.142334\tLR: [2.878968253968254e-05]\n",
            "Train Epoch: 25 [1536/14335 (11%)]\tLoss: 2692.426270\tLR: [2.876984126984127e-05]\n",
            "Train Epoch: 25 [1792/14335 (12%)]\tLoss: 3285.535156\tLR: [2.8749999999999997e-05]\n",
            "Train Epoch: 25 [2048/14335 (14%)]\tLoss: 3060.889648\tLR: [2.8730158730158728e-05]\n",
            "Train Epoch: 25 [2304/14335 (16%)]\tLoss: 2772.670410\tLR: [2.8710317460317458e-05]\n",
            "Train Epoch: 25 [2560/14335 (18%)]\tLoss: 3300.256104\tLR: [2.869047619047619e-05]\n",
            "Train Epoch: 25 [2816/14335 (20%)]\tLoss: 3303.348389\tLR: [2.867063492063492e-05]\n",
            "Train Epoch: 25 [3072/14335 (21%)]\tLoss: 2734.550537\tLR: [2.865079365079365e-05]\n",
            "Train Epoch: 25 [3328/14335 (23%)]\tLoss: 3088.137695\tLR: [2.863095238095238e-05]\n",
            "Train Epoch: 25 [3584/14335 (25%)]\tLoss: 3142.541260\tLR: [2.861111111111111e-05]\n",
            "Train Epoch: 25 [3840/14335 (27%)]\tLoss: 2975.165527\tLR: [2.859126984126984e-05]\n",
            "Train Epoch: 25 [4096/14335 (29%)]\tLoss: 3283.436523\tLR: [2.857142857142857e-05]\n",
            "Train Epoch: 25 [4352/14335 (30%)]\tLoss: 2822.246826\tLR: [2.85515873015873e-05]\n",
            "Train Epoch: 25 [4608/14335 (32%)]\tLoss: 3273.707031\tLR: [2.853174603174603e-05]\n",
            "Train Epoch: 25 [4864/14335 (34%)]\tLoss: 2743.367676\tLR: [2.8511904761904762e-05]\n",
            "Train Epoch: 25 [5120/14335 (36%)]\tLoss: 3055.357422\tLR: [2.8492063492063492e-05]\n",
            "Train Epoch: 25 [5376/14335 (38%)]\tLoss: 3078.481689\tLR: [2.8472222222222223e-05]\n",
            "Train Epoch: 25 [5632/14335 (39%)]\tLoss: 2780.640381\tLR: [2.8452380952380953e-05]\n",
            "Train Epoch: 25 [5888/14335 (41%)]\tLoss: 3175.039307\tLR: [2.8432539682539683e-05]\n",
            "Train Epoch: 25 [6144/14335 (43%)]\tLoss: 2965.842041\tLR: [2.8412698412698414e-05]\n",
            "Train Epoch: 25 [6400/14335 (45%)]\tLoss: 3110.071777\tLR: [2.8392857142857144e-05]\n",
            "Train Epoch: 25 [6656/14335 (46%)]\tLoss: 2645.954834\tLR: [2.8373015873015875e-05]\n",
            "Train Epoch: 25 [6912/14335 (48%)]\tLoss: 2453.752686\tLR: [2.8353174603174605e-05]\n",
            "Train Epoch: 25 [7168/14335 (50%)]\tLoss: 3170.599121\tLR: [2.8333333333333335e-05]\n",
            "Train Epoch: 25 [7424/14335 (52%)]\tLoss: 2949.449951\tLR: [2.8313492063492066e-05]\n",
            "Train Epoch: 25 [7680/14335 (54%)]\tLoss: 3483.970215\tLR: [2.8293650793650793e-05]\n",
            "Train Epoch: 25 [7936/14335 (55%)]\tLoss: 3082.794189\tLR: [2.8273809523809523e-05]\n",
            "Train Epoch: 25 [8192/14335 (57%)]\tLoss: 3173.502441\tLR: [2.8253968253968253e-05]\n",
            "Train Epoch: 25 [8448/14335 (59%)]\tLoss: 2609.280518\tLR: [2.8234126984126984e-05]\n",
            "Train Epoch: 25 [8704/14335 (61%)]\tLoss: 3088.741211\tLR: [2.8214285714285714e-05]\n",
            "Train Epoch: 25 [8960/14335 (62%)]\tLoss: 3007.366699\tLR: [2.8194444444444445e-05]\n",
            "Train Epoch: 25 [9216/14335 (64%)]\tLoss: 3020.588379\tLR: [2.8174603174603175e-05]\n",
            "Train Epoch: 25 [9472/14335 (66%)]\tLoss: 3028.709473\tLR: [2.8154761904761905e-05]\n",
            "Train Epoch: 25 [9728/14335 (68%)]\tLoss: 2700.113770\tLR: [2.8134920634920636e-05]\n",
            "Train Epoch: 25 [9984/14335 (70%)]\tLoss: 2866.103516\tLR: [2.8115079365079366e-05]\n",
            "Train Epoch: 25 [10240/14335 (71%)]\tLoss: 2971.296143\tLR: [2.8095238095238096e-05]\n",
            "Train Epoch: 25 [10496/14335 (73%)]\tLoss: 2553.828857\tLR: [2.8075396825396827e-05]\n",
            "Train Epoch: 25 [10752/14335 (75%)]\tLoss: 3392.083008\tLR: [2.8055555555555557e-05]\n",
            "Train Epoch: 25 [11008/14335 (77%)]\tLoss: 3327.830322\tLR: [2.8035714285714288e-05]\n",
            "Train Epoch: 25 [11264/14335 (79%)]\tLoss: 3236.291992\tLR: [2.8015873015873018e-05]\n",
            "Train Epoch: 25 [11520/14335 (80%)]\tLoss: 2761.955322\tLR: [2.799603174603175e-05]\n",
            "Train Epoch: 25 [11776/14335 (82%)]\tLoss: 3087.649414\tLR: [2.797619047619048e-05]\n",
            "Train Epoch: 25 [12032/14335 (84%)]\tLoss: 3217.903076\tLR: [2.795634920634921e-05]\n",
            "Train Epoch: 25 [12288/14335 (86%)]\tLoss: 3540.420410\tLR: [2.793650793650794e-05]\n",
            "Train Epoch: 25 [12544/14335 (88%)]\tLoss: 2733.506104\tLR: [2.791666666666667e-05]\n",
            "Train Epoch: 25 [12800/14335 (89%)]\tLoss: 2869.426758\tLR: [2.78968253968254e-05]\n",
            "Train Epoch: 25 [13056/14335 (91%)]\tLoss: 3124.260010\tLR: [2.787698412698413e-05]\n",
            "Train Epoch: 25 [13312/14335 (93%)]\tLoss: 3124.910400\tLR: [2.785714285714286e-05]\n",
            "Train Epoch: 25 [13568/14335 (95%)]\tLoss: 2662.466309\tLR: [2.7837301587301588e-05]\n",
            "Train Epoch: 25 [13824/14335 (96%)]\tLoss: 2942.272705\tLR: [2.781746031746032e-05]\n",
            "Train Epoch: 25 [14025/14335 (98%)]\tLoss: 3294.613281\tLR: [2.779761904761905e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 25 is 0.9342105263157895, Best ROC Score is:0.9615036231884058\n",
            "Starting Epoch 26\n",
            "Train Epoch: 26 [0/14335 (0%)]\tLoss: 2892.454102\tLR: [2.777777777777778e-05]\n",
            "Train Epoch: 26 [256/14335 (2%)]\tLoss: 2973.014160\tLR: [2.775793650793651e-05]\n",
            "Train Epoch: 26 [512/14335 (4%)]\tLoss: 3157.431641\tLR: [2.773809523809524e-05]\n",
            "Train Epoch: 26 [768/14335 (5%)]\tLoss: 2733.634766\tLR: [2.771825396825397e-05]\n",
            "Train Epoch: 26 [1024/14335 (7%)]\tLoss: 3095.268311\tLR: [2.76984126984127e-05]\n",
            "Train Epoch: 26 [1280/14335 (9%)]\tLoss: 2695.237793\tLR: [2.767857142857143e-05]\n",
            "Train Epoch: 26 [1536/14335 (11%)]\tLoss: 2827.057617\tLR: [2.765873015873016e-05]\n",
            "Train Epoch: 26 [1792/14335 (12%)]\tLoss: 3404.862305\tLR: [2.7638888888888892e-05]\n",
            "Train Epoch: 26 [2048/14335 (14%)]\tLoss: 3211.686035\tLR: [2.7619047619047622e-05]\n",
            "Train Epoch: 26 [2304/14335 (16%)]\tLoss: 3021.564453\tLR: [2.7599206349206352e-05]\n",
            "Train Epoch: 26 [2560/14335 (18%)]\tLoss: 2634.030762\tLR: [2.7579365079365083e-05]\n",
            "Train Epoch: 26 [2816/14335 (20%)]\tLoss: 2756.798096\tLR: [2.7559523809523813e-05]\n",
            "Train Epoch: 26 [3072/14335 (21%)]\tLoss: 3052.540527\tLR: [2.7539682539682544e-05]\n",
            "Train Epoch: 26 [3328/14335 (23%)]\tLoss: 3216.529541\tLR: [2.7519841269841274e-05]\n",
            "Train Epoch: 26 [3584/14335 (25%)]\tLoss: 3366.309082\tLR: [2.7500000000000004e-05]\n",
            "Train Epoch: 26 [3840/14335 (27%)]\tLoss: 3241.288086\tLR: [2.7480158730158735e-05]\n",
            "Train Epoch: 26 [4096/14335 (29%)]\tLoss: 2820.524658\tLR: [2.7460317460317465e-05]\n",
            "Train Epoch: 26 [4352/14335 (30%)]\tLoss: 3347.731689\tLR: [2.7440476190476195e-05]\n",
            "Train Epoch: 26 [4608/14335 (32%)]\tLoss: 2755.316650\tLR: [2.7420634920634926e-05]\n",
            "Train Epoch: 26 [4864/14335 (34%)]\tLoss: 3150.090576\tLR: [2.7400793650793656e-05]\n",
            "Train Epoch: 26 [5120/14335 (36%)]\tLoss: 2900.714600\tLR: [2.7380952380952383e-05]\n",
            "Train Epoch: 26 [5376/14335 (38%)]\tLoss: 3315.783936\tLR: [2.7361111111111114e-05]\n",
            "Train Epoch: 26 [5632/14335 (39%)]\tLoss: 3325.244629\tLR: [2.734126984126984e-05]\n",
            "Train Epoch: 26 [5888/14335 (41%)]\tLoss: 3209.252441\tLR: [2.732142857142857e-05]\n",
            "Train Epoch: 26 [6144/14335 (43%)]\tLoss: 2874.073975\tLR: [2.73015873015873e-05]\n",
            "Train Epoch: 26 [6400/14335 (45%)]\tLoss: 2846.260254\tLR: [2.7281746031746032e-05]\n",
            "Train Epoch: 26 [6656/14335 (46%)]\tLoss: 2623.837891\tLR: [2.7261904761904762e-05]\n",
            "Train Epoch: 26 [6912/14335 (48%)]\tLoss: 2951.183838\tLR: [2.7242063492063492e-05]\n",
            "Train Epoch: 26 [7168/14335 (50%)]\tLoss: 3585.360107\tLR: [2.7222222222222223e-05]\n",
            "Train Epoch: 26 [7424/14335 (52%)]\tLoss: 3011.774902\tLR: [2.720238095238095e-05]\n",
            "Train Epoch: 26 [7680/14335 (54%)]\tLoss: 2998.168701\tLR: [2.718253968253968e-05]\n",
            "Train Epoch: 26 [7936/14335 (55%)]\tLoss: 2983.830811\tLR: [2.716269841269841e-05]\n",
            "Train Epoch: 26 [8192/14335 (57%)]\tLoss: 2795.980469\tLR: [2.714285714285714e-05]\n",
            "Train Epoch: 26 [8448/14335 (59%)]\tLoss: 2799.680176\tLR: [2.712301587301587e-05]\n",
            "Train Epoch: 26 [8704/14335 (61%)]\tLoss: 3039.573486\tLR: [2.7103174603174602e-05]\n",
            "Train Epoch: 26 [8960/14335 (62%)]\tLoss: 3148.085693\tLR: [2.7083333333333332e-05]\n",
            "Train Epoch: 26 [9216/14335 (64%)]\tLoss: 3178.501953\tLR: [2.7063492063492062e-05]\n",
            "Train Epoch: 26 [9472/14335 (66%)]\tLoss: 2857.935303\tLR: [2.7043650793650793e-05]\n",
            "Train Epoch: 26 [9728/14335 (68%)]\tLoss: 3351.337402\tLR: [2.7023809523809523e-05]\n",
            "Train Epoch: 26 [9984/14335 (70%)]\tLoss: 3136.828125\tLR: [2.7003968253968254e-05]\n",
            "Train Epoch: 26 [10240/14335 (71%)]\tLoss: 3021.502686\tLR: [2.6984126984126984e-05]\n",
            "Train Epoch: 26 [10496/14335 (73%)]\tLoss: 2762.160156\tLR: [2.6964285714285714e-05]\n",
            "Train Epoch: 26 [10752/14335 (75%)]\tLoss: 2968.409180\tLR: [2.6944444444444445e-05]\n",
            "Train Epoch: 26 [11008/14335 (77%)]\tLoss: 2963.283691\tLR: [2.6924603174603175e-05]\n",
            "Train Epoch: 26 [11264/14335 (79%)]\tLoss: 3101.492920\tLR: [2.6904761904761905e-05]\n",
            "Train Epoch: 26 [11520/14335 (80%)]\tLoss: 3140.955566\tLR: [2.6884920634920636e-05]\n",
            "Train Epoch: 26 [11776/14335 (82%)]\tLoss: 2446.077148\tLR: [2.6865079365079366e-05]\n",
            "Train Epoch: 26 [12032/14335 (84%)]\tLoss: 3097.493896\tLR: [2.6845238095238097e-05]\n",
            "Train Epoch: 26 [12288/14335 (86%)]\tLoss: 2648.864746\tLR: [2.6825396825396827e-05]\n",
            "Train Epoch: 26 [12544/14335 (88%)]\tLoss: 3291.520996\tLR: [2.6805555555555557e-05]\n",
            "Train Epoch: 26 [12800/14335 (89%)]\tLoss: 2733.708984\tLR: [2.6785714285714288e-05]\n",
            "Train Epoch: 26 [13056/14335 (91%)]\tLoss: 2972.727051\tLR: [2.6765873015873018e-05]\n",
            "Train Epoch: 26 [13312/14335 (93%)]\tLoss: 2774.902344\tLR: [2.6746031746031745e-05]\n",
            "Train Epoch: 26 [13568/14335 (95%)]\tLoss: 3377.125000\tLR: [2.6726190476190475e-05]\n",
            "Train Epoch: 26 [13824/14335 (96%)]\tLoss: 3165.487549\tLR: [2.6706349206349206e-05]\n",
            "Train Epoch: 26 [14025/14335 (98%)]\tLoss: 3037.556885\tLR: [2.6686507936507936e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 26 is 0.9609810479375697, Best ROC Score is:0.9615036231884058\n",
            "Starting Epoch 27\n",
            "Train Epoch: 27 [0/14335 (0%)]\tLoss: 2724.906738\tLR: [2.6666666666666667e-05]\n",
            "Train Epoch: 27 [256/14335 (2%)]\tLoss: 2714.406982\tLR: [2.6646825396825397e-05]\n",
            "Train Epoch: 27 [512/14335 (4%)]\tLoss: 3391.447998\tLR: [2.6626984126984127e-05]\n",
            "Train Epoch: 27 [768/14335 (5%)]\tLoss: 3231.421875\tLR: [2.6607142857142858e-05]\n",
            "Train Epoch: 27 [1024/14335 (7%)]\tLoss: 2762.916992\tLR: [2.6587301587301588e-05]\n",
            "Train Epoch: 27 [1280/14335 (9%)]\tLoss: 2821.876953\tLR: [2.656746031746032e-05]\n",
            "Train Epoch: 27 [1536/14335 (11%)]\tLoss: 3356.292725\tLR: [2.654761904761905e-05]\n",
            "Train Epoch: 27 [1792/14335 (12%)]\tLoss: 2803.603027\tLR: [2.652777777777778e-05]\n",
            "Train Epoch: 27 [2048/14335 (14%)]\tLoss: 3251.519531\tLR: [2.650793650793651e-05]\n",
            "Train Epoch: 27 [2304/14335 (16%)]\tLoss: 3174.175537\tLR: [2.648809523809524e-05]\n",
            "Train Epoch: 27 [2560/14335 (18%)]\tLoss: 2976.914795\tLR: [2.646825396825397e-05]\n",
            "Train Epoch: 27 [2816/14335 (20%)]\tLoss: 3324.472412\tLR: [2.64484126984127e-05]\n",
            "Train Epoch: 27 [3072/14335 (21%)]\tLoss: 3292.479736\tLR: [2.642857142857143e-05]\n",
            "Train Epoch: 27 [3328/14335 (23%)]\tLoss: 2977.157227\tLR: [2.640873015873016e-05]\n",
            "Train Epoch: 27 [3584/14335 (25%)]\tLoss: 3163.888672\tLR: [2.6388888888888892e-05]\n",
            "Train Epoch: 27 [3840/14335 (27%)]\tLoss: 2978.033936\tLR: [2.6369047619047622e-05]\n",
            "Train Epoch: 27 [4096/14335 (29%)]\tLoss: 2856.274414\tLR: [2.6349206349206353e-05]\n",
            "Train Epoch: 27 [4352/14335 (30%)]\tLoss: 2993.467529\tLR: [2.6329365079365083e-05]\n",
            "Train Epoch: 27 [4608/14335 (32%)]\tLoss: 3054.984131\tLR: [2.6309523809523813e-05]\n",
            "Train Epoch: 27 [4864/14335 (34%)]\tLoss: 2816.446533\tLR: [2.628968253968254e-05]\n",
            "Train Epoch: 27 [5120/14335 (36%)]\tLoss: 3324.436279\tLR: [2.626984126984127e-05]\n",
            "Train Epoch: 27 [5376/14335 (38%)]\tLoss: 3073.454590\tLR: [2.625e-05]\n",
            "Train Epoch: 27 [5632/14335 (39%)]\tLoss: 3120.593750\tLR: [2.623015873015873e-05]\n",
            "Train Epoch: 27 [5888/14335 (41%)]\tLoss: 3062.074219\tLR: [2.6210317460317462e-05]\n",
            "Train Epoch: 27 [6144/14335 (43%)]\tLoss: 3247.459473\tLR: [2.6190476190476192e-05]\n",
            "Train Epoch: 27 [6400/14335 (45%)]\tLoss: 2828.460938\tLR: [2.6170634920634923e-05]\n",
            "Train Epoch: 27 [6656/14335 (46%)]\tLoss: 2993.186035\tLR: [2.6150793650793653e-05]\n",
            "Train Epoch: 27 [6912/14335 (48%)]\tLoss: 3119.506592\tLR: [2.6130952380952383e-05]\n",
            "Train Epoch: 27 [7168/14335 (50%)]\tLoss: 3093.223145\tLR: [2.6111111111111114e-05]\n",
            "Train Epoch: 27 [7424/14335 (52%)]\tLoss: 3125.132080\tLR: [2.6091269841269844e-05]\n",
            "Train Epoch: 27 [7680/14335 (54%)]\tLoss: 3457.628174\tLR: [2.6071428571428574e-05]\n",
            "Train Epoch: 27 [7936/14335 (55%)]\tLoss: 2725.108643\tLR: [2.6051587301587305e-05]\n",
            "Train Epoch: 27 [8192/14335 (57%)]\tLoss: 2871.005371\tLR: [2.6031746031746035e-05]\n",
            "Train Epoch: 27 [8448/14335 (59%)]\tLoss: 2893.274414\tLR: [2.6011904761904766e-05]\n",
            "Train Epoch: 27 [8704/14335 (61%)]\tLoss: 2957.180420\tLR: [2.5992063492063496e-05]\n",
            "Train Epoch: 27 [8960/14335 (62%)]\tLoss: 2903.708496\tLR: [2.5972222222222226e-05]\n",
            "Train Epoch: 27 [9216/14335 (64%)]\tLoss: 3149.788574\tLR: [2.5952380952380957e-05]\n",
            "Train Epoch: 27 [9472/14335 (66%)]\tLoss: 2886.624512\tLR: [2.5932539682539687e-05]\n",
            "Train Epoch: 27 [9728/14335 (68%)]\tLoss: 3134.260986\tLR: [2.5912698412698417e-05]\n",
            "Train Epoch: 27 [9984/14335 (70%)]\tLoss: 3311.918213\tLR: [2.5892857142857148e-05]\n",
            "Train Epoch: 27 [10240/14335 (71%)]\tLoss: 2530.968506\tLR: [2.5873015873015878e-05]\n",
            "Train Epoch: 27 [10496/14335 (73%)]\tLoss: 2472.145996\tLR: [2.585317460317461e-05]\n",
            "Train Epoch: 27 [10752/14335 (75%)]\tLoss: 3214.324707\tLR: [2.5833333333333336e-05]\n",
            "Train Epoch: 27 [11008/14335 (77%)]\tLoss: 2722.259766\tLR: [2.5813492063492066e-05]\n",
            "Train Epoch: 27 [11264/14335 (79%)]\tLoss: 2743.712158\tLR: [2.5793650793650796e-05]\n",
            "Train Epoch: 27 [11520/14335 (80%)]\tLoss: 3225.191406\tLR: [2.5773809523809523e-05]\n",
            "Train Epoch: 27 [11776/14335 (82%)]\tLoss: 2684.682617\tLR: [2.5753968253968254e-05]\n",
            "Train Epoch: 27 [12032/14335 (84%)]\tLoss: 3022.898682\tLR: [2.5734126984126984e-05]\n",
            "Train Epoch: 27 [12288/14335 (86%)]\tLoss: 3295.729004\tLR: [2.5714285714285714e-05]\n",
            "Train Epoch: 27 [12544/14335 (88%)]\tLoss: 2721.618408\tLR: [2.5694444444444445e-05]\n",
            "Train Epoch: 27 [12800/14335 (89%)]\tLoss: 2892.645996\tLR: [2.5674603174603172e-05]\n",
            "Train Epoch: 27 [13056/14335 (91%)]\tLoss: 3211.480713\tLR: [2.5654761904761902e-05]\n",
            "Train Epoch: 27 [13312/14335 (93%)]\tLoss: 2948.995850\tLR: [2.5634920634920633e-05]\n",
            "Train Epoch: 27 [13568/14335 (95%)]\tLoss: 3172.639893\tLR: [2.5615079365079363e-05]\n",
            "Train Epoch: 27 [13824/14335 (96%)]\tLoss: 3075.321045\tLR: [2.5595238095238093e-05]\n",
            "Train Epoch: 27 [14025/14335 (98%)]\tLoss: 2724.117432\tLR: [2.5575396825396824e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 27 is 0.9434523809523809, Best ROC Score is:0.9615036231884058\n",
            "Starting Epoch 28\n",
            "Train Epoch: 28 [0/14335 (0%)]\tLoss: 3263.658691\tLR: [2.5555555555555554e-05]\n",
            "Train Epoch: 28 [256/14335 (2%)]\tLoss: 3373.941406\tLR: [2.5535714285714284e-05]\n",
            "Train Epoch: 28 [512/14335 (4%)]\tLoss: 3129.116943\tLR: [2.5515873015873015e-05]\n",
            "Train Epoch: 28 [768/14335 (5%)]\tLoss: 3363.220947\tLR: [2.5496031746031745e-05]\n",
            "Train Epoch: 28 [1024/14335 (7%)]\tLoss: 3106.779053\tLR: [2.5476190476190476e-05]\n",
            "Train Epoch: 28 [1280/14335 (9%)]\tLoss: 2624.229004\tLR: [2.5456349206349206e-05]\n",
            "Train Epoch: 28 [1536/14335 (11%)]\tLoss: 2928.878906\tLR: [2.5436507936507936e-05]\n",
            "Train Epoch: 28 [1792/14335 (12%)]\tLoss: 3097.169922\tLR: [2.5416666666666667e-05]\n",
            "Train Epoch: 28 [2048/14335 (14%)]\tLoss: 2942.858154\tLR: [2.5396825396825397e-05]\n",
            "Train Epoch: 28 [2304/14335 (16%)]\tLoss: 2841.434326\tLR: [2.5376984126984127e-05]\n",
            "Train Epoch: 28 [2560/14335 (18%)]\tLoss: 3027.754150\tLR: [2.5357142857142858e-05]\n",
            "Train Epoch: 28 [2816/14335 (20%)]\tLoss: 3279.237061\tLR: [2.5337301587301588e-05]\n",
            "Train Epoch: 28 [3072/14335 (21%)]\tLoss: 3072.438232\tLR: [2.531746031746032e-05]\n",
            "Train Epoch: 28 [3328/14335 (23%)]\tLoss: 2683.146240\tLR: [2.529761904761905e-05]\n",
            "Train Epoch: 28 [3584/14335 (25%)]\tLoss: 2826.361816\tLR: [2.527777777777778e-05]\n",
            "Train Epoch: 28 [3840/14335 (27%)]\tLoss: 2881.692871\tLR: [2.525793650793651e-05]\n",
            "Train Epoch: 28 [4096/14335 (29%)]\tLoss: 3211.457520\tLR: [2.523809523809524e-05]\n",
            "Train Epoch: 28 [4352/14335 (30%)]\tLoss: 2657.451660\tLR: [2.5218253968253967e-05]\n",
            "Train Epoch: 28 [4608/14335 (32%)]\tLoss: 2900.357910\tLR: [2.5198412698412697e-05]\n",
            "Train Epoch: 28 [4864/14335 (34%)]\tLoss: 2908.643799\tLR: [2.5178571428571428e-05]\n",
            "Train Epoch: 28 [5120/14335 (36%)]\tLoss: 3258.647217\tLR: [2.5158730158730158e-05]\n",
            "Train Epoch: 28 [5376/14335 (38%)]\tLoss: 3298.490234\tLR: [2.513888888888889e-05]\n",
            "Train Epoch: 28 [5632/14335 (39%)]\tLoss: 2980.743408\tLR: [2.511904761904762e-05]\n",
            "Train Epoch: 28 [5888/14335 (41%)]\tLoss: 3299.849365\tLR: [2.509920634920635e-05]\n",
            "Train Epoch: 28 [6144/14335 (43%)]\tLoss: 2698.497314\tLR: [2.507936507936508e-05]\n",
            "Train Epoch: 28 [6400/14335 (45%)]\tLoss: 2946.063721\tLR: [2.505952380952381e-05]\n",
            "Train Epoch: 28 [6656/14335 (46%)]\tLoss: 2988.051025\tLR: [2.503968253968254e-05]\n",
            "Train Epoch: 28 [6912/14335 (48%)]\tLoss: 2987.828125\tLR: [2.501984126984127e-05]\n",
            "Train Epoch: 28 [7168/14335 (50%)]\tLoss: 2698.950439\tLR: [2.5e-05]\n",
            "Train Epoch: 28 [7424/14335 (52%)]\tLoss: 2534.765381\tLR: [2.498015873015873e-05]\n",
            "Train Epoch: 28 [7680/14335 (54%)]\tLoss: 2787.522217\tLR: [2.4960317460317462e-05]\n",
            "Train Epoch: 28 [7936/14335 (55%)]\tLoss: 3159.663574\tLR: [2.4940476190476192e-05]\n",
            "Train Epoch: 28 [8192/14335 (57%)]\tLoss: 2553.953857\tLR: [2.4920634920634923e-05]\n",
            "Train Epoch: 28 [8448/14335 (59%)]\tLoss: 3126.808105\tLR: [2.4900793650793653e-05]\n",
            "Train Epoch: 28 [8704/14335 (61%)]\tLoss: 3300.625244\tLR: [2.4880952380952383e-05]\n",
            "Train Epoch: 28 [8960/14335 (62%)]\tLoss: 2977.076904\tLR: [2.4861111111111114e-05]\n",
            "Train Epoch: 28 [9216/14335 (64%)]\tLoss: 2917.658691\tLR: [2.4841269841269844e-05]\n",
            "Train Epoch: 28 [9472/14335 (66%)]\tLoss: 3204.467285\tLR: [2.4821428571428575e-05]\n",
            "Train Epoch: 28 [9728/14335 (68%)]\tLoss: 3299.287598\tLR: [2.4801587301587305e-05]\n",
            "Train Epoch: 28 [9984/14335 (70%)]\tLoss: 3139.393066\tLR: [2.4781746031746035e-05]\n",
            "Train Epoch: 28 [10240/14335 (71%)]\tLoss: 2741.607178\tLR: [2.4761904761904762e-05]\n",
            "Train Epoch: 28 [10496/14335 (73%)]\tLoss: 2821.253662\tLR: [2.4742063492063493e-05]\n",
            "Train Epoch: 28 [10752/14335 (75%)]\tLoss: 2862.360107\tLR: [2.4722222222222223e-05]\n",
            "Train Epoch: 28 [11008/14335 (77%)]\tLoss: 3124.357422\tLR: [2.4702380952380953e-05]\n",
            "Train Epoch: 28 [11264/14335 (79%)]\tLoss: 3130.088867\tLR: [2.4682539682539684e-05]\n",
            "Train Epoch: 28 [11520/14335 (80%)]\tLoss: 2710.937256\tLR: [2.4662698412698414e-05]\n",
            "Train Epoch: 28 [11776/14335 (82%)]\tLoss: 3299.630371\tLR: [2.4642857142857145e-05]\n",
            "Train Epoch: 28 [12032/14335 (84%)]\tLoss: 2697.889893\tLR: [2.4623015873015875e-05]\n",
            "Train Epoch: 28 [12288/14335 (86%)]\tLoss: 3320.304199\tLR: [2.4603174603174602e-05]\n",
            "Train Epoch: 28 [12544/14335 (88%)]\tLoss: 3034.125488\tLR: [2.4583333333333332e-05]\n",
            "Train Epoch: 28 [12800/14335 (89%)]\tLoss: 3314.220947\tLR: [2.4563492063492063e-05]\n",
            "Train Epoch: 28 [13056/14335 (91%)]\tLoss: 2813.800293\tLR: [2.4543650793650793e-05]\n",
            "Train Epoch: 28 [13312/14335 (93%)]\tLoss: 3379.260986\tLR: [2.4523809523809523e-05]\n",
            "Train Epoch: 28 [13568/14335 (95%)]\tLoss: 2830.838379\tLR: [2.4503968253968254e-05]\n",
            "Train Epoch: 28 [13824/14335 (96%)]\tLoss: 2827.727539\tLR: [2.4484126984126984e-05]\n",
            "Train Epoch: 28 [14025/14335 (98%)]\tLoss: 3021.917725\tLR: [2.4464285714285715e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 28 is 0.8784838350055743, Best ROC Score is:0.9615036231884058\n",
            "Starting Epoch 29\n",
            "Train Epoch: 29 [0/14335 (0%)]\tLoss: 3230.211914\tLR: [2.4444444444444445e-05]\n",
            "Train Epoch: 29 [256/14335 (2%)]\tLoss: 2953.993896\tLR: [2.4424603174603175e-05]\n",
            "Train Epoch: 29 [512/14335 (4%)]\tLoss: 2989.510498\tLR: [2.4404761904761906e-05]\n",
            "Train Epoch: 29 [768/14335 (5%)]\tLoss: 2812.566650\tLR: [2.4384920634920636e-05]\n",
            "Train Epoch: 29 [1024/14335 (7%)]\tLoss: 3234.728271\tLR: [2.4365079365079366e-05]\n",
            "Train Epoch: 29 [1280/14335 (9%)]\tLoss: 2879.247803\tLR: [2.4345238095238097e-05]\n",
            "Train Epoch: 29 [1536/14335 (11%)]\tLoss: 3004.811768\tLR: [2.4325396825396827e-05]\n",
            "Train Epoch: 29 [1792/14335 (12%)]\tLoss: 3051.068115\tLR: [2.4305555555555558e-05]\n",
            "Train Epoch: 29 [2048/14335 (14%)]\tLoss: 2745.593994\tLR: [2.4285714285714288e-05]\n",
            "Train Epoch: 29 [2304/14335 (16%)]\tLoss: 3307.737305\tLR: [2.426587301587302e-05]\n",
            "Train Epoch: 29 [2560/14335 (18%)]\tLoss: 3097.864990\tLR: [2.424603174603175e-05]\n",
            "Train Epoch: 29 [2816/14335 (20%)]\tLoss: 3045.747314\tLR: [2.4226190476190476e-05]\n",
            "Train Epoch: 29 [3072/14335 (21%)]\tLoss: 3003.760010\tLR: [2.4206349206349206e-05]\n",
            "Train Epoch: 29 [3328/14335 (23%)]\tLoss: 2970.616211\tLR: [2.4186507936507936e-05]\n",
            "Train Epoch: 29 [3584/14335 (25%)]\tLoss: 2617.043457\tLR: [2.4166666666666667e-05]\n",
            "Train Epoch: 29 [3840/14335 (27%)]\tLoss: 3369.396484\tLR: [2.4146825396825397e-05]\n",
            "Train Epoch: 29 [4096/14335 (29%)]\tLoss: 3040.158447\tLR: [2.4126984126984128e-05]\n",
            "Train Epoch: 29 [4352/14335 (30%)]\tLoss: 2958.036377\tLR: [2.4107142857142858e-05]\n",
            "Train Epoch: 29 [4608/14335 (32%)]\tLoss: 2935.719238\tLR: [2.408730158730159e-05]\n",
            "Train Epoch: 29 [4864/14335 (34%)]\tLoss: 3323.090332\tLR: [2.406746031746032e-05]\n",
            "Train Epoch: 29 [5120/14335 (36%)]\tLoss: 2910.812744\tLR: [2.404761904761905e-05]\n",
            "Train Epoch: 29 [5376/14335 (38%)]\tLoss: 3212.943848\tLR: [2.402777777777778e-05]\n",
            "Train Epoch: 29 [5632/14335 (39%)]\tLoss: 2968.605469\tLR: [2.400793650793651e-05]\n",
            "Train Epoch: 29 [5888/14335 (41%)]\tLoss: 2989.128662\tLR: [2.398809523809524e-05]\n",
            "Train Epoch: 29 [6144/14335 (43%)]\tLoss: 3007.074707\tLR: [2.396825396825397e-05]\n",
            "Train Epoch: 29 [6400/14335 (45%)]\tLoss: 3044.483398\tLR: [2.39484126984127e-05]\n",
            "Train Epoch: 29 [6656/14335 (46%)]\tLoss: 3520.875000\tLR: [2.392857142857143e-05]\n",
            "Train Epoch: 29 [6912/14335 (48%)]\tLoss: 2679.860840\tLR: [2.390873015873016e-05]\n",
            "Train Epoch: 29 [7168/14335 (50%)]\tLoss: 2736.435303\tLR: [2.3888888888888892e-05]\n",
            "Train Epoch: 29 [7424/14335 (52%)]\tLoss: 2993.285889\tLR: [2.3869047619047622e-05]\n",
            "Train Epoch: 29 [7680/14335 (54%)]\tLoss: 2994.450684\tLR: [2.3849206349206353e-05]\n",
            "Train Epoch: 29 [7936/14335 (55%)]\tLoss: 2795.986328\tLR: [2.3829365079365083e-05]\n",
            "Train Epoch: 29 [8192/14335 (57%)]\tLoss: 3101.166260\tLR: [2.380952380952381e-05]\n",
            "Train Epoch: 29 [8448/14335 (59%)]\tLoss: 3241.069580\tLR: [2.378968253968254e-05]\n",
            "Train Epoch: 29 [8704/14335 (61%)]\tLoss: 2751.715820\tLR: [2.376984126984127e-05]\n",
            "Train Epoch: 29 [8960/14335 (62%)]\tLoss: 2715.197998\tLR: [2.375e-05]\n",
            "Train Epoch: 29 [9216/14335 (64%)]\tLoss: 2945.860840\tLR: [2.373015873015873e-05]\n",
            "Train Epoch: 29 [9472/14335 (66%)]\tLoss: 3016.843994\tLR: [2.3710317460317462e-05]\n",
            "Train Epoch: 29 [9728/14335 (68%)]\tLoss: 2897.804688\tLR: [2.369047619047619e-05]\n",
            "Train Epoch: 29 [9984/14335 (70%)]\tLoss: 2875.236084\tLR: [2.367063492063492e-05]\n",
            "Train Epoch: 29 [10240/14335 (71%)]\tLoss: 3098.845459\tLR: [2.365079365079365e-05]\n",
            "Train Epoch: 29 [10496/14335 (73%)]\tLoss: 3098.480225\tLR: [2.363095238095238e-05]\n",
            "Train Epoch: 29 [10752/14335 (75%)]\tLoss: 3365.131592\tLR: [2.361111111111111e-05]\n",
            "Train Epoch: 29 [11008/14335 (77%)]\tLoss: 3173.615723\tLR: [2.359126984126984e-05]\n",
            "Train Epoch: 29 [11264/14335 (79%)]\tLoss: 3033.185059\tLR: [2.357142857142857e-05]\n",
            "Train Epoch: 29 [11520/14335 (80%)]\tLoss: 2919.955322\tLR: [2.35515873015873e-05]\n",
            "Train Epoch: 29 [11776/14335 (82%)]\tLoss: 2735.782471\tLR: [2.3531746031746032e-05]\n",
            "Train Epoch: 29 [12032/14335 (84%)]\tLoss: 3032.859863\tLR: [2.3511904761904762e-05]\n",
            "Train Epoch: 29 [12288/14335 (86%)]\tLoss: 2752.287109\tLR: [2.3492063492063493e-05]\n",
            "Train Epoch: 29 [12544/14335 (88%)]\tLoss: 2729.979980\tLR: [2.3472222222222223e-05]\n",
            "Train Epoch: 29 [12800/14335 (89%)]\tLoss: 2972.704834\tLR: [2.3452380952380954e-05]\n",
            "Train Epoch: 29 [13056/14335 (91%)]\tLoss: 2696.348145\tLR: [2.3432539682539684e-05]\n",
            "Train Epoch: 29 [13312/14335 (93%)]\tLoss: 3060.503418\tLR: [2.3412698412698414e-05]\n",
            "Train Epoch: 29 [13568/14335 (95%)]\tLoss: 2753.629150\tLR: [2.3392857142857145e-05]\n",
            "Train Epoch: 29 [13824/14335 (96%)]\tLoss: 2924.863037\tLR: [2.3373015873015875e-05]\n",
            "Train Epoch: 29 [14025/14335 (98%)]\tLoss: 3253.160156\tLR: [2.3353174603174605e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 29 is 0.9444444444444446, Best ROC Score is:0.9615036231884058\n",
            "Starting Epoch 30\n",
            "Train Epoch: 30 [0/14335 (0%)]\tLoss: 2926.232910\tLR: [2.3333333333333336e-05]\n",
            "Train Epoch: 30 [256/14335 (2%)]\tLoss: 3184.680664\tLR: [2.3313492063492066e-05]\n",
            "Train Epoch: 30 [512/14335 (4%)]\tLoss: 2658.789062\tLR: [2.3293650793650797e-05]\n",
            "Train Epoch: 30 [768/14335 (5%)]\tLoss: 3022.177490\tLR: [2.3273809523809527e-05]\n",
            "Train Epoch: 30 [1024/14335 (7%)]\tLoss: 3148.510986\tLR: [2.3253968253968257e-05]\n",
            "Train Epoch: 30 [1280/14335 (9%)]\tLoss: 2811.779053\tLR: [2.3234126984126984e-05]\n",
            "Train Epoch: 30 [1536/14335 (11%)]\tLoss: 3351.701416\tLR: [2.3214285714285715e-05]\n",
            "Train Epoch: 30 [1792/14335 (12%)]\tLoss: 3148.122803\tLR: [2.3194444444444445e-05]\n",
            "Train Epoch: 30 [2048/14335 (14%)]\tLoss: 3166.433838\tLR: [2.3174603174603175e-05]\n",
            "Train Epoch: 30 [2304/14335 (16%)]\tLoss: 2773.334961\tLR: [2.3154761904761906e-05]\n",
            "Train Epoch: 30 [2560/14335 (18%)]\tLoss: 2642.940186\tLR: [2.3134920634920636e-05]\n",
            "Train Epoch: 30 [2816/14335 (20%)]\tLoss: 3178.585693\tLR: [2.3115079365079367e-05]\n",
            "Train Epoch: 30 [3072/14335 (21%)]\tLoss: 2931.207764\tLR: [2.3095238095238097e-05]\n",
            "Train Epoch: 30 [3328/14335 (23%)]\tLoss: 2566.090332\tLR: [2.3075396825396827e-05]\n",
            "Train Epoch: 30 [3584/14335 (25%)]\tLoss: 2935.001221\tLR: [2.3055555555555558e-05]\n",
            "Train Epoch: 30 [3840/14335 (27%)]\tLoss: 2725.372559\tLR: [2.3035714285714285e-05]\n",
            "Train Epoch: 30 [4096/14335 (29%)]\tLoss: 2800.673584\tLR: [2.3015873015873015e-05]\n",
            "Train Epoch: 30 [4352/14335 (30%)]\tLoss: 3288.095947\tLR: [2.2996031746031745e-05]\n",
            "Train Epoch: 30 [4608/14335 (32%)]\tLoss: 3602.397461\tLR: [2.2976190476190476e-05]\n",
            "Train Epoch: 30 [4864/14335 (34%)]\tLoss: 2942.718018\tLR: [2.2956349206349206e-05]\n",
            "Train Epoch: 30 [5120/14335 (36%)]\tLoss: 2838.820801\tLR: [2.2936507936507937e-05]\n",
            "Train Epoch: 30 [5376/14335 (38%)]\tLoss: 2804.931641\tLR: [2.2916666666666667e-05]\n",
            "Train Epoch: 30 [5632/14335 (39%)]\tLoss: 3020.522461\tLR: [2.2896825396825397e-05]\n",
            "Train Epoch: 30 [5888/14335 (41%)]\tLoss: 3169.275391\tLR: [2.2876984126984128e-05]\n",
            "Train Epoch: 30 [6144/14335 (43%)]\tLoss: 3105.264893\tLR: [2.2857142857142858e-05]\n",
            "Train Epoch: 30 [6400/14335 (45%)]\tLoss: 2764.548096\tLR: [2.283730158730159e-05]\n",
            "Train Epoch: 30 [6656/14335 (46%)]\tLoss: 3419.496826\tLR: [2.281746031746032e-05]\n",
            "Train Epoch: 30 [6912/14335 (48%)]\tLoss: 2692.979248\tLR: [2.279761904761905e-05]\n",
            "Train Epoch: 30 [7168/14335 (50%)]\tLoss: 2625.357910\tLR: [2.277777777777778e-05]\n",
            "Train Epoch: 30 [7424/14335 (52%)]\tLoss: 2808.238525\tLR: [2.275793650793651e-05]\n",
            "Train Epoch: 30 [7680/14335 (54%)]\tLoss: 2799.655029\tLR: [2.273809523809524e-05]\n",
            "Train Epoch: 30 [7936/14335 (55%)]\tLoss: 3221.653809\tLR: [2.271825396825397e-05]\n",
            "Train Epoch: 30 [8192/14335 (57%)]\tLoss: 3103.002441\tLR: [2.2698412698412698e-05]\n",
            "Train Epoch: 30 [8448/14335 (59%)]\tLoss: 2938.947021\tLR: [2.2678571428571428e-05]\n",
            "Train Epoch: 30 [8704/14335 (61%)]\tLoss: 3095.154297\tLR: [2.265873015873016e-05]\n",
            "Train Epoch: 30 [8960/14335 (62%)]\tLoss: 2997.155518\tLR: [2.263888888888889e-05]\n",
            "Train Epoch: 30 [9216/14335 (64%)]\tLoss: 2860.673828\tLR: [2.261904761904762e-05]\n",
            "Train Epoch: 30 [9472/14335 (66%)]\tLoss: 3501.185791\tLR: [2.259920634920635e-05]\n",
            "Train Epoch: 30 [9728/14335 (68%)]\tLoss: 3275.012939\tLR: [2.257936507936508e-05]\n",
            "Train Epoch: 30 [9984/14335 (70%)]\tLoss: 3199.839111\tLR: [2.255952380952381e-05]\n",
            "Train Epoch: 30 [10240/14335 (71%)]\tLoss: 3106.143799\tLR: [2.253968253968254e-05]\n",
            "Train Epoch: 30 [10496/14335 (73%)]\tLoss: 3042.187256\tLR: [2.251984126984127e-05]\n",
            "Train Epoch: 30 [10752/14335 (75%)]\tLoss: 2669.720459\tLR: [2.25e-05]\n",
            "Train Epoch: 30 [11008/14335 (77%)]\tLoss: 3038.365967\tLR: [2.2480158730158732e-05]\n",
            "Train Epoch: 30 [11264/14335 (79%)]\tLoss: 2582.600830\tLR: [2.2460317460317462e-05]\n",
            "Train Epoch: 30 [11520/14335 (80%)]\tLoss: 3124.896484\tLR: [2.2440476190476193e-05]\n",
            "Train Epoch: 30 [11776/14335 (82%)]\tLoss: 2648.412842\tLR: [2.2420634920634923e-05]\n",
            "Train Epoch: 30 [12032/14335 (84%)]\tLoss: 2905.498291\tLR: [2.2400793650793653e-05]\n",
            "Train Epoch: 30 [12288/14335 (86%)]\tLoss: 3041.602051\tLR: [2.2380952380952384e-05]\n",
            "Train Epoch: 30 [12544/14335 (88%)]\tLoss: 2968.300293\tLR: [2.2361111111111114e-05]\n",
            "Train Epoch: 30 [12800/14335 (89%)]\tLoss: 2696.855469\tLR: [2.2341269841269844e-05]\n",
            "Train Epoch: 30 [13056/14335 (91%)]\tLoss: 3018.507324\tLR: [2.2321428571428575e-05]\n",
            "Train Epoch: 30 [13312/14335 (93%)]\tLoss: 3487.424805\tLR: [2.2301587301587305e-05]\n",
            "Train Epoch: 30 [13568/14335 (95%)]\tLoss: 2967.603516\tLR: [2.2281746031746036e-05]\n",
            "Train Epoch: 30 [13824/14335 (96%)]\tLoss: 3289.462891\tLR: [2.2261904761904763e-05]\n",
            "Train Epoch: 30 [14025/14335 (98%)]\tLoss: 3093.690186\tLR: [2.2242063492063493e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 30 is 0.8894324853228962, Best ROC Score is:0.9615036231884058\n",
            "Starting Epoch 31\n",
            "Train Epoch: 31 [0/14335 (0%)]\tLoss: 2746.256836\tLR: [2.2222222222222223e-05]\n",
            "Train Epoch: 31 [256/14335 (2%)]\tLoss: 2551.254150\tLR: [2.2202380952380954e-05]\n",
            "Train Epoch: 31 [512/14335 (4%)]\tLoss: 2698.290771\tLR: [2.2182539682539684e-05]\n",
            "Train Epoch: 31 [768/14335 (5%)]\tLoss: 2772.506592\tLR: [2.2162698412698414e-05]\n",
            "Train Epoch: 31 [1024/14335 (7%)]\tLoss: 3171.030518\tLR: [2.214285714285714e-05]\n",
            "Train Epoch: 31 [1280/14335 (9%)]\tLoss: 3158.683105\tLR: [2.2123015873015872e-05]\n",
            "Train Epoch: 31 [1536/14335 (11%)]\tLoss: 2753.183350\tLR: [2.2103174603174602e-05]\n",
            "Train Epoch: 31 [1792/14335 (12%)]\tLoss: 3360.022705\tLR: [2.2083333333333333e-05]\n",
            "Train Epoch: 31 [2048/14335 (14%)]\tLoss: 2636.213135\tLR: [2.2063492063492063e-05]\n",
            "Train Epoch: 31 [2304/14335 (16%)]\tLoss: 3145.638428\tLR: [2.2043650793650793e-05]\n",
            "Train Epoch: 31 [2560/14335 (18%)]\tLoss: 3086.860352\tLR: [2.2023809523809524e-05]\n",
            "Train Epoch: 31 [2816/14335 (20%)]\tLoss: 2914.668945\tLR: [2.2003968253968254e-05]\n",
            "Train Epoch: 31 [3072/14335 (21%)]\tLoss: 3049.721924\tLR: [2.1984126984126984e-05]\n",
            "Train Epoch: 31 [3328/14335 (23%)]\tLoss: 3037.808594\tLR: [2.1964285714285715e-05]\n",
            "Train Epoch: 31 [3584/14335 (25%)]\tLoss: 3210.666260\tLR: [2.1944444444444445e-05]\n",
            "Train Epoch: 31 [3840/14335 (27%)]\tLoss: 2948.736816\tLR: [2.1924603174603176e-05]\n",
            "Train Epoch: 31 [4096/14335 (29%)]\tLoss: 2971.909668\tLR: [2.1904761904761906e-05]\n",
            "Train Epoch: 31 [4352/14335 (30%)]\tLoss: 2785.801270\tLR: [2.1884920634920636e-05]\n",
            "Train Epoch: 31 [4608/14335 (32%)]\tLoss: 3089.592041\tLR: [2.1865079365079367e-05]\n",
            "Train Epoch: 31 [4864/14335 (34%)]\tLoss: 3359.078857\tLR: [2.1845238095238097e-05]\n",
            "Train Epoch: 31 [5120/14335 (36%)]\tLoss: 3192.412842\tLR: [2.1825396825396827e-05]\n",
            "Train Epoch: 31 [5376/14335 (38%)]\tLoss: 3308.159424\tLR: [2.1805555555555558e-05]\n",
            "Train Epoch: 31 [5632/14335 (39%)]\tLoss: 2908.535400\tLR: [2.1785714285714288e-05]\n",
            "Train Epoch: 31 [5888/14335 (41%)]\tLoss: 3363.355713\tLR: [2.176587301587302e-05]\n",
            "Train Epoch: 31 [6144/14335 (43%)]\tLoss: 2851.593994\tLR: [2.174603174603175e-05]\n",
            "Train Epoch: 31 [6400/14335 (45%)]\tLoss: 2784.103027\tLR: [2.172619047619048e-05]\n",
            "Train Epoch: 31 [6656/14335 (46%)]\tLoss: 2952.209717\tLR: [2.170634920634921e-05]\n",
            "Train Epoch: 31 [6912/14335 (48%)]\tLoss: 2592.852783\tLR: [2.1686507936507937e-05]\n",
            "Train Epoch: 31 [7168/14335 (50%)]\tLoss: 2920.298340\tLR: [2.1666666666666667e-05]\n",
            "Train Epoch: 31 [7424/14335 (52%)]\tLoss: 2529.947266\tLR: [2.1646825396825397e-05]\n",
            "Train Epoch: 31 [7680/14335 (54%)]\tLoss: 3148.186523\tLR: [2.1626984126984128e-05]\n",
            "Train Epoch: 31 [7936/14335 (55%)]\tLoss: 3193.224854\tLR: [2.1607142857142858e-05]\n",
            "Train Epoch: 31 [8192/14335 (57%)]\tLoss: 3190.520752\tLR: [2.158730158730159e-05]\n",
            "Train Epoch: 31 [8448/14335 (59%)]\tLoss: 2840.593506\tLR: [2.156746031746032e-05]\n",
            "Train Epoch: 31 [8704/14335 (61%)]\tLoss: 2766.881592\tLR: [2.154761904761905e-05]\n",
            "Train Epoch: 31 [8960/14335 (62%)]\tLoss: 2528.716064\tLR: [2.152777777777778e-05]\n",
            "Train Epoch: 31 [9216/14335 (64%)]\tLoss: 3309.789307\tLR: [2.150793650793651e-05]\n",
            "Train Epoch: 31 [9472/14335 (66%)]\tLoss: 3281.944092\tLR: [2.148809523809524e-05]\n",
            "Train Epoch: 31 [9728/14335 (68%)]\tLoss: 2756.791260\tLR: [2.1468253968253967e-05]\n",
            "Train Epoch: 31 [9984/14335 (70%)]\tLoss: 3402.705322\tLR: [2.1448412698412698e-05]\n",
            "Train Epoch: 31 [10240/14335 (71%)]\tLoss: 3018.078125\tLR: [2.1428571428571428e-05]\n",
            "Train Epoch: 31 [10496/14335 (73%)]\tLoss: 2723.078369\tLR: [2.140873015873016e-05]\n",
            "Train Epoch: 31 [10752/14335 (75%)]\tLoss: 3343.900146\tLR: [2.138888888888889e-05]\n",
            "Train Epoch: 31 [11008/14335 (77%)]\tLoss: 3666.140869\tLR: [2.136904761904762e-05]\n",
            "Train Epoch: 31 [11264/14335 (79%)]\tLoss: 2939.312256\tLR: [2.134920634920635e-05]\n",
            "Train Epoch: 31 [11520/14335 (80%)]\tLoss: 2831.830566\tLR: [2.132936507936508e-05]\n",
            "Train Epoch: 31 [11776/14335 (82%)]\tLoss: 2686.248047\tLR: [2.130952380952381e-05]\n",
            "Train Epoch: 31 [12032/14335 (84%)]\tLoss: 3042.552490\tLR: [2.128968253968254e-05]\n",
            "Train Epoch: 31 [12288/14335 (86%)]\tLoss: 3214.984375\tLR: [2.126984126984127e-05]\n",
            "Train Epoch: 31 [12544/14335 (88%)]\tLoss: 2903.688721\tLR: [2.125e-05]\n",
            "Train Epoch: 31 [12800/14335 (89%)]\tLoss: 3232.681641\tLR: [2.1230158730158732e-05]\n",
            "Train Epoch: 31 [13056/14335 (91%)]\tLoss: 2896.386963\tLR: [2.1210317460317462e-05]\n",
            "Train Epoch: 31 [13312/14335 (93%)]\tLoss: 3337.667236\tLR: [2.1190476190476193e-05]\n",
            "Train Epoch: 31 [13568/14335 (95%)]\tLoss: 2996.905518\tLR: [2.1170634920634923e-05]\n",
            "Train Epoch: 31 [13824/14335 (96%)]\tLoss: 2666.011230\tLR: [2.115079365079365e-05]\n",
            "Train Epoch: 31 [14025/14335 (98%)]\tLoss: 2917.534424\tLR: [2.113095238095238e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 31 is 0.9572769953051643, Best ROC Score is:0.9615036231884058\n",
            "Starting Epoch 32\n",
            "Train Epoch: 32 [0/14335 (0%)]\tLoss: 3309.626465\tLR: [2.111111111111111e-05]\n",
            "Train Epoch: 32 [256/14335 (2%)]\tLoss: 2788.369385\tLR: [2.109126984126984e-05]\n",
            "Train Epoch: 32 [512/14335 (4%)]\tLoss: 3286.513428\tLR: [2.107142857142857e-05]\n",
            "Train Epoch: 32 [768/14335 (5%)]\tLoss: 2708.540039\tLR: [2.1051587301587302e-05]\n",
            "Train Epoch: 32 [1024/14335 (7%)]\tLoss: 2714.519775\tLR: [2.1031746031746032e-05]\n",
            "Train Epoch: 32 [1280/14335 (9%)]\tLoss: 3526.244141\tLR: [2.1011904761904763e-05]\n",
            "Train Epoch: 32 [1536/14335 (11%)]\tLoss: 2872.962891\tLR: [2.0992063492063493e-05]\n",
            "Train Epoch: 32 [1792/14335 (12%)]\tLoss: 2863.113770\tLR: [2.0972222222222223e-05]\n",
            "Train Epoch: 32 [2048/14335 (14%)]\tLoss: 2884.727051\tLR: [2.0952380952380954e-05]\n",
            "Train Epoch: 32 [2304/14335 (16%)]\tLoss: 3426.919922\tLR: [2.0932539682539684e-05]\n",
            "Train Epoch: 32 [2560/14335 (18%)]\tLoss: 2832.291260\tLR: [2.0912698412698415e-05]\n",
            "Train Epoch: 32 [2816/14335 (20%)]\tLoss: 2812.967041\tLR: [2.0892857142857145e-05]\n",
            "Train Epoch: 32 [3072/14335 (21%)]\tLoss: 2715.823242\tLR: [2.0873015873015875e-05]\n",
            "Train Epoch: 32 [3328/14335 (23%)]\tLoss: 2933.846680\tLR: [2.0853174603174606e-05]\n",
            "Train Epoch: 32 [3584/14335 (25%)]\tLoss: 2867.367188\tLR: [2.0833333333333336e-05]\n",
            "Train Epoch: 32 [3840/14335 (27%)]\tLoss: 3268.818848\tLR: [2.0813492063492066e-05]\n",
            "Train Epoch: 32 [4096/14335 (29%)]\tLoss: 2800.036377\tLR: [2.0793650793650797e-05]\n",
            "Train Epoch: 32 [4352/14335 (30%)]\tLoss: 2688.587402\tLR: [2.0773809523809527e-05]\n",
            "Train Epoch: 32 [4608/14335 (32%)]\tLoss: 2928.684326\tLR: [2.0753968253968258e-05]\n",
            "Train Epoch: 32 [4864/14335 (34%)]\tLoss: 3097.296387\tLR: [2.0734126984126988e-05]\n",
            "Train Epoch: 32 [5120/14335 (36%)]\tLoss: 2879.902588\tLR: [2.0714285714285718e-05]\n",
            "Train Epoch: 32 [5376/14335 (38%)]\tLoss: 3166.851318\tLR: [2.0694444444444445e-05]\n",
            "Train Epoch: 32 [5632/14335 (39%)]\tLoss: 2833.322021\tLR: [2.0674603174603176e-05]\n",
            "Train Epoch: 32 [5888/14335 (41%)]\tLoss: 3078.784912\tLR: [2.0654761904761906e-05]\n",
            "Train Epoch: 32 [6144/14335 (43%)]\tLoss: 3100.809814\tLR: [2.0634920634920636e-05]\n",
            "Train Epoch: 32 [6400/14335 (45%)]\tLoss: 2829.001953\tLR: [2.0615079365079363e-05]\n",
            "Train Epoch: 32 [6656/14335 (46%)]\tLoss: 3223.085693\tLR: [2.0595238095238094e-05]\n",
            "Train Epoch: 32 [6912/14335 (48%)]\tLoss: 3382.949219\tLR: [2.0575396825396824e-05]\n",
            "Train Epoch: 32 [7168/14335 (50%)]\tLoss: 2860.979248\tLR: [2.0555555555555555e-05]\n",
            "Train Epoch: 32 [7424/14335 (52%)]\tLoss: 2591.691895\tLR: [2.0535714285714285e-05]\n",
            "Train Epoch: 32 [7680/14335 (54%)]\tLoss: 3054.722412\tLR: [2.0515873015873015e-05]\n",
            "Train Epoch: 32 [7936/14335 (55%)]\tLoss: 3208.279785\tLR: [2.0496031746031746e-05]\n",
            "Train Epoch: 32 [8192/14335 (57%)]\tLoss: 2680.266357\tLR: [2.0476190476190476e-05]\n",
            "Train Epoch: 32 [8448/14335 (59%)]\tLoss: 3039.589600\tLR: [2.0456349206349206e-05]\n",
            "Train Epoch: 32 [8704/14335 (61%)]\tLoss: 2911.131348\tLR: [2.0436507936507937e-05]\n",
            "Train Epoch: 32 [8960/14335 (62%)]\tLoss: 3221.162354\tLR: [2.0416666666666667e-05]\n",
            "Train Epoch: 32 [9216/14335 (64%)]\tLoss: 2619.234375\tLR: [2.0396825396825398e-05]\n",
            "Train Epoch: 32 [9472/14335 (66%)]\tLoss: 3056.072998\tLR: [2.0376984126984128e-05]\n",
            "Train Epoch: 32 [9728/14335 (68%)]\tLoss: 3101.476318\tLR: [2.0357142857142858e-05]\n",
            "Train Epoch: 32 [9984/14335 (70%)]\tLoss: 2658.696289\tLR: [2.033730158730159e-05]\n",
            "Train Epoch: 32 [10240/14335 (71%)]\tLoss: 3044.441650\tLR: [2.031746031746032e-05]\n",
            "Train Epoch: 32 [10496/14335 (73%)]\tLoss: 3086.264648\tLR: [2.029761904761905e-05]\n",
            "Train Epoch: 32 [10752/14335 (75%)]\tLoss: 3041.149170\tLR: [2.027777777777778e-05]\n",
            "Train Epoch: 32 [11008/14335 (77%)]\tLoss: 2735.239014\tLR: [2.025793650793651e-05]\n",
            "Train Epoch: 32 [11264/14335 (79%)]\tLoss: 3107.212158\tLR: [2.023809523809524e-05]\n",
            "Train Epoch: 32 [11520/14335 (80%)]\tLoss: 2999.935303\tLR: [2.021825396825397e-05]\n",
            "Train Epoch: 32 [11776/14335 (82%)]\tLoss: 2939.578613\tLR: [2.01984126984127e-05]\n",
            "Train Epoch: 32 [12032/14335 (84%)]\tLoss: 3553.593750\tLR: [2.017857142857143e-05]\n",
            "Train Epoch: 32 [12288/14335 (86%)]\tLoss: 2910.654785\tLR: [2.015873015873016e-05]\n",
            "Train Epoch: 32 [12544/14335 (88%)]\tLoss: 3534.886963\tLR: [2.013888888888889e-05]\n",
            "Train Epoch: 32 [12800/14335 (89%)]\tLoss: 2796.505127\tLR: [2.011904761904762e-05]\n",
            "Train Epoch: 32 [13056/14335 (91%)]\tLoss: 2963.575195\tLR: [2.009920634920635e-05]\n",
            "Train Epoch: 32 [13312/14335 (93%)]\tLoss: 3028.414795\tLR: [2.007936507936508e-05]\n",
            "Train Epoch: 32 [13568/14335 (95%)]\tLoss: 2762.903809\tLR: [2.005952380952381e-05]\n",
            "Train Epoch: 32 [13824/14335 (96%)]\tLoss: 2973.927002\tLR: [2.003968253968254e-05]\n",
            "Train Epoch: 32 [14025/14335 (98%)]\tLoss: 3105.238770\tLR: [2.001984126984127e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 32 is 0.9523076923076923, Best ROC Score is:0.9615036231884058\n",
            "Starting Epoch 33\n",
            "Train Epoch: 33 [0/14335 (0%)]\tLoss: 3169.921387\tLR: [2e-05]\n",
            "Train Epoch: 33 [256/14335 (2%)]\tLoss: 2923.449463\tLR: [1.9980158730158732e-05]\n",
            "Train Epoch: 33 [512/14335 (4%)]\tLoss: 3125.743408\tLR: [1.9960317460317462e-05]\n",
            "Train Epoch: 33 [768/14335 (5%)]\tLoss: 2884.720947\tLR: [1.9940476190476193e-05]\n",
            "Train Epoch: 33 [1024/14335 (7%)]\tLoss: 3379.423340\tLR: [1.992063492063492e-05]\n",
            "Train Epoch: 33 [1280/14335 (9%)]\tLoss: 2768.097656\tLR: [1.990079365079365e-05]\n",
            "Train Epoch: 33 [1536/14335 (11%)]\tLoss: 2839.374512\tLR: [1.988095238095238e-05]\n",
            "Train Epoch: 33 [1792/14335 (12%)]\tLoss: 3251.015381\tLR: [1.986111111111111e-05]\n",
            "Train Epoch: 33 [2048/14335 (14%)]\tLoss: 3424.521240\tLR: [1.984126984126984e-05]\n",
            "Train Epoch: 33 [2304/14335 (16%)]\tLoss: 2918.785156\tLR: [1.982142857142857e-05]\n",
            "Train Epoch: 33 [2560/14335 (18%)]\tLoss: 3103.766846\tLR: [1.9801587301587302e-05]\n",
            "Train Epoch: 33 [2816/14335 (20%)]\tLoss: 2632.963379\tLR: [1.9781746031746032e-05]\n",
            "Train Epoch: 33 [3072/14335 (21%)]\tLoss: 3343.414307\tLR: [1.9761904761904763e-05]\n",
            "Train Epoch: 33 [3328/14335 (23%)]\tLoss: 3280.227051\tLR: [1.9742063492063493e-05]\n",
            "Train Epoch: 33 [3584/14335 (25%)]\tLoss: 3085.263428\tLR: [1.9722222222222224e-05]\n",
            "Train Epoch: 33 [3840/14335 (27%)]\tLoss: 2842.880127\tLR: [1.9702380952380954e-05]\n",
            "Train Epoch: 33 [4096/14335 (29%)]\tLoss: 3013.428711\tLR: [1.9682539682539684e-05]\n",
            "Train Epoch: 33 [4352/14335 (30%)]\tLoss: 2865.956299\tLR: [1.9662698412698415e-05]\n",
            "Train Epoch: 33 [4608/14335 (32%)]\tLoss: 2669.772949\tLR: [1.9642857142857145e-05]\n",
            "Train Epoch: 33 [4864/14335 (34%)]\tLoss: 3156.279785\tLR: [1.9623015873015872e-05]\n",
            "Train Epoch: 33 [5120/14335 (36%)]\tLoss: 2869.488770\tLR: [1.9603174603174602e-05]\n",
            "Train Epoch: 33 [5376/14335 (38%)]\tLoss: 2646.716553\tLR: [1.9583333333333333e-05]\n",
            "Train Epoch: 33 [5632/14335 (39%)]\tLoss: 2924.038330\tLR: [1.9563492063492063e-05]\n",
            "Train Epoch: 33 [5888/14335 (41%)]\tLoss: 2958.571777\tLR: [1.9543650793650793e-05]\n",
            "Train Epoch: 33 [6144/14335 (43%)]\tLoss: 3281.337402\tLR: [1.9523809523809524e-05]\n",
            "Train Epoch: 33 [6400/14335 (45%)]\tLoss: 2886.381104\tLR: [1.9503968253968254e-05]\n",
            "Train Epoch: 33 [6656/14335 (46%)]\tLoss: 3184.448242\tLR: [1.9484126984126985e-05]\n",
            "Train Epoch: 33 [6912/14335 (48%)]\tLoss: 3003.794678\tLR: [1.9464285714285715e-05]\n",
            "Train Epoch: 33 [7168/14335 (50%)]\tLoss: 2778.192627\tLR: [1.9444444444444445e-05]\n",
            "Train Epoch: 33 [7424/14335 (52%)]\tLoss: 2883.684082\tLR: [1.9424603174603176e-05]\n",
            "Train Epoch: 33 [7680/14335 (54%)]\tLoss: 2802.227539\tLR: [1.9404761904761906e-05]\n",
            "Train Epoch: 33 [7936/14335 (55%)]\tLoss: 3031.858887\tLR: [1.9384920634920637e-05]\n",
            "Train Epoch: 33 [8192/14335 (57%)]\tLoss: 2584.929443\tLR: [1.9365079365079367e-05]\n",
            "Train Epoch: 33 [8448/14335 (59%)]\tLoss: 3101.830322\tLR: [1.9345238095238097e-05]\n",
            "Train Epoch: 33 [8704/14335 (61%)]\tLoss: 2473.311035\tLR: [1.9325396825396828e-05]\n",
            "Train Epoch: 33 [8960/14335 (62%)]\tLoss: 3257.313232\tLR: [1.9305555555555558e-05]\n",
            "Train Epoch: 33 [9216/14335 (64%)]\tLoss: 2923.189453\tLR: [1.928571428571429e-05]\n",
            "Train Epoch: 33 [9472/14335 (66%)]\tLoss: 3038.360596\tLR: [1.926587301587302e-05]\n",
            "Train Epoch: 33 [9728/14335 (68%)]\tLoss: 2896.004883\tLR: [1.924603174603175e-05]\n",
            "Train Epoch: 33 [9984/14335 (70%)]\tLoss: 2852.767090\tLR: [1.922619047619048e-05]\n",
            "Train Epoch: 33 [10240/14335 (71%)]\tLoss: 3382.515381\tLR: [1.920634920634921e-05]\n",
            "Train Epoch: 33 [10496/14335 (73%)]\tLoss: 3383.134521\tLR: [1.918650793650794e-05]\n",
            "Train Epoch: 33 [10752/14335 (75%)]\tLoss: 2837.677979\tLR: [1.9166666666666667e-05]\n",
            "Train Epoch: 33 [11008/14335 (77%)]\tLoss: 3064.949463\tLR: [1.9146825396825398e-05]\n",
            "Train Epoch: 33 [11264/14335 (79%)]\tLoss: 2947.343994\tLR: [1.9126984126984128e-05]\n",
            "Train Epoch: 33 [11520/14335 (80%)]\tLoss: 2563.895508\tLR: [1.910714285714286e-05]\n",
            "Train Epoch: 33 [11776/14335 (82%)]\tLoss: 2857.588379\tLR: [1.9087301587301585e-05]\n",
            "Train Epoch: 33 [12032/14335 (84%)]\tLoss: 2871.049316\tLR: [1.9067460317460316e-05]\n",
            "Train Epoch: 33 [12288/14335 (86%)]\tLoss: 3004.385742\tLR: [1.9047619047619046e-05]\n",
            "Train Epoch: 33 [12544/14335 (88%)]\tLoss: 3303.474854\tLR: [1.9027777777777776e-05]\n",
            "Train Epoch: 33 [12800/14335 (89%)]\tLoss: 3331.460205\tLR: [1.9007936507936507e-05]\n",
            "Train Epoch: 33 [13056/14335 (91%)]\tLoss: 3020.890137\tLR: [1.8988095238095237e-05]\n",
            "Train Epoch: 33 [13312/14335 (93%)]\tLoss: 3050.127441\tLR: [1.8968253968253968e-05]\n",
            "Train Epoch: 33 [13568/14335 (95%)]\tLoss: 3142.010986\tLR: [1.8948412698412698e-05]\n",
            "Train Epoch: 33 [13824/14335 (96%)]\tLoss: 3699.455322\tLR: [1.892857142857143e-05]\n",
            "Train Epoch: 33 [14025/14335 (98%)]\tLoss: 2960.326904\tLR: [1.890873015873016e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "saved model with best validaton in epoch33\n",
            "Evaluation ROC Score in epoch 33 is 0.9757894736842105, Best ROC Score is:0.9757894736842105\n",
            "Starting Epoch 34\n",
            "Train Epoch: 34 [0/14335 (0%)]\tLoss: 3116.400146\tLR: [1.888888888888889e-05]\n",
            "Train Epoch: 34 [256/14335 (2%)]\tLoss: 3058.867920\tLR: [1.886904761904762e-05]\n",
            "Train Epoch: 34 [512/14335 (4%)]\tLoss: 3170.162598\tLR: [1.884920634920635e-05]\n",
            "Train Epoch: 34 [768/14335 (5%)]\tLoss: 2905.994141\tLR: [1.882936507936508e-05]\n",
            "Train Epoch: 34 [1024/14335 (7%)]\tLoss: 2836.733887\tLR: [1.880952380952381e-05]\n",
            "Train Epoch: 34 [1280/14335 (9%)]\tLoss: 2977.404785\tLR: [1.878968253968254e-05]\n",
            "Train Epoch: 34 [1536/14335 (11%)]\tLoss: 3356.905029\tLR: [1.876984126984127e-05]\n",
            "Train Epoch: 34 [1792/14335 (12%)]\tLoss: 3182.028809\tLR: [1.8750000000000002e-05]\n",
            "Train Epoch: 34 [2048/14335 (14%)]\tLoss: 3135.835205\tLR: [1.8730158730158732e-05]\n",
            "Train Epoch: 34 [2304/14335 (16%)]\tLoss: 2949.010010\tLR: [1.8710317460317462e-05]\n",
            "Train Epoch: 34 [2560/14335 (18%)]\tLoss: 2852.431885\tLR: [1.8690476190476193e-05]\n",
            "Train Epoch: 34 [2816/14335 (20%)]\tLoss: 3330.460205\tLR: [1.8670634920634923e-05]\n",
            "Train Epoch: 34 [3072/14335 (21%)]\tLoss: 3183.477539\tLR: [1.8650793650793654e-05]\n",
            "Train Epoch: 34 [3328/14335 (23%)]\tLoss: 2704.840820\tLR: [1.863095238095238e-05]\n",
            "Train Epoch: 34 [3584/14335 (25%)]\tLoss: 3420.366943\tLR: [1.861111111111111e-05]\n",
            "Train Epoch: 34 [3840/14335 (27%)]\tLoss: 3237.720215\tLR: [1.859126984126984e-05]\n",
            "Train Epoch: 34 [4096/14335 (29%)]\tLoss: 3278.812988\tLR: [1.8571428571428572e-05]\n",
            "Train Epoch: 34 [4352/14335 (30%)]\tLoss: 2785.336426\tLR: [1.8551587301587302e-05]\n",
            "Train Epoch: 34 [4608/14335 (32%)]\tLoss: 3034.964355\tLR: [1.8531746031746032e-05]\n",
            "Train Epoch: 34 [4864/14335 (34%)]\tLoss: 3168.198975\tLR: [1.8511904761904763e-05]\n",
            "Train Epoch: 34 [5120/14335 (36%)]\tLoss: 2906.467529\tLR: [1.8492063492063493e-05]\n",
            "Train Epoch: 34 [5376/14335 (38%)]\tLoss: 2905.324219\tLR: [1.8472222222222224e-05]\n",
            "Train Epoch: 34 [5632/14335 (39%)]\tLoss: 2935.995850\tLR: [1.8452380952380954e-05]\n",
            "Train Epoch: 34 [5888/14335 (41%)]\tLoss: 2794.860352\tLR: [1.8432539682539684e-05]\n",
            "Train Epoch: 34 [6144/14335 (43%)]\tLoss: 3188.993408\tLR: [1.8412698412698415e-05]\n",
            "Train Epoch: 34 [6400/14335 (45%)]\tLoss: 3160.736816\tLR: [1.8392857142857145e-05]\n",
            "Train Epoch: 34 [6656/14335 (46%)]\tLoss: 2803.515869\tLR: [1.8373015873015875e-05]\n",
            "Train Epoch: 34 [6912/14335 (48%)]\tLoss: 2872.918213\tLR: [1.8353174603174602e-05]\n",
            "Train Epoch: 34 [7168/14335 (50%)]\tLoss: 2653.990723\tLR: [1.8333333333333333e-05]\n",
            "Train Epoch: 34 [7424/14335 (52%)]\tLoss: 3100.716797\tLR: [1.8313492063492063e-05]\n",
            "Train Epoch: 34 [7680/14335 (54%)]\tLoss: 3025.818848\tLR: [1.8293650793650794e-05]\n",
            "Train Epoch: 34 [7936/14335 (55%)]\tLoss: 3148.468750\tLR: [1.8273809523809524e-05]\n",
            "Train Epoch: 34 [8192/14335 (57%)]\tLoss: 2788.597412\tLR: [1.8253968253968254e-05]\n",
            "Train Epoch: 34 [8448/14335 (59%)]\tLoss: 2992.722656\tLR: [1.8234126984126985e-05]\n",
            "Train Epoch: 34 [8704/14335 (61%)]\tLoss: 3099.062500\tLR: [1.8214285714285715e-05]\n",
            "Train Epoch: 34 [8960/14335 (62%)]\tLoss: 2368.541748\tLR: [1.8194444444444445e-05]\n",
            "Train Epoch: 34 [9216/14335 (64%)]\tLoss: 2960.790283\tLR: [1.8174603174603176e-05]\n",
            "Train Epoch: 34 [9472/14335 (66%)]\tLoss: 3026.400635\tLR: [1.8154761904761906e-05]\n",
            "Train Epoch: 34 [9728/14335 (68%)]\tLoss: 2704.591797\tLR: [1.8134920634920637e-05]\n",
            "Train Epoch: 34 [9984/14335 (70%)]\tLoss: 3002.515869\tLR: [1.8115079365079367e-05]\n",
            "Train Epoch: 34 [10240/14335 (71%)]\tLoss: 2766.294189\tLR: [1.8095238095238094e-05]\n",
            "Train Epoch: 34 [10496/14335 (73%)]\tLoss: 3118.595703\tLR: [1.8075396825396824e-05]\n",
            "Train Epoch: 34 [10752/14335 (75%)]\tLoss: 3079.376221\tLR: [1.8055555555555555e-05]\n",
            "Train Epoch: 34 [11008/14335 (77%)]\tLoss: 2798.623047\tLR: [1.8035714285714285e-05]\n",
            "Train Epoch: 34 [11264/14335 (79%)]\tLoss: 2963.663330\tLR: [1.8015873015873015e-05]\n",
            "Train Epoch: 34 [11520/14335 (80%)]\tLoss: 3247.117188\tLR: [1.7996031746031746e-05]\n",
            "Train Epoch: 34 [11776/14335 (82%)]\tLoss: 3145.700928\tLR: [1.7976190476190476e-05]\n",
            "Train Epoch: 34 [12032/14335 (84%)]\tLoss: 3596.374023\tLR: [1.7956349206349207e-05]\n",
            "Train Epoch: 34 [12288/14335 (86%)]\tLoss: 3046.795166\tLR: [1.7936507936507937e-05]\n",
            "Train Epoch: 34 [12544/14335 (88%)]\tLoss: 3048.219727\tLR: [1.7916666666666667e-05]\n",
            "Train Epoch: 34 [12800/14335 (89%)]\tLoss: 2765.730713\tLR: [1.7896825396825398e-05]\n",
            "Train Epoch: 34 [13056/14335 (91%)]\tLoss: 2892.339355\tLR: [1.7876984126984128e-05]\n",
            "Train Epoch: 34 [13312/14335 (93%)]\tLoss: 2869.593018\tLR: [1.785714285714286e-05]\n",
            "Train Epoch: 34 [13568/14335 (95%)]\tLoss: 3007.775879\tLR: [1.783730158730159e-05]\n",
            "Train Epoch: 34 [13824/14335 (96%)]\tLoss: 2646.299805\tLR: [1.781746031746032e-05]\n",
            "Train Epoch: 34 [14025/14335 (98%)]\tLoss: 3015.201172\tLR: [1.779761904761905e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 34 is 0.9403080872913993, Best ROC Score is:0.9757894736842105\n",
            "Starting Epoch 35\n",
            "Train Epoch: 35 [0/14335 (0%)]\tLoss: 3137.223877\tLR: [1.777777777777778e-05]\n",
            "Train Epoch: 35 [256/14335 (2%)]\tLoss: 2856.751465\tLR: [1.775793650793651e-05]\n",
            "Train Epoch: 35 [512/14335 (4%)]\tLoss: 3065.761963\tLR: [1.773809523809524e-05]\n",
            "Train Epoch: 35 [768/14335 (5%)]\tLoss: 3042.739990\tLR: [1.771825396825397e-05]\n",
            "Train Epoch: 35 [1024/14335 (7%)]\tLoss: 3330.788086\tLR: [1.76984126984127e-05]\n",
            "Train Epoch: 35 [1280/14335 (9%)]\tLoss: 3395.269531\tLR: [1.7678571428571432e-05]\n",
            "Train Epoch: 35 [1536/14335 (11%)]\tLoss: 2716.893066\tLR: [1.7658730158730162e-05]\n",
            "Train Epoch: 35 [1792/14335 (12%)]\tLoss: 3016.962891\tLR: [1.763888888888889e-05]\n",
            "Train Epoch: 35 [2048/14335 (14%)]\tLoss: 2714.344482\tLR: [1.761904761904762e-05]\n",
            "Train Epoch: 35 [2304/14335 (16%)]\tLoss: 3224.370361\tLR: [1.759920634920635e-05]\n",
            "Train Epoch: 35 [2560/14335 (18%)]\tLoss: 2815.905273\tLR: [1.757936507936508e-05]\n",
            "Train Epoch: 35 [2816/14335 (20%)]\tLoss: 3307.880127\tLR: [1.755952380952381e-05]\n",
            "Train Epoch: 35 [3072/14335 (21%)]\tLoss: 3137.628174\tLR: [1.7539682539682538e-05]\n",
            "Train Epoch: 35 [3328/14335 (23%)]\tLoss: 3152.990967\tLR: [1.7519841269841268e-05]\n",
            "Train Epoch: 35 [3584/14335 (25%)]\tLoss: 2931.575928\tLR: [1.75e-05]\n",
            "Train Epoch: 35 [3840/14335 (27%)]\tLoss: 2888.887451\tLR: [1.748015873015873e-05]\n",
            "Train Epoch: 35 [4096/14335 (29%)]\tLoss: 2997.392578\tLR: [1.746031746031746e-05]\n",
            "Train Epoch: 35 [4352/14335 (30%)]\tLoss: 2874.691650\tLR: [1.744047619047619e-05]\n",
            "Train Epoch: 35 [4608/14335 (32%)]\tLoss: 3101.058350\tLR: [1.742063492063492e-05]\n",
            "Train Epoch: 35 [4864/14335 (34%)]\tLoss: 3265.269287\tLR: [1.740079365079365e-05]\n",
            "Train Epoch: 35 [5120/14335 (36%)]\tLoss: 3166.550537\tLR: [1.738095238095238e-05]\n",
            "Train Epoch: 35 [5376/14335 (38%)]\tLoss: 3399.673096\tLR: [1.736111111111111e-05]\n",
            "Train Epoch: 35 [5632/14335 (39%)]\tLoss: 2750.537842\tLR: [1.734126984126984e-05]\n",
            "Train Epoch: 35 [5888/14335 (41%)]\tLoss: 3147.325928\tLR: [1.7321428571428572e-05]\n",
            "Train Epoch: 35 [6144/14335 (43%)]\tLoss: 2747.836670\tLR: [1.7301587301587302e-05]\n",
            "Train Epoch: 35 [6400/14335 (45%)]\tLoss: 2812.060303\tLR: [1.7281746031746033e-05]\n",
            "Train Epoch: 35 [6656/14335 (46%)]\tLoss: 2803.937256\tLR: [1.7261904761904763e-05]\n",
            "Train Epoch: 35 [6912/14335 (48%)]\tLoss: 2944.684326\tLR: [1.7242063492063493e-05]\n",
            "Train Epoch: 35 [7168/14335 (50%)]\tLoss: 3182.427002\tLR: [1.7222222222222224e-05]\n",
            "Train Epoch: 35 [7424/14335 (52%)]\tLoss: 3124.399902\tLR: [1.7202380952380954e-05]\n",
            "Train Epoch: 35 [7680/14335 (54%)]\tLoss: 3247.634033\tLR: [1.7182539682539684e-05]\n",
            "Train Epoch: 35 [7936/14335 (55%)]\tLoss: 3023.243652\tLR: [1.7162698412698415e-05]\n",
            "Train Epoch: 35 [8192/14335 (57%)]\tLoss: 3079.382080\tLR: [1.7142857142857145e-05]\n",
            "Train Epoch: 35 [8448/14335 (59%)]\tLoss: 2699.786621\tLR: [1.7123015873015876e-05]\n",
            "Train Epoch: 35 [8704/14335 (61%)]\tLoss: 2916.985352\tLR: [1.7103174603174606e-05]\n",
            "Train Epoch: 35 [8960/14335 (62%)]\tLoss: 2991.191895\tLR: [1.7083333333333333e-05]\n",
            "Train Epoch: 35 [9216/14335 (64%)]\tLoss: 2741.219971\tLR: [1.7063492063492063e-05]\n",
            "Train Epoch: 35 [9472/14335 (66%)]\tLoss: 2923.393555\tLR: [1.7043650793650794e-05]\n",
            "Train Epoch: 35 [9728/14335 (68%)]\tLoss: 2705.507324\tLR: [1.7023809523809524e-05]\n",
            "Train Epoch: 35 [9984/14335 (70%)]\tLoss: 3084.370361\tLR: [1.7003968253968254e-05]\n",
            "Train Epoch: 35 [10240/14335 (71%)]\tLoss: 2836.236572\tLR: [1.6984126984126985e-05]\n",
            "Train Epoch: 35 [10496/14335 (73%)]\tLoss: 3116.704346\tLR: [1.6964285714285715e-05]\n",
            "Train Epoch: 35 [10752/14335 (75%)]\tLoss: 3192.039551\tLR: [1.6944444444444446e-05]\n",
            "Train Epoch: 35 [11008/14335 (77%)]\tLoss: 3113.523438\tLR: [1.6924603174603176e-05]\n",
            "Train Epoch: 35 [11264/14335 (79%)]\tLoss: 2909.379883\tLR: [1.6904761904761906e-05]\n",
            "Train Epoch: 35 [11520/14335 (80%)]\tLoss: 3095.653809\tLR: [1.6884920634920637e-05]\n",
            "Train Epoch: 35 [11776/14335 (82%)]\tLoss: 3106.481689\tLR: [1.6865079365079367e-05]\n",
            "Train Epoch: 35 [12032/14335 (84%)]\tLoss: 2840.518799\tLR: [1.6845238095238097e-05]\n",
            "Train Epoch: 35 [12288/14335 (86%)]\tLoss: 2737.293701\tLR: [1.6825396825396828e-05]\n",
            "Train Epoch: 35 [12544/14335 (88%)]\tLoss: 2749.466553\tLR: [1.6805555555555558e-05]\n",
            "Train Epoch: 35 [12800/14335 (89%)]\tLoss: 2610.471436\tLR: [1.6785714285714285e-05]\n",
            "Train Epoch: 35 [13056/14335 (91%)]\tLoss: 2965.850342\tLR: [1.6765873015873016e-05]\n",
            "Train Epoch: 35 [13312/14335 (93%)]\tLoss: 3276.178467\tLR: [1.6746031746031746e-05]\n",
            "Train Epoch: 35 [13568/14335 (95%)]\tLoss: 3144.976318\tLR: [1.6726190476190476e-05]\n",
            "Train Epoch: 35 [13824/14335 (96%)]\tLoss: 3080.242432\tLR: [1.6706349206349207e-05]\n",
            "Train Epoch: 35 [14025/14335 (98%)]\tLoss: 2818.730225\tLR: [1.6686507936507937e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 35 is 0.9247491638795987, Best ROC Score is:0.9757894736842105\n",
            "Starting Epoch 36\n",
            "Train Epoch: 36 [0/14335 (0%)]\tLoss: 2676.434570\tLR: [1.6666666666666667e-05]\n",
            "Train Epoch: 36 [256/14335 (2%)]\tLoss: 2741.030029\tLR: [1.6646825396825398e-05]\n",
            "Train Epoch: 36 [512/14335 (4%)]\tLoss: 3139.798584\tLR: [1.6626984126984128e-05]\n",
            "Train Epoch: 36 [768/14335 (5%)]\tLoss: 3117.761475\tLR: [1.660714285714286e-05]\n",
            "Train Epoch: 36 [1024/14335 (7%)]\tLoss: 3034.671143\tLR: [1.658730158730159e-05]\n",
            "Train Epoch: 36 [1280/14335 (9%)]\tLoss: 3185.046143\tLR: [1.656746031746032e-05]\n",
            "Train Epoch: 36 [1536/14335 (11%)]\tLoss: 2903.999512\tLR: [1.6547619047619046e-05]\n",
            "Train Epoch: 36 [1792/14335 (12%)]\tLoss: 3133.226318\tLR: [1.6527777777777777e-05]\n",
            "Train Epoch: 36 [2048/14335 (14%)]\tLoss: 2838.005859\tLR: [1.6507936507936507e-05]\n",
            "Train Epoch: 36 [2304/14335 (16%)]\tLoss: 3156.407471\tLR: [1.6488095238095237e-05]\n",
            "Train Epoch: 36 [2560/14335 (18%)]\tLoss: 3200.866943\tLR: [1.6468253968253968e-05]\n",
            "Train Epoch: 36 [2816/14335 (20%)]\tLoss: 2821.774902\tLR: [1.6448412698412698e-05]\n",
            "Train Epoch: 36 [3072/14335 (21%)]\tLoss: 2710.904297\tLR: [1.642857142857143e-05]\n",
            "Train Epoch: 36 [3328/14335 (23%)]\tLoss: 2904.594238\tLR: [1.640873015873016e-05]\n",
            "Train Epoch: 36 [3584/14335 (25%)]\tLoss: 2456.617676\tLR: [1.638888888888889e-05]\n",
            "Train Epoch: 36 [3840/14335 (27%)]\tLoss: 2771.311035\tLR: [1.636904761904762e-05]\n",
            "Train Epoch: 36 [4096/14335 (29%)]\tLoss: 3477.562988\tLR: [1.634920634920635e-05]\n",
            "Train Epoch: 36 [4352/14335 (30%)]\tLoss: 3019.641846\tLR: [1.632936507936508e-05]\n",
            "Train Epoch: 36 [4608/14335 (32%)]\tLoss: 3081.228516\tLR: [1.630952380952381e-05]\n",
            "Train Epoch: 36 [4864/14335 (34%)]\tLoss: 2983.862549\tLR: [1.628968253968254e-05]\n",
            "Train Epoch: 36 [5120/14335 (36%)]\tLoss: 3172.408691\tLR: [1.626984126984127e-05]\n",
            "Train Epoch: 36 [5376/14335 (38%)]\tLoss: 2993.444336\tLR: [1.6250000000000002e-05]\n",
            "Train Epoch: 36 [5632/14335 (39%)]\tLoss: 2871.972656\tLR: [1.6230158730158732e-05]\n",
            "Train Epoch: 36 [5888/14335 (41%)]\tLoss: 3424.023438\tLR: [1.6210317460317463e-05]\n",
            "Train Epoch: 36 [6144/14335 (43%)]\tLoss: 2969.614502\tLR: [1.6190476190476193e-05]\n",
            "Train Epoch: 36 [6400/14335 (45%)]\tLoss: 2727.427490\tLR: [1.6170634920634923e-05]\n",
            "Train Epoch: 36 [6656/14335 (46%)]\tLoss: 3051.678223\tLR: [1.6150793650793654e-05]\n",
            "Train Epoch: 36 [6912/14335 (48%)]\tLoss: 2953.552002\tLR: [1.6130952380952384e-05]\n",
            "Train Epoch: 36 [7168/14335 (50%)]\tLoss: 3181.984131\tLR: [1.6111111111111115e-05]\n",
            "Train Epoch: 36 [7424/14335 (52%)]\tLoss: 2680.214844\tLR: [1.609126984126984e-05]\n",
            "Train Epoch: 36 [7680/14335 (54%)]\tLoss: 2822.332520\tLR: [1.6071428571428572e-05]\n",
            "Train Epoch: 36 [7936/14335 (55%)]\tLoss: 2637.937744\tLR: [1.6051587301587302e-05]\n",
            "Train Epoch: 36 [8192/14335 (57%)]\tLoss: 3163.955566\tLR: [1.6031746031746033e-05]\n",
            "Train Epoch: 36 [8448/14335 (59%)]\tLoss: 2961.275391\tLR: [1.601190476190476e-05]\n",
            "Train Epoch: 36 [8704/14335 (61%)]\tLoss: 3168.761719\tLR: [1.599206349206349e-05]\n",
            "Train Epoch: 36 [8960/14335 (62%)]\tLoss: 2988.255371\tLR: [1.597222222222222e-05]\n",
            "Train Epoch: 36 [9216/14335 (64%)]\tLoss: 2838.999512\tLR: [1.595238095238095e-05]\n",
            "Train Epoch: 36 [9472/14335 (66%)]\tLoss: 3184.987549\tLR: [1.593253968253968e-05]\n",
            "Train Epoch: 36 [9728/14335 (68%)]\tLoss: 2848.359863\tLR: [1.591269841269841e-05]\n",
            "Train Epoch: 36 [9984/14335 (70%)]\tLoss: 2831.563477\tLR: [1.5892857142857142e-05]\n",
            "Train Epoch: 36 [10240/14335 (71%)]\tLoss: 3052.002441\tLR: [1.5873015873015872e-05]\n",
            "Train Epoch: 36 [10496/14335 (73%)]\tLoss: 3421.653564\tLR: [1.5853174603174603e-05]\n",
            "Train Epoch: 36 [10752/14335 (75%)]\tLoss: 3118.072021\tLR: [1.5833333333333333e-05]\n",
            "Train Epoch: 36 [11008/14335 (77%)]\tLoss: 2971.920898\tLR: [1.5813492063492063e-05]\n",
            "Train Epoch: 36 [11264/14335 (79%)]\tLoss: 2765.780518\tLR: [1.5793650793650794e-05]\n",
            "Train Epoch: 36 [11520/14335 (80%)]\tLoss: 2879.297363\tLR: [1.5773809523809524e-05]\n",
            "Train Epoch: 36 [11776/14335 (82%)]\tLoss: 3083.102539\tLR: [1.5753968253968255e-05]\n",
            "Train Epoch: 36 [12032/14335 (84%)]\tLoss: 3536.655518\tLR: [1.5734126984126985e-05]\n",
            "Train Epoch: 36 [12288/14335 (86%)]\tLoss: 3176.528564\tLR: [1.5714285714285715e-05]\n",
            "Train Epoch: 36 [12544/14335 (88%)]\tLoss: 3070.111816\tLR: [1.5694444444444446e-05]\n",
            "Train Epoch: 36 [12800/14335 (89%)]\tLoss: 3128.436768\tLR: [1.5674603174603176e-05]\n",
            "Train Epoch: 36 [13056/14335 (91%)]\tLoss: 3096.271240\tLR: [1.5654761904761906e-05]\n",
            "Train Epoch: 36 [13312/14335 (93%)]\tLoss: 3179.395020\tLR: [1.5634920634920637e-05]\n",
            "Train Epoch: 36 [13568/14335 (95%)]\tLoss: 3045.417480\tLR: [1.5615079365079367e-05]\n",
            "Train Epoch: 36 [13824/14335 (96%)]\tLoss: 2878.503174\tLR: [1.5595238095238098e-05]\n",
            "Train Epoch: 36 [14025/14335 (98%)]\tLoss: 2538.610596\tLR: [1.5575396825396828e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 36 is 0.9550387596899226, Best ROC Score is:0.9757894736842105\n",
            "Starting Epoch 37\n",
            "Train Epoch: 37 [0/14335 (0%)]\tLoss: 2737.143311\tLR: [1.5555555555555555e-05]\n",
            "Train Epoch: 37 [256/14335 (2%)]\tLoss: 3711.420898\tLR: [1.5535714285714285e-05]\n",
            "Train Epoch: 37 [512/14335 (4%)]\tLoss: 3219.459961\tLR: [1.5515873015873016e-05]\n",
            "Train Epoch: 37 [768/14335 (5%)]\tLoss: 3189.609863\tLR: [1.5496031746031746e-05]\n",
            "Train Epoch: 37 [1024/14335 (7%)]\tLoss: 3165.111572\tLR: [1.5476190476190476e-05]\n",
            "Train Epoch: 37 [1280/14335 (9%)]\tLoss: 3000.232910\tLR: [1.5456349206349207e-05]\n",
            "Train Epoch: 37 [1536/14335 (11%)]\tLoss: 3132.459473\tLR: [1.5436507936507937e-05]\n",
            "Train Epoch: 37 [1792/14335 (12%)]\tLoss: 2888.281494\tLR: [1.5416666666666668e-05]\n",
            "Train Epoch: 37 [2048/14335 (14%)]\tLoss: 2920.810303\tLR: [1.5396825396825398e-05]\n",
            "Train Epoch: 37 [2304/14335 (16%)]\tLoss: 2967.570557\tLR: [1.537698412698413e-05]\n",
            "Train Epoch: 37 [2560/14335 (18%)]\tLoss: 3065.354980\tLR: [1.535714285714286e-05]\n",
            "Train Epoch: 37 [2816/14335 (20%)]\tLoss: 3090.945557\tLR: [1.533730158730159e-05]\n",
            "Train Epoch: 37 [3072/14335 (21%)]\tLoss: 2813.211182\tLR: [1.531746031746032e-05]\n",
            "Train Epoch: 37 [3328/14335 (23%)]\tLoss: 3002.562988\tLR: [1.529761904761905e-05]\n",
            "Train Epoch: 37 [3584/14335 (25%)]\tLoss: 3013.146973\tLR: [1.527777777777778e-05]\n",
            "Train Epoch: 37 [3840/14335 (27%)]\tLoss: 3302.625977\tLR: [1.525793650793651e-05]\n",
            "Train Epoch: 37 [4096/14335 (29%)]\tLoss: 3103.320068\tLR: [1.5238095238095241e-05]\n",
            "Train Epoch: 37 [4352/14335 (30%)]\tLoss: 3203.880127\tLR: [1.5218253968253968e-05]\n",
            "Train Epoch: 37 [4608/14335 (32%)]\tLoss: 2975.871826\tLR: [1.5198412698412698e-05]\n",
            "Train Epoch: 37 [4864/14335 (34%)]\tLoss: 3026.011230\tLR: [1.5178571428571429e-05]\n",
            "Train Epoch: 37 [5120/14335 (36%)]\tLoss: 2606.404297\tLR: [1.5158730158730159e-05]\n",
            "Train Epoch: 37 [5376/14335 (38%)]\tLoss: 3271.424561\tLR: [1.5138888888888888e-05]\n",
            "Train Epoch: 37 [5632/14335 (39%)]\tLoss: 3073.378906\tLR: [1.5119047619047618e-05]\n",
            "Train Epoch: 37 [5888/14335 (41%)]\tLoss: 3172.437500\tLR: [1.5099206349206349e-05]\n",
            "Train Epoch: 37 [6144/14335 (43%)]\tLoss: 3289.474121\tLR: [1.5079365079365079e-05]\n",
            "Train Epoch: 37 [6400/14335 (45%)]\tLoss: 2885.540039\tLR: [1.505952380952381e-05]\n",
            "Train Epoch: 37 [6656/14335 (46%)]\tLoss: 2839.572021\tLR: [1.503968253968254e-05]\n",
            "Train Epoch: 37 [6912/14335 (48%)]\tLoss: 3043.250488\tLR: [1.501984126984127e-05]\n",
            "Train Epoch: 37 [7168/14335 (50%)]\tLoss: 3120.368652\tLR: [1.5e-05]\n",
            "Train Epoch: 37 [7424/14335 (52%)]\tLoss: 3207.656494\tLR: [1.498015873015873e-05]\n",
            "Train Epoch: 37 [7680/14335 (54%)]\tLoss: 3108.675537\tLR: [1.4960317460317461e-05]\n",
            "Train Epoch: 37 [7936/14335 (55%)]\tLoss: 2886.647949\tLR: [1.4940476190476192e-05]\n",
            "Train Epoch: 37 [8192/14335 (57%)]\tLoss: 3227.202148\tLR: [1.4920634920634922e-05]\n",
            "Train Epoch: 37 [8448/14335 (59%)]\tLoss: 2742.755615\tLR: [1.490079365079365e-05]\n",
            "Train Epoch: 37 [8704/14335 (61%)]\tLoss: 2671.077148\tLR: [1.4880952380952381e-05]\n",
            "Train Epoch: 37 [8960/14335 (62%)]\tLoss: 2792.670166\tLR: [1.4861111111111111e-05]\n",
            "Train Epoch: 37 [9216/14335 (64%)]\tLoss: 2888.441895\tLR: [1.4841269841269842e-05]\n",
            "Train Epoch: 37 [9472/14335 (66%)]\tLoss: 3141.155029\tLR: [1.4821428571428572e-05]\n",
            "Train Epoch: 37 [9728/14335 (68%)]\tLoss: 2830.601807\tLR: [1.4801587301587302e-05]\n",
            "Train Epoch: 37 [9984/14335 (70%)]\tLoss: 2525.246094\tLR: [1.4781746031746033e-05]\n",
            "Train Epoch: 37 [10240/14335 (71%)]\tLoss: 3156.568848\tLR: [1.4761904761904763e-05]\n",
            "Train Epoch: 37 [10496/14335 (73%)]\tLoss: 3209.794922\tLR: [1.4742063492063494e-05]\n",
            "Train Epoch: 37 [10752/14335 (75%)]\tLoss: 2602.374512\tLR: [1.4722222222222224e-05]\n",
            "Train Epoch: 37 [11008/14335 (77%)]\tLoss: 3083.288574\tLR: [1.4702380952380954e-05]\n",
            "Train Epoch: 37 [11264/14335 (79%)]\tLoss: 2812.913818\tLR: [1.4682539682539683e-05]\n",
            "Train Epoch: 37 [11520/14335 (80%)]\tLoss: 3044.697510\tLR: [1.4662698412698413e-05]\n",
            "Train Epoch: 37 [11776/14335 (82%)]\tLoss: 2976.221680\tLR: [1.4642857142857144e-05]\n",
            "Train Epoch: 37 [12032/14335 (84%)]\tLoss: 2768.519043\tLR: [1.4623015873015874e-05]\n",
            "Train Epoch: 37 [12288/14335 (86%)]\tLoss: 2987.430176\tLR: [1.4603174603174605e-05]\n",
            "Train Epoch: 37 [12544/14335 (88%)]\tLoss: 2986.888916\tLR: [1.4583333333333335e-05]\n",
            "Train Epoch: 37 [12800/14335 (89%)]\tLoss: 3315.608887\tLR: [1.4563492063492065e-05]\n",
            "Train Epoch: 37 [13056/14335 (91%)]\tLoss: 3036.867188\tLR: [1.4543650793650796e-05]\n",
            "Train Epoch: 37 [13312/14335 (93%)]\tLoss: 2602.770020\tLR: [1.4523809523809526e-05]\n",
            "Train Epoch: 37 [13568/14335 (95%)]\tLoss: 2643.192871\tLR: [1.4503968253968256e-05]\n",
            "Train Epoch: 37 [13824/14335 (96%)]\tLoss: 2927.915283\tLR: [1.4484126984126987e-05]\n",
            "Train Epoch: 37 [14025/14335 (98%)]\tLoss: 2651.416504\tLR: [1.4464285714285717e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 37 is 0.9241917502787068, Best ROC Score is:0.9757894736842105\n",
            "Starting Epoch 38\n",
            "Train Epoch: 38 [0/14335 (0%)]\tLoss: 3247.322021\tLR: [1.4444444444444444e-05]\n",
            "Train Epoch: 38 [256/14335 (2%)]\tLoss: 2895.590820\tLR: [1.4424603174603174e-05]\n",
            "Train Epoch: 38 [512/14335 (4%)]\tLoss: 2868.406250\tLR: [1.4404761904761905e-05]\n",
            "Train Epoch: 38 [768/14335 (5%)]\tLoss: 3018.671631\tLR: [1.4384920634920635e-05]\n",
            "Train Epoch: 38 [1024/14335 (7%)]\tLoss: 2910.855225\tLR: [1.4365079365079364e-05]\n",
            "Train Epoch: 38 [1280/14335 (9%)]\tLoss: 2710.706299\tLR: [1.4345238095238094e-05]\n",
            "Train Epoch: 38 [1536/14335 (11%)]\tLoss: 3187.658936\tLR: [1.4325396825396825e-05]\n",
            "Train Epoch: 38 [1792/14335 (12%)]\tLoss: 2922.232910\tLR: [1.4305555555555555e-05]\n",
            "Train Epoch: 38 [2048/14335 (14%)]\tLoss: 2852.711182\tLR: [1.4285714285714285e-05]\n",
            "Train Epoch: 38 [2304/14335 (16%)]\tLoss: 3107.854492\tLR: [1.4265873015873016e-05]\n",
            "Train Epoch: 38 [2560/14335 (18%)]\tLoss: 2910.884766\tLR: [1.4246031746031746e-05]\n",
            "Train Epoch: 38 [2816/14335 (20%)]\tLoss: 2732.538574\tLR: [1.4226190476190477e-05]\n",
            "Train Epoch: 38 [3072/14335 (21%)]\tLoss: 2616.198486\tLR: [1.4206349206349207e-05]\n",
            "Train Epoch: 38 [3328/14335 (23%)]\tLoss: 2664.974854\tLR: [1.4186507936507937e-05]\n",
            "Train Epoch: 38 [3584/14335 (25%)]\tLoss: 2703.541992\tLR: [1.4166666666666668e-05]\n",
            "Train Epoch: 38 [3840/14335 (27%)]\tLoss: 2807.038086\tLR: [1.4146825396825396e-05]\n",
            "Train Epoch: 38 [4096/14335 (29%)]\tLoss: 3068.117432\tLR: [1.4126984126984127e-05]\n",
            "Train Epoch: 38 [4352/14335 (30%)]\tLoss: 3076.033203\tLR: [1.4107142857142857e-05]\n",
            "Train Epoch: 38 [4608/14335 (32%)]\tLoss: 2780.729492\tLR: [1.4087301587301587e-05]\n",
            "Train Epoch: 38 [4864/14335 (34%)]\tLoss: 2872.223633\tLR: [1.4067460317460318e-05]\n",
            "Train Epoch: 38 [5120/14335 (36%)]\tLoss: 3134.543457\tLR: [1.4047619047619048e-05]\n",
            "Train Epoch: 38 [5376/14335 (38%)]\tLoss: 2774.344971\tLR: [1.4027777777777779e-05]\n",
            "Train Epoch: 38 [5632/14335 (39%)]\tLoss: 2762.407471\tLR: [1.4007936507936509e-05]\n",
            "Train Epoch: 38 [5888/14335 (41%)]\tLoss: 2489.445557\tLR: [1.398809523809524e-05]\n",
            "Train Epoch: 38 [6144/14335 (43%)]\tLoss: 3346.973633\tLR: [1.396825396825397e-05]\n",
            "Train Epoch: 38 [6400/14335 (45%)]\tLoss: 2987.010986\tLR: [1.39484126984127e-05]\n",
            "Train Epoch: 38 [6656/14335 (46%)]\tLoss: 3090.867676\tLR: [1.392857142857143e-05]\n",
            "Train Epoch: 38 [6912/14335 (48%)]\tLoss: 3179.936035\tLR: [1.390873015873016e-05]\n",
            "Train Epoch: 38 [7168/14335 (50%)]\tLoss: 3177.093750\tLR: [1.388888888888889e-05]\n",
            "Train Epoch: 38 [7424/14335 (52%)]\tLoss: 2816.130371\tLR: [1.386904761904762e-05]\n",
            "Train Epoch: 38 [7680/14335 (54%)]\tLoss: 2931.314941\tLR: [1.384920634920635e-05]\n",
            "Train Epoch: 38 [7936/14335 (55%)]\tLoss: 3082.103271\tLR: [1.382936507936508e-05]\n",
            "Train Epoch: 38 [8192/14335 (57%)]\tLoss: 3405.088623\tLR: [1.3809523809523811e-05]\n",
            "Train Epoch: 38 [8448/14335 (59%)]\tLoss: 2874.298828\tLR: [1.3789682539682541e-05]\n",
            "Train Epoch: 38 [8704/14335 (61%)]\tLoss: 3540.068359\tLR: [1.3769841269841272e-05]\n",
            "Train Epoch: 38 [8960/14335 (62%)]\tLoss: 3195.607910\tLR: [1.3750000000000002e-05]\n",
            "Train Epoch: 38 [9216/14335 (64%)]\tLoss: 2785.446289\tLR: [1.3730158730158733e-05]\n",
            "Train Epoch: 38 [9472/14335 (66%)]\tLoss: 3176.194580\tLR: [1.3710317460317463e-05]\n",
            "Train Epoch: 38 [9728/14335 (68%)]\tLoss: 3134.089355\tLR: [1.3690476190476192e-05]\n",
            "Train Epoch: 38 [9984/14335 (70%)]\tLoss: 3136.988281\tLR: [1.367063492063492e-05]\n",
            "Train Epoch: 38 [10240/14335 (71%)]\tLoss: 2987.642090\tLR: [1.365079365079365e-05]\n",
            "Train Epoch: 38 [10496/14335 (73%)]\tLoss: 2972.341064\tLR: [1.3630952380952381e-05]\n",
            "Train Epoch: 38 [10752/14335 (75%)]\tLoss: 3227.069580\tLR: [1.3611111111111111e-05]\n",
            "Train Epoch: 38 [11008/14335 (77%)]\tLoss: 2861.757324\tLR: [1.359126984126984e-05]\n",
            "Train Epoch: 38 [11264/14335 (79%)]\tLoss: 3463.522461\tLR: [1.357142857142857e-05]\n",
            "Train Epoch: 38 [11520/14335 (80%)]\tLoss: 2964.113525\tLR: [1.3551587301587301e-05]\n",
            "Train Epoch: 38 [11776/14335 (82%)]\tLoss: 3255.489258\tLR: [1.3531746031746031e-05]\n",
            "Train Epoch: 38 [12032/14335 (84%)]\tLoss: 2950.104004\tLR: [1.3511904761904762e-05]\n",
            "Train Epoch: 38 [12288/14335 (86%)]\tLoss: 2999.419189\tLR: [1.3492063492063492e-05]\n",
            "Train Epoch: 38 [12544/14335 (88%)]\tLoss: 3161.097412\tLR: [1.3472222222222222e-05]\n",
            "Train Epoch: 38 [12800/14335 (89%)]\tLoss: 3285.303955\tLR: [1.3452380952380953e-05]\n",
            "Train Epoch: 38 [13056/14335 (91%)]\tLoss: 2792.399414\tLR: [1.3432539682539683e-05]\n",
            "Train Epoch: 38 [13312/14335 (93%)]\tLoss: 3164.107422\tLR: [1.3412698412698413e-05]\n",
            "Train Epoch: 38 [13568/14335 (95%)]\tLoss: 2904.502686\tLR: [1.3392857142857144e-05]\n",
            "Train Epoch: 38 [13824/14335 (96%)]\tLoss: 2948.986328\tLR: [1.3373015873015873e-05]\n",
            "Train Epoch: 38 [14025/14335 (98%)]\tLoss: 2559.902344\tLR: [1.3353174603174603e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 38 is 0.9417670682730924, Best ROC Score is:0.9757894736842105\n",
            "Starting Epoch 39\n",
            "Train Epoch: 39 [0/14335 (0%)]\tLoss: 2853.986572\tLR: [1.3333333333333333e-05]\n",
            "Train Epoch: 39 [256/14335 (2%)]\tLoss: 3316.967285\tLR: [1.3313492063492064e-05]\n",
            "Train Epoch: 39 [512/14335 (4%)]\tLoss: 2947.933350\tLR: [1.3293650793650794e-05]\n",
            "Train Epoch: 39 [768/14335 (5%)]\tLoss: 3154.127930\tLR: [1.3273809523809524e-05]\n",
            "Train Epoch: 39 [1024/14335 (7%)]\tLoss: 2969.414795\tLR: [1.3253968253968255e-05]\n",
            "Train Epoch: 39 [1280/14335 (9%)]\tLoss: 2554.572510\tLR: [1.3234126984126985e-05]\n",
            "Train Epoch: 39 [1536/14335 (11%)]\tLoss: 2656.190186\tLR: [1.3214285714285716e-05]\n",
            "Train Epoch: 39 [1792/14335 (12%)]\tLoss: 3240.672607\tLR: [1.3194444444444446e-05]\n",
            "Train Epoch: 39 [2048/14335 (14%)]\tLoss: 3345.414551\tLR: [1.3174603174603176e-05]\n",
            "Train Epoch: 39 [2304/14335 (16%)]\tLoss: 2573.130615\tLR: [1.3154761904761907e-05]\n",
            "Train Epoch: 39 [2560/14335 (18%)]\tLoss: 2888.878418\tLR: [1.3134920634920635e-05]\n",
            "Train Epoch: 39 [2816/14335 (20%)]\tLoss: 2788.600830\tLR: [1.3115079365079366e-05]\n",
            "Train Epoch: 39 [3072/14335 (21%)]\tLoss: 3327.736572\tLR: [1.3095238095238096e-05]\n",
            "Train Epoch: 39 [3328/14335 (23%)]\tLoss: 3031.173096\tLR: [1.3075396825396826e-05]\n",
            "Train Epoch: 39 [3584/14335 (25%)]\tLoss: 2730.713623\tLR: [1.3055555555555557e-05]\n",
            "Train Epoch: 39 [3840/14335 (27%)]\tLoss: 3014.659424\tLR: [1.3035714285714287e-05]\n",
            "Train Epoch: 39 [4096/14335 (29%)]\tLoss: 2969.236572\tLR: [1.3015873015873018e-05]\n",
            "Train Epoch: 39 [4352/14335 (30%)]\tLoss: 2917.231689\tLR: [1.2996031746031748e-05]\n",
            "Train Epoch: 39 [4608/14335 (32%)]\tLoss: 2910.958984\tLR: [1.2976190476190478e-05]\n",
            "Train Epoch: 39 [4864/14335 (34%)]\tLoss: 2981.103516\tLR: [1.2956349206349209e-05]\n",
            "Train Epoch: 39 [5120/14335 (36%)]\tLoss: 3092.557617\tLR: [1.2936507936507939e-05]\n",
            "Train Epoch: 39 [5376/14335 (38%)]\tLoss: 3107.568848\tLR: [1.2916666666666668e-05]\n",
            "Train Epoch: 39 [5632/14335 (39%)]\tLoss: 2849.657227\tLR: [1.2896825396825398e-05]\n",
            "Train Epoch: 39 [5888/14335 (41%)]\tLoss: 3033.684082\tLR: [1.2876984126984127e-05]\n",
            "Train Epoch: 39 [6144/14335 (43%)]\tLoss: 3018.890381\tLR: [1.2857142857142857e-05]\n",
            "Train Epoch: 39 [6400/14335 (45%)]\tLoss: 3165.427979\tLR: [1.2837301587301586e-05]\n",
            "Train Epoch: 39 [6656/14335 (46%)]\tLoss: 3342.202148\tLR: [1.2817460317460316e-05]\n",
            "Train Epoch: 39 [6912/14335 (48%)]\tLoss: 2807.686279\tLR: [1.2797619047619047e-05]\n",
            "Train Epoch: 39 [7168/14335 (50%)]\tLoss: 3096.890137\tLR: [1.2777777777777777e-05]\n",
            "Train Epoch: 39 [7424/14335 (52%)]\tLoss: 2867.161133\tLR: [1.2757936507936507e-05]\n",
            "Train Epoch: 39 [7680/14335 (54%)]\tLoss: 2977.837158\tLR: [1.2738095238095238e-05]\n",
            "Train Epoch: 39 [7936/14335 (55%)]\tLoss: 3366.877197\tLR: [1.2718253968253968e-05]\n",
            "Train Epoch: 39 [8192/14335 (57%)]\tLoss: 2919.399414\tLR: [1.2698412698412699e-05]\n",
            "Train Epoch: 39 [8448/14335 (59%)]\tLoss: 3264.804688\tLR: [1.2678571428571429e-05]\n",
            "Train Epoch: 39 [8704/14335 (61%)]\tLoss: 3024.554443\tLR: [1.265873015873016e-05]\n",
            "Train Epoch: 39 [8960/14335 (62%)]\tLoss: 3276.070801\tLR: [1.263888888888889e-05]\n",
            "Train Epoch: 39 [9216/14335 (64%)]\tLoss: 2950.174316\tLR: [1.261904761904762e-05]\n",
            "Train Epoch: 39 [9472/14335 (66%)]\tLoss: 3249.268555\tLR: [1.2599206349206349e-05]\n",
            "Train Epoch: 39 [9728/14335 (68%)]\tLoss: 2563.091797\tLR: [1.2579365079365079e-05]\n",
            "Train Epoch: 39 [9984/14335 (70%)]\tLoss: 3245.396973\tLR: [1.255952380952381e-05]\n",
            "Train Epoch: 39 [10240/14335 (71%)]\tLoss: 3053.407471\tLR: [1.253968253968254e-05]\n",
            "Train Epoch: 39 [10496/14335 (73%)]\tLoss: 2965.606445\tLR: [1.251984126984127e-05]\n",
            "Train Epoch: 39 [10752/14335 (75%)]\tLoss: 3227.781738\tLR: [1.25e-05]\n",
            "Train Epoch: 39 [11008/14335 (77%)]\tLoss: 2814.643311\tLR: [1.2480158730158731e-05]\n",
            "Train Epoch: 39 [11264/14335 (79%)]\tLoss: 2689.683838\tLR: [1.2460317460317461e-05]\n",
            "Train Epoch: 39 [11520/14335 (80%)]\tLoss: 3214.406006\tLR: [1.2440476190476192e-05]\n",
            "Train Epoch: 39 [11776/14335 (82%)]\tLoss: 2788.763916\tLR: [1.2420634920634922e-05]\n",
            "Train Epoch: 39 [12032/14335 (84%)]\tLoss: 2790.890137\tLR: [1.2400793650793652e-05]\n",
            "Train Epoch: 39 [12288/14335 (86%)]\tLoss: 3556.285156\tLR: [1.2380952380952381e-05]\n",
            "Train Epoch: 39 [12544/14335 (88%)]\tLoss: 3131.869873\tLR: [1.2361111111111112e-05]\n",
            "Train Epoch: 39 [12800/14335 (89%)]\tLoss: 3162.342529\tLR: [1.2341269841269842e-05]\n",
            "Train Epoch: 39 [13056/14335 (91%)]\tLoss: 2483.901855\tLR: [1.2321428571428572e-05]\n",
            "Train Epoch: 39 [13312/14335 (93%)]\tLoss: 3234.180908\tLR: [1.2301587301587301e-05]\n",
            "Train Epoch: 39 [13568/14335 (95%)]\tLoss: 2670.276123\tLR: [1.2281746031746031e-05]\n",
            "Train Epoch: 39 [13824/14335 (96%)]\tLoss: 3103.195801\tLR: [1.2261904761904762e-05]\n",
            "Train Epoch: 39 [14025/14335 (98%)]\tLoss: 2752.097656\tLR: [1.2242063492063492e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 39 is 0.9471218206157965, Best ROC Score is:0.9757894736842105\n",
            "Starting Epoch 40\n",
            "Train Epoch: 40 [0/14335 (0%)]\tLoss: 2892.334473\tLR: [1.2222222222222222e-05]\n",
            "Train Epoch: 40 [256/14335 (2%)]\tLoss: 3175.675537\tLR: [1.2202380952380953e-05]\n",
            "Train Epoch: 40 [512/14335 (4%)]\tLoss: 3054.307373\tLR: [1.2182539682539683e-05]\n",
            "Train Epoch: 40 [768/14335 (5%)]\tLoss: 3127.132324\tLR: [1.2162698412698414e-05]\n",
            "Train Epoch: 40 [1024/14335 (7%)]\tLoss: 3145.790771\tLR: [1.2142857142857144e-05]\n",
            "Train Epoch: 40 [1280/14335 (9%)]\tLoss: 3021.417236\tLR: [1.2123015873015874e-05]\n",
            "Train Epoch: 40 [1536/14335 (11%)]\tLoss: 3137.393311\tLR: [1.2103174603174603e-05]\n",
            "Train Epoch: 40 [1792/14335 (12%)]\tLoss: 3136.537842\tLR: [1.2083333333333333e-05]\n",
            "Train Epoch: 40 [2048/14335 (14%)]\tLoss: 2962.285889\tLR: [1.2063492063492064e-05]\n",
            "Train Epoch: 40 [2304/14335 (16%)]\tLoss: 2918.986084\tLR: [1.2043650793650794e-05]\n",
            "Train Epoch: 40 [2560/14335 (18%)]\tLoss: 3057.319580\tLR: [1.2023809523809525e-05]\n",
            "Train Epoch: 40 [2816/14335 (20%)]\tLoss: 2809.410400\tLR: [1.2003968253968255e-05]\n",
            "Train Epoch: 40 [3072/14335 (21%)]\tLoss: 3032.569092\tLR: [1.1984126984126985e-05]\n",
            "Train Epoch: 40 [3328/14335 (23%)]\tLoss: 2863.427734\tLR: [1.1964285714285716e-05]\n",
            "Train Epoch: 40 [3584/14335 (25%)]\tLoss: 3360.187988\tLR: [1.1944444444444446e-05]\n",
            "Train Epoch: 40 [3840/14335 (27%)]\tLoss: 3010.449219\tLR: [1.1924603174603176e-05]\n",
            "Train Epoch: 40 [4096/14335 (29%)]\tLoss: 2682.553955\tLR: [1.1904761904761905e-05]\n",
            "Train Epoch: 40 [4352/14335 (30%)]\tLoss: 2916.961914\tLR: [1.1884920634920635e-05]\n",
            "Train Epoch: 40 [4608/14335 (32%)]\tLoss: 2955.247803\tLR: [1.1865079365079366e-05]\n",
            "Train Epoch: 40 [4864/14335 (34%)]\tLoss: 2876.887695\tLR: [1.1845238095238095e-05]\n",
            "Train Epoch: 40 [5120/14335 (36%)]\tLoss: 2508.874268\tLR: [1.1825396825396825e-05]\n",
            "Train Epoch: 40 [5376/14335 (38%)]\tLoss: 3051.776367\tLR: [1.1805555555555555e-05]\n",
            "Train Epoch: 40 [5632/14335 (39%)]\tLoss: 3020.591553\tLR: [1.1785714285714286e-05]\n",
            "Train Epoch: 40 [5888/14335 (41%)]\tLoss: 3238.170410\tLR: [1.1765873015873016e-05]\n",
            "Train Epoch: 40 [6144/14335 (43%)]\tLoss: 3187.839355\tLR: [1.1746031746031746e-05]\n",
            "Train Epoch: 40 [6400/14335 (45%)]\tLoss: 2918.856934\tLR: [1.1726190476190477e-05]\n",
            "Train Epoch: 40 [6656/14335 (46%)]\tLoss: 3429.288086\tLR: [1.1706349206349207e-05]\n",
            "Train Epoch: 40 [6912/14335 (48%)]\tLoss: 3153.382324\tLR: [1.1686507936507938e-05]\n",
            "Train Epoch: 40 [7168/14335 (50%)]\tLoss: 2909.153809\tLR: [1.1666666666666668e-05]\n",
            "Train Epoch: 40 [7424/14335 (52%)]\tLoss: 2913.458008\tLR: [1.1646825396825398e-05]\n",
            "Train Epoch: 40 [7680/14335 (54%)]\tLoss: 2843.010498\tLR: [1.1626984126984129e-05]\n",
            "Train Epoch: 40 [7936/14335 (55%)]\tLoss: 3186.184570\tLR: [1.1607142857142857e-05]\n",
            "Train Epoch: 40 [8192/14335 (57%)]\tLoss: 2752.822510\tLR: [1.1587301587301588e-05]\n",
            "Train Epoch: 40 [8448/14335 (59%)]\tLoss: 3101.504639\tLR: [1.1567460317460318e-05]\n",
            "Train Epoch: 40 [8704/14335 (61%)]\tLoss: 3161.961670\tLR: [1.1547619047619048e-05]\n",
            "Train Epoch: 40 [8960/14335 (62%)]\tLoss: 2823.561279\tLR: [1.1527777777777779e-05]\n",
            "Train Epoch: 40 [9216/14335 (64%)]\tLoss: 3197.358154\tLR: [1.1507936507936508e-05]\n",
            "Train Epoch: 40 [9472/14335 (66%)]\tLoss: 2764.171875\tLR: [1.1488095238095238e-05]\n",
            "Train Epoch: 40 [9728/14335 (68%)]\tLoss: 3144.448975\tLR: [1.1468253968253968e-05]\n",
            "Train Epoch: 40 [9984/14335 (70%)]\tLoss: 2716.799805\tLR: [1.1448412698412699e-05]\n",
            "Train Epoch: 40 [10240/14335 (71%)]\tLoss: 2955.797607\tLR: [1.1428571428571429e-05]\n",
            "Train Epoch: 40 [10496/14335 (73%)]\tLoss: 2979.004883\tLR: [1.140873015873016e-05]\n",
            "Train Epoch: 40 [10752/14335 (75%)]\tLoss: 2947.967285\tLR: [1.138888888888889e-05]\n",
            "Train Epoch: 40 [11008/14335 (77%)]\tLoss: 2946.527588\tLR: [1.136904761904762e-05]\n",
            "Train Epoch: 40 [11264/14335 (79%)]\tLoss: 2974.429199\tLR: [1.1349206349206349e-05]\n",
            "Train Epoch: 40 [11520/14335 (80%)]\tLoss: 2949.522217\tLR: [1.132936507936508e-05]\n",
            "Train Epoch: 40 [11776/14335 (82%)]\tLoss: 2906.769775\tLR: [1.130952380952381e-05]\n",
            "Train Epoch: 40 [12032/14335 (84%)]\tLoss: 3080.387939\tLR: [1.128968253968254e-05]\n",
            "Train Epoch: 40 [12288/14335 (86%)]\tLoss: 2661.487305\tLR: [1.126984126984127e-05]\n",
            "Train Epoch: 40 [12544/14335 (88%)]\tLoss: 3292.051758\tLR: [1.125e-05]\n",
            "Train Epoch: 40 [12800/14335 (89%)]\tLoss: 3102.300537\tLR: [1.1230158730158731e-05]\n",
            "Train Epoch: 40 [13056/14335 (91%)]\tLoss: 2863.393066\tLR: [1.1210317460317461e-05]\n",
            "Train Epoch: 40 [13312/14335 (93%)]\tLoss: 2859.171143\tLR: [1.1190476190476192e-05]\n",
            "Train Epoch: 40 [13568/14335 (95%)]\tLoss: 2911.429688\tLR: [1.1170634920634922e-05]\n",
            "Train Epoch: 40 [13824/14335 (96%)]\tLoss: 2789.973633\tLR: [1.1150793650793653e-05]\n",
            "Train Epoch: 40 [14025/14335 (98%)]\tLoss: 3140.184082\tLR: [1.1130952380952381e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 40 is 0.9297435897435897, Best ROC Score is:0.9757894736842105\n",
            "Starting Epoch 41\n",
            "Train Epoch: 41 [0/14335 (0%)]\tLoss: 3584.602539\tLR: [1.1111111111111112e-05]\n",
            "Train Epoch: 41 [256/14335 (2%)]\tLoss: 2855.327637\tLR: [1.1091269841269842e-05]\n",
            "Train Epoch: 41 [512/14335 (4%)]\tLoss: 3004.052246\tLR: [1.107142857142857e-05]\n",
            "Train Epoch: 41 [768/14335 (5%)]\tLoss: 3076.569336\tLR: [1.1051587301587301e-05]\n",
            "Train Epoch: 41 [1024/14335 (7%)]\tLoss: 3043.968994\tLR: [1.1031746031746031e-05]\n",
            "Train Epoch: 41 [1280/14335 (9%)]\tLoss: 3142.981934\tLR: [1.1011904761904762e-05]\n",
            "Train Epoch: 41 [1536/14335 (11%)]\tLoss: 3019.133545\tLR: [1.0992063492063492e-05]\n",
            "Train Epoch: 41 [1792/14335 (12%)]\tLoss: 3041.972168\tLR: [1.0972222222222223e-05]\n",
            "Train Epoch: 41 [2048/14335 (14%)]\tLoss: 2943.666016\tLR: [1.0952380952380953e-05]\n",
            "Train Epoch: 41 [2304/14335 (16%)]\tLoss: 2943.644531\tLR: [1.0932539682539683e-05]\n",
            "Train Epoch: 41 [2560/14335 (18%)]\tLoss: 2175.378174\tLR: [1.0912698412698414e-05]\n",
            "Train Epoch: 41 [2816/14335 (20%)]\tLoss: 3218.289795\tLR: [1.0892857142857144e-05]\n",
            "Train Epoch: 41 [3072/14335 (21%)]\tLoss: 2793.290039\tLR: [1.0873015873015874e-05]\n",
            "Train Epoch: 41 [3328/14335 (23%)]\tLoss: 3148.667969\tLR: [1.0853174603174605e-05]\n",
            "Train Epoch: 41 [3584/14335 (25%)]\tLoss: 2634.455322\tLR: [1.0833333333333334e-05]\n",
            "Train Epoch: 41 [3840/14335 (27%)]\tLoss: 2713.250000\tLR: [1.0813492063492064e-05]\n",
            "Train Epoch: 41 [4096/14335 (29%)]\tLoss: 3192.648193\tLR: [1.0793650793650794e-05]\n",
            "Train Epoch: 41 [4352/14335 (30%)]\tLoss: 3366.171143\tLR: [1.0773809523809525e-05]\n",
            "Train Epoch: 41 [4608/14335 (32%)]\tLoss: 2842.360840\tLR: [1.0753968253968255e-05]\n",
            "Train Epoch: 41 [4864/14335 (34%)]\tLoss: 3383.808350\tLR: [1.0734126984126984e-05]\n",
            "Train Epoch: 41 [5120/14335 (36%)]\tLoss: 3458.026855\tLR: [1.0714285714285714e-05]\n",
            "Train Epoch: 41 [5376/14335 (38%)]\tLoss: 2661.679443\tLR: [1.0694444444444444e-05]\n",
            "Train Epoch: 41 [5632/14335 (39%)]\tLoss: 2866.243896\tLR: [1.0674603174603175e-05]\n",
            "Train Epoch: 41 [5888/14335 (41%)]\tLoss: 3338.716553\tLR: [1.0654761904761905e-05]\n",
            "Train Epoch: 41 [6144/14335 (43%)]\tLoss: 2983.567383\tLR: [1.0634920634920636e-05]\n",
            "Train Epoch: 41 [6400/14335 (45%)]\tLoss: 3033.783691\tLR: [1.0615079365079366e-05]\n",
            "Train Epoch: 41 [6656/14335 (46%)]\tLoss: 3094.002197\tLR: [1.0595238095238096e-05]\n",
            "Train Epoch: 41 [6912/14335 (48%)]\tLoss: 3066.379150\tLR: [1.0575396825396825e-05]\n",
            "Train Epoch: 41 [7168/14335 (50%)]\tLoss: 2809.142822\tLR: [1.0555555555555555e-05]\n",
            "Train Epoch: 41 [7424/14335 (52%)]\tLoss: 2777.428955\tLR: [1.0535714285714286e-05]\n",
            "Train Epoch: 41 [7680/14335 (54%)]\tLoss: 3093.443115\tLR: [1.0515873015873016e-05]\n",
            "Train Epoch: 41 [7936/14335 (55%)]\tLoss: 2677.386719\tLR: [1.0496031746031747e-05]\n",
            "Train Epoch: 41 [8192/14335 (57%)]\tLoss: 3169.224365\tLR: [1.0476190476190477e-05]\n",
            "Train Epoch: 41 [8448/14335 (59%)]\tLoss: 2887.232666\tLR: [1.0456349206349207e-05]\n",
            "Train Epoch: 41 [8704/14335 (61%)]\tLoss: 3336.352539\tLR: [1.0436507936507938e-05]\n",
            "Train Epoch: 41 [8960/14335 (62%)]\tLoss: 3005.170654\tLR: [1.0416666666666668e-05]\n",
            "Train Epoch: 41 [9216/14335 (64%)]\tLoss: 2940.043701\tLR: [1.0396825396825398e-05]\n",
            "Train Epoch: 41 [9472/14335 (66%)]\tLoss: 2937.702637\tLR: [1.0376984126984129e-05]\n",
            "Train Epoch: 41 [9728/14335 (68%)]\tLoss: 2947.744141\tLR: [1.0357142857142859e-05]\n",
            "Train Epoch: 41 [9984/14335 (70%)]\tLoss: 2900.126465\tLR: [1.0337301587301588e-05]\n",
            "Train Epoch: 41 [10240/14335 (71%)]\tLoss: 3122.259521\tLR: [1.0317460317460318e-05]\n",
            "Train Epoch: 41 [10496/14335 (73%)]\tLoss: 2778.533203\tLR: [1.0297619047619047e-05]\n",
            "Train Epoch: 41 [10752/14335 (75%)]\tLoss: 2841.283691\tLR: [1.0277777777777777e-05]\n",
            "Train Epoch: 41 [11008/14335 (77%)]\tLoss: 3282.017090\tLR: [1.0257936507936508e-05]\n",
            "Train Epoch: 41 [11264/14335 (79%)]\tLoss: 2971.436279\tLR: [1.0238095238095238e-05]\n",
            "Train Epoch: 41 [11520/14335 (80%)]\tLoss: 2767.410889\tLR: [1.0218253968253968e-05]\n",
            "Train Epoch: 41 [11776/14335 (82%)]\tLoss: 2881.190674\tLR: [1.0198412698412699e-05]\n",
            "Train Epoch: 41 [12032/14335 (84%)]\tLoss: 2840.783447\tLR: [1.0178571428571429e-05]\n",
            "Train Epoch: 41 [12288/14335 (86%)]\tLoss: 3456.052490\tLR: [1.015873015873016e-05]\n",
            "Train Epoch: 41 [12544/14335 (88%)]\tLoss: 2768.073486\tLR: [1.013888888888889e-05]\n",
            "Train Epoch: 41 [12800/14335 (89%)]\tLoss: 3293.838379\tLR: [1.011904761904762e-05]\n",
            "Train Epoch: 41 [13056/14335 (91%)]\tLoss: 2944.283936\tLR: [1.009920634920635e-05]\n",
            "Train Epoch: 41 [13312/14335 (93%)]\tLoss: 3028.939209\tLR: [1.007936507936508e-05]\n",
            "Train Epoch: 41 [13568/14335 (95%)]\tLoss: 2867.519043\tLR: [1.005952380952381e-05]\n",
            "Train Epoch: 41 [13824/14335 (96%)]\tLoss: 2925.840576\tLR: [1.003968253968254e-05]\n",
            "Train Epoch: 41 [14025/14335 (98%)]\tLoss: 3081.800537\tLR: [1.001984126984127e-05]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 41 is 0.9732441471571907, Best ROC Score is:0.9757894736842105\n",
            "Starting Epoch 42\n",
            "Train Epoch: 42 [0/14335 (0%)]\tLoss: 2746.009521\tLR: [1e-05]\n",
            "Train Epoch: 42 [256/14335 (2%)]\tLoss: 2908.224609\tLR: [9.980158730158731e-06]\n",
            "Train Epoch: 42 [512/14335 (4%)]\tLoss: 2976.737793\tLR: [9.96031746031746e-06]\n",
            "Train Epoch: 42 [768/14335 (5%)]\tLoss: 2800.378906\tLR: [9.94047619047619e-06]\n",
            "Train Epoch: 42 [1024/14335 (7%)]\tLoss: 3142.833252\tLR: [9.92063492063492e-06]\n",
            "Train Epoch: 42 [1280/14335 (9%)]\tLoss: 2945.788574\tLR: [9.900793650793651e-06]\n",
            "Train Epoch: 42 [1536/14335 (11%)]\tLoss: 3047.738770\tLR: [9.880952380952381e-06]\n",
            "Train Epoch: 42 [1792/14335 (12%)]\tLoss: 3002.573486\tLR: [9.861111111111112e-06]\n",
            "Train Epoch: 42 [2048/14335 (14%)]\tLoss: 3136.587402\tLR: [9.841269841269842e-06]\n",
            "Train Epoch: 42 [2304/14335 (16%)]\tLoss: 2662.197021\tLR: [9.821428571428573e-06]\n",
            "Train Epoch: 42 [2560/14335 (18%)]\tLoss: 2988.152100\tLR: [9.801587301587301e-06]\n",
            "Train Epoch: 42 [2816/14335 (20%)]\tLoss: 3067.355957\tLR: [9.781746031746032e-06]\n",
            "Train Epoch: 42 [3072/14335 (21%)]\tLoss: 2524.257080\tLR: [9.761904761904762e-06]\n",
            "Train Epoch: 42 [3328/14335 (23%)]\tLoss: 2915.778076\tLR: [9.742063492063492e-06]\n",
            "Train Epoch: 42 [3584/14335 (25%)]\tLoss: 3060.071777\tLR: [9.722222222222223e-06]\n",
            "Train Epoch: 42 [3840/14335 (27%)]\tLoss: 3089.727783\tLR: [9.702380952380953e-06]\n",
            "Train Epoch: 42 [4096/14335 (29%)]\tLoss: 3222.122314\tLR: [9.682539682539683e-06]\n",
            "Train Epoch: 42 [4352/14335 (30%)]\tLoss: 3145.671143\tLR: [9.662698412698414e-06]\n",
            "Train Epoch: 42 [4608/14335 (32%)]\tLoss: 3379.161621\tLR: [9.642857142857144e-06]\n",
            "Train Epoch: 42 [4864/14335 (34%)]\tLoss: 2937.153320\tLR: [9.623015873015875e-06]\n",
            "Train Epoch: 42 [5120/14335 (36%)]\tLoss: 3121.623047\tLR: [9.603174603174605e-06]\n",
            "Train Epoch: 42 [5376/14335 (38%)]\tLoss: 3255.163086\tLR: [9.583333333333334e-06]\n",
            "Train Epoch: 42 [5632/14335 (39%)]\tLoss: 2926.546387\tLR: [9.563492063492064e-06]\n",
            "Train Epoch: 42 [5888/14335 (41%)]\tLoss: 2844.071777\tLR: [9.543650793650793e-06]\n",
            "Train Epoch: 42 [6144/14335 (43%)]\tLoss: 2840.902588\tLR: [9.523809523809523e-06]\n",
            "Train Epoch: 42 [6400/14335 (45%)]\tLoss: 2895.229492\tLR: [9.503968253968253e-06]\n",
            "Train Epoch: 42 [6656/14335 (46%)]\tLoss: 2761.311035\tLR: [9.484126984126984e-06]\n",
            "Train Epoch: 42 [6912/14335 (48%)]\tLoss: 3259.196777\tLR: [9.464285714285714e-06]\n",
            "Train Epoch: 42 [7168/14335 (50%)]\tLoss: 2959.395264\tLR: [9.444444444444445e-06]\n",
            "Train Epoch: 42 [7424/14335 (52%)]\tLoss: 2860.200928\tLR: [9.424603174603175e-06]\n",
            "Train Epoch: 42 [7680/14335 (54%)]\tLoss: 3126.020996\tLR: [9.404761904761905e-06]\n",
            "Train Epoch: 42 [7936/14335 (55%)]\tLoss: 2891.044922\tLR: [9.384920634920636e-06]\n",
            "Train Epoch: 42 [8192/14335 (57%)]\tLoss: 2649.266846\tLR: [9.365079365079366e-06]\n",
            "Train Epoch: 42 [8448/14335 (59%)]\tLoss: 3070.836426\tLR: [9.345238095238096e-06]\n",
            "Train Epoch: 42 [8704/14335 (61%)]\tLoss: 3596.188965\tLR: [9.325396825396827e-06]\n",
            "Train Epoch: 42 [8960/14335 (62%)]\tLoss: 3092.043457\tLR: [9.305555555555555e-06]\n",
            "Train Epoch: 42 [9216/14335 (64%)]\tLoss: 3016.389404\tLR: [9.285714285714286e-06]\n",
            "Train Epoch: 42 [9472/14335 (66%)]\tLoss: 2761.436279\tLR: [9.265873015873016e-06]\n",
            "Train Epoch: 42 [9728/14335 (68%)]\tLoss: 3215.660889\tLR: [9.246031746031747e-06]\n",
            "Train Epoch: 42 [9984/14335 (70%)]\tLoss: 3345.215820\tLR: [9.226190476190477e-06]\n",
            "Train Epoch: 42 [10240/14335 (71%)]\tLoss: 3427.843994\tLR: [9.206349206349207e-06]\n",
            "Train Epoch: 42 [10496/14335 (73%)]\tLoss: 3319.389404\tLR: [9.186507936507938e-06]\n",
            "Train Epoch: 42 [10752/14335 (75%)]\tLoss: 2968.358643\tLR: [9.166666666666666e-06]\n",
            "Train Epoch: 42 [11008/14335 (77%)]\tLoss: 3003.796387\tLR: [9.146825396825397e-06]\n",
            "Train Epoch: 42 [11264/14335 (79%)]\tLoss: 2664.799072\tLR: [9.126984126984127e-06]\n",
            "Train Epoch: 42 [11520/14335 (80%)]\tLoss: 2928.562744\tLR: [9.107142857142858e-06]\n",
            "Train Epoch: 42 [11776/14335 (82%)]\tLoss: 2893.501465\tLR: [9.087301587301588e-06]\n",
            "Train Epoch: 42 [12032/14335 (84%)]\tLoss: 2949.470215\tLR: [9.067460317460318e-06]\n",
            "Train Epoch: 42 [12288/14335 (86%)]\tLoss: 2852.551514\tLR: [9.047619047619047e-06]\n",
            "Train Epoch: 42 [12544/14335 (88%)]\tLoss: 3225.036621\tLR: [9.027777777777777e-06]\n",
            "Train Epoch: 42 [12800/14335 (89%)]\tLoss: 3082.889648\tLR: [9.007936507936508e-06]\n",
            "Train Epoch: 42 [13056/14335 (91%)]\tLoss: 2973.178711\tLR: [8.988095238095238e-06]\n",
            "Train Epoch: 42 [13312/14335 (93%)]\tLoss: 2917.408936\tLR: [8.968253968253968e-06]\n",
            "Train Epoch: 42 [13568/14335 (95%)]\tLoss: 2572.826660\tLR: [8.948412698412699e-06]\n",
            "Train Epoch: 42 [13824/14335 (96%)]\tLoss: 3154.205811\tLR: [8.92857142857143e-06]\n",
            "Train Epoch: 42 [14025/14335 (98%)]\tLoss: 2771.900391\tLR: [8.90873015873016e-06]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 42 is 0.9123456790123456, Best ROC Score is:0.9757894736842105\n",
            "Starting Epoch 43\n",
            "Train Epoch: 43 [0/14335 (0%)]\tLoss: 2872.380615\tLR: [8.88888888888889e-06]\n",
            "Train Epoch: 43 [256/14335 (2%)]\tLoss: 3109.381592\tLR: [8.86904761904762e-06]\n",
            "Train Epoch: 43 [512/14335 (4%)]\tLoss: 2631.279541\tLR: [8.84920634920635e-06]\n",
            "Train Epoch: 43 [768/14335 (5%)]\tLoss: 3232.056396\tLR: [8.829365079365081e-06]\n",
            "Train Epoch: 43 [1024/14335 (7%)]\tLoss: 2982.954102\tLR: [8.80952380952381e-06]\n",
            "Train Epoch: 43 [1280/14335 (9%)]\tLoss: 2997.714111\tLR: [8.78968253968254e-06]\n",
            "Train Epoch: 43 [1536/14335 (11%)]\tLoss: 3107.967285\tLR: [8.769841269841269e-06]\n",
            "Train Epoch: 43 [1792/14335 (12%)]\tLoss: 3013.299316\tLR: [8.75e-06]\n",
            "Train Epoch: 43 [2048/14335 (14%)]\tLoss: 3065.540771\tLR: [8.73015873015873e-06]\n",
            "Train Epoch: 43 [2304/14335 (16%)]\tLoss: 2857.681641\tLR: [8.71031746031746e-06]\n",
            "Train Epoch: 43 [2560/14335 (18%)]\tLoss: 3599.391357\tLR: [8.69047619047619e-06]\n",
            "Train Epoch: 43 [2816/14335 (20%)]\tLoss: 2924.430176\tLR: [8.67063492063492e-06]\n",
            "Train Epoch: 43 [3072/14335 (21%)]\tLoss: 2689.036865\tLR: [8.650793650793651e-06]\n",
            "Train Epoch: 43 [3328/14335 (23%)]\tLoss: 3049.274414\tLR: [8.630952380952381e-06]\n",
            "Train Epoch: 43 [3584/14335 (25%)]\tLoss: 3068.238770\tLR: [8.611111111111112e-06]\n",
            "Train Epoch: 43 [3840/14335 (27%)]\tLoss: 2694.497314\tLR: [8.591269841269842e-06]\n",
            "Train Epoch: 43 [4096/14335 (29%)]\tLoss: 3237.269775\tLR: [8.571428571428573e-06]\n",
            "Train Epoch: 43 [4352/14335 (30%)]\tLoss: 3212.940918\tLR: [8.551587301587303e-06]\n",
            "Train Epoch: 43 [4608/14335 (32%)]\tLoss: 3149.349854\tLR: [8.531746031746032e-06]\n",
            "Train Epoch: 43 [4864/14335 (34%)]\tLoss: 3106.954590\tLR: [8.511904761904762e-06]\n",
            "Train Epoch: 43 [5120/14335 (36%)]\tLoss: 2552.591797\tLR: [8.492063492063492e-06]\n",
            "Train Epoch: 43 [5376/14335 (38%)]\tLoss: 2881.470947\tLR: [8.472222222222223e-06]\n",
            "Train Epoch: 43 [5632/14335 (39%)]\tLoss: 3399.399170\tLR: [8.452380952380953e-06]\n",
            "Train Epoch: 43 [5888/14335 (41%)]\tLoss: 3053.130615\tLR: [8.432539682539684e-06]\n",
            "Train Epoch: 43 [6144/14335 (43%)]\tLoss: 2878.424072\tLR: [8.412698412698414e-06]\n",
            "Train Epoch: 43 [6400/14335 (45%)]\tLoss: 2970.084229\tLR: [8.392857142857143e-06]\n",
            "Train Epoch: 43 [6656/14335 (46%)]\tLoss: 3302.082031\tLR: [8.373015873015873e-06]\n",
            "Train Epoch: 43 [6912/14335 (48%)]\tLoss: 3219.030029\tLR: [8.353174603174603e-06]\n",
            "Train Epoch: 43 [7168/14335 (50%)]\tLoss: 3063.899902\tLR: [8.333333333333334e-06]\n",
            "Train Epoch: 43 [7424/14335 (52%)]\tLoss: 2850.008545\tLR: [8.313492063492064e-06]\n",
            "Train Epoch: 43 [7680/14335 (54%)]\tLoss: 3117.826660\tLR: [8.293650793650794e-06]\n",
            "Train Epoch: 43 [7936/14335 (55%)]\tLoss: 3361.854736\tLR: [8.273809523809523e-06]\n",
            "Train Epoch: 43 [8192/14335 (57%)]\tLoss: 2959.236084\tLR: [8.253968253968254e-06]\n",
            "Train Epoch: 43 [8448/14335 (59%)]\tLoss: 3093.642334\tLR: [8.234126984126984e-06]\n",
            "Train Epoch: 43 [8704/14335 (61%)]\tLoss: 2917.604004\tLR: [8.214285714285714e-06]\n",
            "Train Epoch: 43 [8960/14335 (62%)]\tLoss: 2833.994629\tLR: [8.194444444444445e-06]\n",
            "Train Epoch: 43 [9216/14335 (64%)]\tLoss: 2742.607422\tLR: [8.174603174603175e-06]\n",
            "Train Epoch: 43 [9472/14335 (66%)]\tLoss: 2932.340576\tLR: [8.154761904761905e-06]\n",
            "Train Epoch: 43 [9728/14335 (68%)]\tLoss: 2386.555176\tLR: [8.134920634920636e-06]\n",
            "Train Epoch: 43 [9984/14335 (70%)]\tLoss: 3187.665771\tLR: [8.115079365079366e-06]\n",
            "Train Epoch: 43 [10240/14335 (71%)]\tLoss: 3011.636963\tLR: [8.095238095238097e-06]\n",
            "Train Epoch: 43 [10496/14335 (73%)]\tLoss: 2829.517090\tLR: [8.075396825396827e-06]\n",
            "Train Epoch: 43 [10752/14335 (75%)]\tLoss: 2765.057129\tLR: [8.055555555555557e-06]\n",
            "Train Epoch: 43 [11008/14335 (77%)]\tLoss: 3297.834961\tLR: [8.035714285714286e-06]\n",
            "Train Epoch: 43 [11264/14335 (79%)]\tLoss: 3212.947754\tLR: [8.015873015873016e-06]\n",
            "Train Epoch: 43 [11520/14335 (80%)]\tLoss: 3013.076416\tLR: [7.996031746031745e-06]\n",
            "Train Epoch: 43 [11776/14335 (82%)]\tLoss: 3068.558594\tLR: [7.976190476190475e-06]\n",
            "Train Epoch: 43 [12032/14335 (84%)]\tLoss: 2604.252441\tLR: [7.956349206349206e-06]\n",
            "Train Epoch: 43 [12288/14335 (86%)]\tLoss: 2743.268311\tLR: [7.936507936507936e-06]\n",
            "Train Epoch: 43 [12544/14335 (88%)]\tLoss: 2892.059326\tLR: [7.916666666666667e-06]\n",
            "Train Epoch: 43 [12800/14335 (89%)]\tLoss: 3253.608887\tLR: [7.896825396825397e-06]\n",
            "Train Epoch: 43 [13056/14335 (91%)]\tLoss: 2987.146484\tLR: [7.876984126984127e-06]\n",
            "Train Epoch: 43 [13312/14335 (93%)]\tLoss: 3191.762695\tLR: [7.857142857142858e-06]\n",
            "Train Epoch: 43 [13568/14335 (95%)]\tLoss: 2906.672363\tLR: [7.837301587301588e-06]\n",
            "Train Epoch: 43 [13824/14335 (96%)]\tLoss: 2537.857422\tLR: [7.817460317460318e-06]\n",
            "Train Epoch: 43 [14025/14335 (98%)]\tLoss: 3234.134766\tLR: [7.797619047619049e-06]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 43 is 0.9620512820512821, Best ROC Score is:0.9757894736842105\n",
            "Starting Epoch 44\n",
            "Train Epoch: 44 [0/14335 (0%)]\tLoss: 2978.466797\tLR: [7.777777777777777e-06]\n",
            "Train Epoch: 44 [256/14335 (2%)]\tLoss: 3287.820312\tLR: [7.757936507936508e-06]\n",
            "Train Epoch: 44 [512/14335 (4%)]\tLoss: 3382.188477\tLR: [7.738095238095238e-06]\n",
            "Train Epoch: 44 [768/14335 (5%)]\tLoss: 3319.164551\tLR: [7.718253968253969e-06]\n",
            "Train Epoch: 44 [1024/14335 (7%)]\tLoss: 3227.399658\tLR: [7.698412698412699e-06]\n",
            "Train Epoch: 44 [1280/14335 (9%)]\tLoss: 3078.668213\tLR: [7.67857142857143e-06]\n",
            "Train Epoch: 44 [1536/14335 (11%)]\tLoss: 2790.591309\tLR: [7.65873015873016e-06]\n",
            "Train Epoch: 44 [1792/14335 (12%)]\tLoss: 2845.705078\tLR: [7.63888888888889e-06]\n",
            "Train Epoch: 44 [2048/14335 (14%)]\tLoss: 2878.688965\tLR: [7.6190476190476205e-06]\n",
            "Train Epoch: 44 [2304/14335 (16%)]\tLoss: 3039.488525\tLR: [7.599206349206349e-06]\n",
            "Train Epoch: 44 [2560/14335 (18%)]\tLoss: 3141.072021\tLR: [7.5793650793650795e-06]\n",
            "Train Epoch: 44 [2816/14335 (20%)]\tLoss: 2830.392334\tLR: [7.559523809523809e-06]\n",
            "Train Epoch: 44 [3072/14335 (21%)]\tLoss: 3123.330566\tLR: [7.5396825396825394e-06]\n",
            "Train Epoch: 44 [3328/14335 (23%)]\tLoss: 3042.286377\tLR: [7.51984126984127e-06]\n",
            "Train Epoch: 44 [3584/14335 (25%)]\tLoss: 2688.749512\tLR: [7.5e-06]\n",
            "Train Epoch: 44 [3840/14335 (27%)]\tLoss: 3251.719482\tLR: [7.4801587301587306e-06]\n",
            "Train Epoch: 44 [4096/14335 (29%)]\tLoss: 2580.552002\tLR: [7.460317460317461e-06]\n",
            "Train Epoch: 44 [4352/14335 (30%)]\tLoss: 2350.148682\tLR: [7.4404761904761905e-06]\n",
            "Train Epoch: 44 [4608/14335 (32%)]\tLoss: 3072.865967\tLR: [7.420634920634921e-06]\n",
            "Train Epoch: 44 [4864/14335 (34%)]\tLoss: 3025.304688\tLR: [7.400793650793651e-06]\n",
            "Train Epoch: 44 [5120/14335 (36%)]\tLoss: 2933.364990\tLR: [7.380952380952382e-06]\n",
            "Train Epoch: 44 [5376/14335 (38%)]\tLoss: 2890.197510\tLR: [7.361111111111112e-06]\n",
            "Train Epoch: 44 [5632/14335 (39%)]\tLoss: 3208.244385\tLR: [7.3412698412698415e-06]\n",
            "Train Epoch: 44 [5888/14335 (41%)]\tLoss: 2734.592285\tLR: [7.321428571428572e-06]\n",
            "Train Epoch: 44 [6144/14335 (43%)]\tLoss: 2866.129395\tLR: [7.301587301587302e-06]\n",
            "Train Epoch: 44 [6400/14335 (45%)]\tLoss: 3411.495850\tLR: [7.281746031746033e-06]\n",
            "Train Epoch: 44 [6656/14335 (46%)]\tLoss: 2967.675293\tLR: [7.261904761904763e-06]\n",
            "Train Epoch: 44 [6912/14335 (48%)]\tLoss: 3058.192139\tLR: [7.242063492063493e-06]\n",
            "Train Epoch: 44 [7168/14335 (50%)]\tLoss: 3220.190430\tLR: [7.222222222222222e-06]\n",
            "Train Epoch: 44 [7424/14335 (52%)]\tLoss: 3096.747559\tLR: [7.2023809523809524e-06]\n",
            "Train Epoch: 44 [7680/14335 (54%)]\tLoss: 3382.853027\tLR: [7.182539682539682e-06]\n",
            "Train Epoch: 44 [7936/14335 (55%)]\tLoss: 2431.715820\tLR: [7.162698412698412e-06]\n",
            "Train Epoch: 44 [8192/14335 (57%)]\tLoss: 2799.486816\tLR: [7.142857142857143e-06]\n",
            "Train Epoch: 44 [8448/14335 (59%)]\tLoss: 3180.251465\tLR: [7.123015873015873e-06]\n",
            "Train Epoch: 44 [8704/14335 (61%)]\tLoss: 2631.689453\tLR: [7.1031746031746035e-06]\n",
            "Train Epoch: 44 [8960/14335 (62%)]\tLoss: 3058.245605\tLR: [7.083333333333334e-06]\n",
            "Train Epoch: 44 [9216/14335 (64%)]\tLoss: 2971.347900\tLR: [7.063492063492063e-06]\n",
            "Train Epoch: 44 [9472/14335 (66%)]\tLoss: 3296.315186\tLR: [7.043650793650794e-06]\n",
            "Train Epoch: 44 [9728/14335 (68%)]\tLoss: 3381.716553\tLR: [7.023809523809524e-06]\n",
            "Train Epoch: 44 [9984/14335 (70%)]\tLoss: 2781.843506\tLR: [7.0039682539682545e-06]\n",
            "Train Epoch: 44 [10240/14335 (71%)]\tLoss: 3121.165039\tLR: [6.984126984126985e-06]\n",
            "Train Epoch: 44 [10496/14335 (73%)]\tLoss: 3085.182861\tLR: [6.964285714285715e-06]\n",
            "Train Epoch: 44 [10752/14335 (75%)]\tLoss: 2763.422119\tLR: [6.944444444444445e-06]\n",
            "Train Epoch: 44 [11008/14335 (77%)]\tLoss: 2921.630371\tLR: [6.924603174603175e-06]\n",
            "Train Epoch: 44 [11264/14335 (79%)]\tLoss: 2933.113037\tLR: [6.9047619047619055e-06]\n",
            "Train Epoch: 44 [11520/14335 (80%)]\tLoss: 2920.392822\tLR: [6.884920634920636e-06]\n",
            "Train Epoch: 44 [11776/14335 (82%)]\tLoss: 3183.115723\tLR: [6.865079365079366e-06]\n",
            "Train Epoch: 44 [12032/14335 (84%)]\tLoss: 3182.864502\tLR: [6.845238095238096e-06]\n",
            "Train Epoch: 44 [12288/14335 (86%)]\tLoss: 2997.635498\tLR: [6.825396825396825e-06]\n",
            "Train Epoch: 44 [12544/14335 (88%)]\tLoss: 3051.112793\tLR: [6.805555555555556e-06]\n",
            "Train Epoch: 44 [12800/14335 (89%)]\tLoss: 2854.504395\tLR: [6.785714285714285e-06]\n",
            "Train Epoch: 44 [13056/14335 (91%)]\tLoss: 2102.752441\tLR: [6.765873015873016e-06]\n",
            "Train Epoch: 44 [13312/14335 (93%)]\tLoss: 3032.318359\tLR: [6.746031746031746e-06]\n",
            "Train Epoch: 44 [13568/14335 (95%)]\tLoss: 3193.862061\tLR: [6.726190476190476e-06]\n",
            "Train Epoch: 44 [13824/14335 (96%)]\tLoss: 2859.482422\tLR: [6.706349206349207e-06]\n",
            "Train Epoch: 44 [14025/14335 (98%)]\tLoss: 3184.809570\tLR: [6.686507936507936e-06]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 44 is 0.9089473684210526, Best ROC Score is:0.9757894736842105\n",
            "Starting Epoch 45\n",
            "Train Epoch: 45 [0/14335 (0%)]\tLoss: 3095.009766\tLR: [6.666666666666667e-06]\n",
            "Train Epoch: 45 [256/14335 (2%)]\tLoss: 2747.316650\tLR: [6.646825396825397e-06]\n",
            "Train Epoch: 45 [512/14335 (4%)]\tLoss: 3186.124268\tLR: [6.626984126984127e-06]\n",
            "Train Epoch: 45 [768/14335 (5%)]\tLoss: 2703.894531\tLR: [6.607142857142858e-06]\n",
            "Train Epoch: 45 [1024/14335 (7%)]\tLoss: 3175.974365\tLR: [6.587301587301588e-06]\n",
            "Train Epoch: 45 [1280/14335 (9%)]\tLoss: 3196.919189\tLR: [6.567460317460318e-06]\n",
            "Train Epoch: 45 [1536/14335 (11%)]\tLoss: 2841.429932\tLR: [6.547619047619048e-06]\n",
            "Train Epoch: 45 [1792/14335 (12%)]\tLoss: 2963.616455\tLR: [6.5277777777777784e-06]\n",
            "Train Epoch: 45 [2048/14335 (14%)]\tLoss: 3195.769775\tLR: [6.507936507936509e-06]\n",
            "Train Epoch: 45 [2304/14335 (16%)]\tLoss: 2578.227783\tLR: [6.488095238095239e-06]\n",
            "Train Epoch: 45 [2560/14335 (18%)]\tLoss: 2954.940674\tLR: [6.4682539682539696e-06]\n",
            "Train Epoch: 45 [2816/14335 (20%)]\tLoss: 3125.991699\tLR: [6.448412698412699e-06]\n",
            "Train Epoch: 45 [3072/14335 (21%)]\tLoss: 3000.745117\tLR: [6.428571428571429e-06]\n",
            "Train Epoch: 45 [3328/14335 (23%)]\tLoss: 3152.303223\tLR: [6.408730158730158e-06]\n",
            "Train Epoch: 45 [3584/14335 (25%)]\tLoss: 2916.871826\tLR: [6.3888888888888885e-06]\n",
            "Train Epoch: 45 [3840/14335 (27%)]\tLoss: 2957.356201\tLR: [6.369047619047619e-06]\n",
            "Train Epoch: 45 [4096/14335 (29%)]\tLoss: 3415.674805\tLR: [6.349206349206349e-06]\n",
            "Train Epoch: 45 [4352/14335 (30%)]\tLoss: 3270.598145\tLR: [6.32936507936508e-06]\n",
            "Train Epoch: 45 [4608/14335 (32%)]\tLoss: 2920.225098\tLR: [6.30952380952381e-06]\n",
            "Train Epoch: 45 [4864/14335 (34%)]\tLoss: 3216.971924\tLR: [6.2896825396825395e-06]\n",
            "Train Epoch: 45 [5120/14335 (36%)]\tLoss: 3129.223389\tLR: [6.26984126984127e-06]\n",
            "Train Epoch: 45 [5376/14335 (38%)]\tLoss: 2770.684082\tLR: [6.25e-06]\n",
            "Train Epoch: 45 [5632/14335 (39%)]\tLoss: 3134.215576\tLR: [6.230158730158731e-06]\n",
            "Train Epoch: 45 [5888/14335 (41%)]\tLoss: 2919.918945\tLR: [6.210317460317461e-06]\n",
            "Train Epoch: 45 [6144/14335 (43%)]\tLoss: 2904.807861\tLR: [6.190476190476191e-06]\n",
            "Train Epoch: 45 [6400/14335 (45%)]\tLoss: 2772.196045\tLR: [6.170634920634921e-06]\n",
            "Train Epoch: 45 [6656/14335 (46%)]\tLoss: 2894.245850\tLR: [6.1507936507936505e-06]\n",
            "Train Epoch: 45 [6912/14335 (48%)]\tLoss: 3135.884277\tLR: [6.130952380952381e-06]\n",
            "Train Epoch: 45 [7168/14335 (50%)]\tLoss: 3184.183350\tLR: [6.111111111111111e-06]\n",
            "Train Epoch: 45 [7424/14335 (52%)]\tLoss: 3338.410400\tLR: [6.091269841269842e-06]\n",
            "Train Epoch: 45 [7680/14335 (54%)]\tLoss: 2748.584473\tLR: [6.071428571428572e-06]\n",
            "Train Epoch: 45 [7936/14335 (55%)]\tLoss: 3067.595215\tLR: [6.0515873015873015e-06]\n",
            "Train Epoch: 45 [8192/14335 (57%)]\tLoss: 3142.884033\tLR: [6.031746031746032e-06]\n",
            "Train Epoch: 45 [8448/14335 (59%)]\tLoss: 3039.155273\tLR: [6.011904761904762e-06]\n",
            "Train Epoch: 45 [8704/14335 (61%)]\tLoss: 3051.557861\tLR: [5.992063492063493e-06]\n",
            "Train Epoch: 45 [8960/14335 (62%)]\tLoss: 3331.483398\tLR: [5.972222222222223e-06]\n",
            "Train Epoch: 45 [9216/14335 (64%)]\tLoss: 3276.962402\tLR: [5.9523809523809525e-06]\n",
            "Train Epoch: 45 [9472/14335 (66%)]\tLoss: 2483.160400\tLR: [5.932539682539683e-06]\n",
            "Train Epoch: 45 [9728/14335 (68%)]\tLoss: 3196.152344\tLR: [5.9126984126984124e-06]\n",
            "Train Epoch: 45 [9984/14335 (70%)]\tLoss: 3270.489746\tLR: [5.892857142857143e-06]\n",
            "Train Epoch: 45 [10240/14335 (71%)]\tLoss: 2940.055664\tLR: [5.873015873015873e-06]\n",
            "Train Epoch: 45 [10496/14335 (73%)]\tLoss: 2677.305664\tLR: [5.8531746031746036e-06]\n",
            "Train Epoch: 45 [10752/14335 (75%)]\tLoss: 3198.266602\tLR: [5.833333333333334e-06]\n",
            "Train Epoch: 45 [11008/14335 (77%)]\tLoss: 3043.039551\tLR: [5.813492063492064e-06]\n",
            "Train Epoch: 45 [11264/14335 (79%)]\tLoss: 2947.531250\tLR: [5.793650793650794e-06]\n",
            "Train Epoch: 45 [11520/14335 (80%)]\tLoss: 2991.798096\tLR: [5.773809523809524e-06]\n",
            "Train Epoch: 45 [11776/14335 (82%)]\tLoss: 3190.260498\tLR: [5.753968253968254e-06]\n",
            "Train Epoch: 45 [12032/14335 (84%)]\tLoss: 2695.941162\tLR: [5.734126984126984e-06]\n",
            "Train Epoch: 45 [12288/14335 (86%)]\tLoss: 2955.610840\tLR: [5.7142857142857145e-06]\n",
            "Train Epoch: 45 [12544/14335 (88%)]\tLoss: 2871.689209\tLR: [5.694444444444445e-06]\n",
            "Train Epoch: 45 [12800/14335 (89%)]\tLoss: 2785.817139\tLR: [5.674603174603174e-06]\n",
            "Train Epoch: 45 [13056/14335 (91%)]\tLoss: 3333.442383\tLR: [5.654761904761905e-06]\n",
            "Train Epoch: 45 [13312/14335 (93%)]\tLoss: 2651.664307\tLR: [5.634920634920635e-06]\n",
            "Train Epoch: 45 [13568/14335 (95%)]\tLoss: 3112.553711\tLR: [5.6150793650793655e-06]\n",
            "Train Epoch: 45 [13824/14335 (96%)]\tLoss: 2699.042236\tLR: [5.595238095238096e-06]\n",
            "Train Epoch: 45 [14025/14335 (98%)]\tLoss: 2839.909180\tLR: [5.575396825396826e-06]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 45 is 0.9216867469879518, Best ROC Score is:0.9757894736842105\n",
            "Starting Epoch 46\n",
            "Train Epoch: 46 [0/14335 (0%)]\tLoss: 3032.462402\tLR: [5.555555555555556e-06]\n",
            "Train Epoch: 46 [256/14335 (2%)]\tLoss: 2828.682617\tLR: [5.535714285714285e-06]\n",
            "Train Epoch: 46 [512/14335 (4%)]\tLoss: 2805.566162\tLR: [5.515873015873016e-06]\n",
            "Train Epoch: 46 [768/14335 (5%)]\tLoss: 2918.743652\tLR: [5.496031746031746e-06]\n",
            "Train Epoch: 46 [1024/14335 (7%)]\tLoss: 2927.022705\tLR: [5.4761904761904765e-06]\n",
            "Train Epoch: 46 [1280/14335 (9%)]\tLoss: 2659.067871\tLR: [5.456349206349207e-06]\n",
            "Train Epoch: 46 [1536/14335 (11%)]\tLoss: 3019.673584\tLR: [5.436507936507937e-06]\n",
            "Train Epoch: 46 [1792/14335 (12%)]\tLoss: 2937.604004\tLR: [5.416666666666667e-06]\n",
            "Train Epoch: 46 [2048/14335 (14%)]\tLoss: 2982.583008\tLR: [5.396825396825397e-06]\n",
            "Train Epoch: 46 [2304/14335 (16%)]\tLoss: 2963.152100\tLR: [5.3769841269841275e-06]\n",
            "Train Epoch: 46 [2560/14335 (18%)]\tLoss: 3011.083740\tLR: [5.357142857142857e-06]\n",
            "Train Epoch: 46 [2816/14335 (20%)]\tLoss: 2779.905762\tLR: [5.337301587301587e-06]\n",
            "Train Epoch: 46 [3072/14335 (21%)]\tLoss: 3044.519287\tLR: [5.317460317460318e-06]\n",
            "Train Epoch: 46 [3328/14335 (23%)]\tLoss: 3036.963867\tLR: [5.297619047619048e-06]\n",
            "Train Epoch: 46 [3584/14335 (25%)]\tLoss: 2883.470215\tLR: [5.277777777777778e-06]\n",
            "Train Epoch: 46 [3840/14335 (27%)]\tLoss: 3356.410889\tLR: [5.257936507936508e-06]\n",
            "Train Epoch: 46 [4096/14335 (29%)]\tLoss: 2845.681885\tLR: [5.2380952380952384e-06]\n",
            "Train Epoch: 46 [4352/14335 (30%)]\tLoss: 2843.854736\tLR: [5.218253968253969e-06]\n",
            "Train Epoch: 46 [4608/14335 (32%)]\tLoss: 2972.135498\tLR: [5.198412698412699e-06]\n",
            "Train Epoch: 46 [4864/14335 (34%)]\tLoss: 3057.197754\tLR: [5.1785714285714296e-06]\n",
            "Train Epoch: 46 [5120/14335 (36%)]\tLoss: 2850.517090\tLR: [5.158730158730159e-06]\n",
            "Train Epoch: 46 [5376/14335 (38%)]\tLoss: 2746.698242\tLR: [5.138888888888889e-06]\n",
            "Train Epoch: 46 [5632/14335 (39%)]\tLoss: 3083.995361\tLR: [5.119047619047619e-06]\n",
            "Train Epoch: 46 [5888/14335 (41%)]\tLoss: 3114.395020\tLR: [5.099206349206349e-06]\n",
            "Train Epoch: 46 [6144/14335 (43%)]\tLoss: 2862.603516\tLR: [5.07936507936508e-06]\n",
            "Train Epoch: 46 [6400/14335 (45%)]\tLoss: 3520.865967\tLR: [5.05952380952381e-06]\n",
            "Train Epoch: 46 [6656/14335 (46%)]\tLoss: 2983.271240\tLR: [5.03968253968254e-06]\n",
            "Train Epoch: 46 [6912/14335 (48%)]\tLoss: 3330.243164\tLR: [5.01984126984127e-06]\n",
            "Train Epoch: 46 [7168/14335 (50%)]\tLoss: 3362.218506\tLR: [5e-06]\n",
            "Train Epoch: 46 [7424/14335 (52%)]\tLoss: 3214.768311\tLR: [4.98015873015873e-06]\n",
            "Train Epoch: 46 [7680/14335 (54%)]\tLoss: 3029.782227\tLR: [4.96031746031746e-06]\n",
            "Train Epoch: 46 [7936/14335 (55%)]\tLoss: 3239.984131\tLR: [4.940476190476191e-06]\n",
            "Train Epoch: 46 [8192/14335 (57%)]\tLoss: 2770.272461\tLR: [4.920634920634921e-06]\n",
            "Train Epoch: 46 [8448/14335 (59%)]\tLoss: 2869.044434\tLR: [4.900793650793651e-06]\n",
            "Train Epoch: 46 [8704/14335 (61%)]\tLoss: 2985.516602\tLR: [4.880952380952381e-06]\n",
            "Train Epoch: 46 [8960/14335 (62%)]\tLoss: 2706.502441\tLR: [4.861111111111111e-06]\n",
            "Train Epoch: 46 [9216/14335 (64%)]\tLoss: 3011.718506\tLR: [4.841269841269842e-06]\n",
            "Train Epoch: 46 [9472/14335 (66%)]\tLoss: 2949.619141\tLR: [4.821428571428572e-06]\n",
            "Train Epoch: 46 [9728/14335 (68%)]\tLoss: 3253.187500\tLR: [4.8015873015873025e-06]\n",
            "Train Epoch: 46 [9984/14335 (70%)]\tLoss: 2892.819092\tLR: [4.781746031746032e-06]\n",
            "Train Epoch: 46 [10240/14335 (71%)]\tLoss: 3322.585938\tLR: [4.7619047619047615e-06]\n",
            "Train Epoch: 46 [10496/14335 (73%)]\tLoss: 3141.639404\tLR: [4.742063492063492e-06]\n",
            "Train Epoch: 46 [10752/14335 (75%)]\tLoss: 2630.433838\tLR: [4.722222222222222e-06]\n",
            "Train Epoch: 46 [11008/14335 (77%)]\tLoss: 2578.797607\tLR: [4.702380952380953e-06]\n",
            "Train Epoch: 46 [11264/14335 (79%)]\tLoss: 2985.966553\tLR: [4.682539682539683e-06]\n",
            "Train Epoch: 46 [11520/14335 (80%)]\tLoss: 3163.212891\tLR: [4.662698412698413e-06]\n",
            "Train Epoch: 46 [11776/14335 (82%)]\tLoss: 2855.140625\tLR: [4.642857142857143e-06]\n",
            "Train Epoch: 46 [12032/14335 (84%)]\tLoss: 2507.157471\tLR: [4.623015873015873e-06]\n",
            "Train Epoch: 46 [12288/14335 (86%)]\tLoss: 2948.437988\tLR: [4.603174603174604e-06]\n",
            "Train Epoch: 46 [12544/14335 (88%)]\tLoss: 2997.218750\tLR: [4.583333333333333e-06]\n",
            "Train Epoch: 46 [12800/14335 (89%)]\tLoss: 3125.528564\tLR: [4.563492063492064e-06]\n",
            "Train Epoch: 46 [13056/14335 (91%)]\tLoss: 3155.760986\tLR: [4.543650793650794e-06]\n",
            "Train Epoch: 46 [13312/14335 (93%)]\tLoss: 2970.752197\tLR: [4.5238095238095235e-06]\n",
            "Train Epoch: 46 [13568/14335 (95%)]\tLoss: 3566.493164\tLR: [4.503968253968254e-06]\n",
            "Train Epoch: 46 [13824/14335 (96%)]\tLoss: 3096.628662\tLR: [4.484126984126984e-06]\n",
            "Train Epoch: 46 [14025/14335 (98%)]\tLoss: 3135.727295\tLR: [4.464285714285715e-06]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 46 is 0.9512987012987013, Best ROC Score is:0.9757894736842105\n",
            "Starting Epoch 47\n",
            "Train Epoch: 47 [0/14335 (0%)]\tLoss: 3191.685791\tLR: [4.444444444444445e-06]\n",
            "Train Epoch: 47 [256/14335 (2%)]\tLoss: 3234.605713\tLR: [4.424603174603175e-06]\n",
            "Train Epoch: 47 [512/14335 (4%)]\tLoss: 3239.944824\tLR: [4.404761904761905e-06]\n",
            "Train Epoch: 47 [768/14335 (5%)]\tLoss: 2967.533936\tLR: [4.3849206349206344e-06]\n",
            "Train Epoch: 47 [1024/14335 (7%)]\tLoss: 3311.223877\tLR: [4.365079365079365e-06]\n",
            "Train Epoch: 47 [1280/14335 (9%)]\tLoss: 2943.166992\tLR: [4.345238095238095e-06]\n",
            "Train Epoch: 47 [1536/14335 (11%)]\tLoss: 3057.445801\tLR: [4.3253968253968256e-06]\n",
            "Train Epoch: 47 [1792/14335 (12%)]\tLoss: 2701.158691\tLR: [4.305555555555556e-06]\n",
            "Train Epoch: 47 [2048/14335 (14%)]\tLoss: 2860.071045\tLR: [4.285714285714286e-06]\n",
            "Train Epoch: 47 [2304/14335 (16%)]\tLoss: 3078.244873\tLR: [4.265873015873016e-06]\n",
            "Train Epoch: 47 [2560/14335 (18%)]\tLoss: 2864.392578\tLR: [4.246031746031746e-06]\n",
            "Train Epoch: 47 [2816/14335 (20%)]\tLoss: 3285.445557\tLR: [4.226190476190477e-06]\n",
            "Train Epoch: 47 [3072/14335 (21%)]\tLoss: 2870.540039\tLR: [4.206349206349207e-06]\n",
            "Train Epoch: 47 [3328/14335 (23%)]\tLoss: 3030.444336\tLR: [4.1865079365079365e-06]\n",
            "Train Epoch: 47 [3584/14335 (25%)]\tLoss: 3240.528320\tLR: [4.166666666666667e-06]\n",
            "Train Epoch: 47 [3840/14335 (27%)]\tLoss: 2972.783936\tLR: [4.146825396825397e-06]\n",
            "Train Epoch: 47 [4096/14335 (29%)]\tLoss: 3119.928955\tLR: [4.126984126984127e-06]\n",
            "Train Epoch: 47 [4352/14335 (30%)]\tLoss: 3075.662109\tLR: [4.107142857142857e-06]\n",
            "Train Epoch: 47 [4608/14335 (32%)]\tLoss: 2796.310303\tLR: [4.0873015873015875e-06]\n",
            "Train Epoch: 47 [4864/14335 (34%)]\tLoss: 2559.377686\tLR: [4.067460317460318e-06]\n",
            "Train Epoch: 47 [5120/14335 (36%)]\tLoss: 2689.662598\tLR: [4.047619047619048e-06]\n",
            "Train Epoch: 47 [5376/14335 (38%)]\tLoss: 2711.301758\tLR: [4.027777777777779e-06]\n",
            "Train Epoch: 47 [5632/14335 (39%)]\tLoss: 2846.969482\tLR: [4.007936507936508e-06]\n",
            "Train Epoch: 47 [5888/14335 (41%)]\tLoss: 2862.259033\tLR: [3.988095238095238e-06]\n",
            "Train Epoch: 47 [6144/14335 (43%)]\tLoss: 2897.215088\tLR: [3.968253968253968e-06]\n",
            "Train Epoch: 47 [6400/14335 (45%)]\tLoss: 2767.415527\tLR: [3.9484126984126985e-06]\n",
            "Train Epoch: 47 [6656/14335 (46%)]\tLoss: 3170.238037\tLR: [3.928571428571429e-06]\n",
            "Train Epoch: 47 [6912/14335 (48%)]\tLoss: 2866.114258\tLR: [3.908730158730159e-06]\n",
            "Train Epoch: 47 [7168/14335 (50%)]\tLoss: 3114.919189\tLR: [3.888888888888889e-06]\n",
            "Train Epoch: 47 [7424/14335 (52%)]\tLoss: 2788.926270\tLR: [3.869047619047619e-06]\n",
            "Train Epoch: 47 [7680/14335 (54%)]\tLoss: 3129.815186\tLR: [3.8492063492063495e-06]\n",
            "Train Epoch: 47 [7936/14335 (55%)]\tLoss: 3195.766357\tLR: [3.82936507936508e-06]\n",
            "Train Epoch: 47 [8192/14335 (57%)]\tLoss: 2810.033691\tLR: [3.8095238095238102e-06]\n",
            "Train Epoch: 47 [8448/14335 (59%)]\tLoss: 3004.336426\tLR: [3.7896825396825398e-06]\n",
            "Train Epoch: 47 [8704/14335 (61%)]\tLoss: 3200.971924\tLR: [3.7698412698412697e-06]\n",
            "Train Epoch: 47 [8960/14335 (62%)]\tLoss: 2964.686768\tLR: [3.75e-06]\n",
            "Train Epoch: 47 [9216/14335 (64%)]\tLoss: 2967.925781\tLR: [3.7301587301587305e-06]\n",
            "Train Epoch: 47 [9472/14335 (66%)]\tLoss: 3261.947266\tLR: [3.7103174603174604e-06]\n",
            "Train Epoch: 47 [9728/14335 (68%)]\tLoss: 3218.130127\tLR: [3.690476190476191e-06]\n",
            "Train Epoch: 47 [9984/14335 (70%)]\tLoss: 2882.563232\tLR: [3.6706349206349208e-06]\n",
            "Train Epoch: 47 [10240/14335 (71%)]\tLoss: 2982.850098\tLR: [3.650793650793651e-06]\n",
            "Train Epoch: 47 [10496/14335 (73%)]\tLoss: 2660.244141\tLR: [3.6309523809523815e-06]\n",
            "Train Epoch: 47 [10752/14335 (75%)]\tLoss: 3329.185059\tLR: [3.611111111111111e-06]\n",
            "Train Epoch: 47 [11008/14335 (77%)]\tLoss: 3141.318604\tLR: [3.591269841269841e-06]\n",
            "Train Epoch: 47 [11264/14335 (79%)]\tLoss: 3020.599854\tLR: [3.5714285714285714e-06]\n",
            "Train Epoch: 47 [11520/14335 (80%)]\tLoss: 2738.417480\tLR: [3.5515873015873017e-06]\n",
            "Train Epoch: 47 [11776/14335 (82%)]\tLoss: 2545.346436\tLR: [3.5317460317460317e-06]\n",
            "Train Epoch: 47 [12032/14335 (84%)]\tLoss: 3265.569824\tLR: [3.511904761904762e-06]\n",
            "Train Epoch: 47 [12288/14335 (86%)]\tLoss: 2906.049316\tLR: [3.4920634920634924e-06]\n",
            "Train Epoch: 47 [12544/14335 (88%)]\tLoss: 3116.203125\tLR: [3.4722222222222224e-06]\n",
            "Train Epoch: 47 [12800/14335 (89%)]\tLoss: 2971.888916\tLR: [3.4523809523809528e-06]\n",
            "Train Epoch: 47 [13056/14335 (91%)]\tLoss: 2861.063232\tLR: [3.432539682539683e-06]\n",
            "Train Epoch: 47 [13312/14335 (93%)]\tLoss: 2746.698242\tLR: [3.4126984126984127e-06]\n",
            "Train Epoch: 47 [13568/14335 (95%)]\tLoss: 2910.856201\tLR: [3.3928571428571426e-06]\n",
            "Train Epoch: 47 [13824/14335 (96%)]\tLoss: 3379.151855\tLR: [3.373015873015873e-06]\n",
            "Train Epoch: 47 [14025/14335 (98%)]\tLoss: 3001.221436\tLR: [3.3531746031746034e-06]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 47 is 0.9671361502347419, Best ROC Score is:0.9757894736842105\n",
            "Starting Epoch 48\n",
            "Train Epoch: 48 [0/14335 (0%)]\tLoss: 3753.629883\tLR: [3.3333333333333333e-06]\n",
            "Train Epoch: 48 [256/14335 (2%)]\tLoss: 3053.981445\tLR: [3.3134920634920637e-06]\n",
            "Train Epoch: 48 [512/14335 (4%)]\tLoss: 2576.368896\tLR: [3.293650793650794e-06]\n",
            "Train Epoch: 48 [768/14335 (5%)]\tLoss: 2968.360596\tLR: [3.273809523809524e-06]\n",
            "Train Epoch: 48 [1024/14335 (7%)]\tLoss: 2803.120117\tLR: [3.2539682539682544e-06]\n",
            "Train Epoch: 48 [1280/14335 (9%)]\tLoss: 2875.190186\tLR: [3.2341269841269848e-06]\n",
            "Train Epoch: 48 [1536/14335 (11%)]\tLoss: 3278.472168\tLR: [3.2142857142857143e-06]\n",
            "Train Epoch: 48 [1792/14335 (12%)]\tLoss: 3143.759521\tLR: [3.1944444444444443e-06]\n",
            "Train Epoch: 48 [2048/14335 (14%)]\tLoss: 3129.484375\tLR: [3.1746031746031746e-06]\n",
            "Train Epoch: 48 [2304/14335 (16%)]\tLoss: 2965.804443\tLR: [3.154761904761905e-06]\n",
            "Train Epoch: 48 [2560/14335 (18%)]\tLoss: 2568.460938\tLR: [3.134920634920635e-06]\n",
            "Train Epoch: 48 [2816/14335 (20%)]\tLoss: 2816.541260\tLR: [3.1150793650793653e-06]\n",
            "Train Epoch: 48 [3072/14335 (21%)]\tLoss: 3021.348145\tLR: [3.0952380952380953e-06]\n",
            "Train Epoch: 48 [3328/14335 (23%)]\tLoss: 2959.589111\tLR: [3.0753968253968252e-06]\n",
            "Train Epoch: 48 [3584/14335 (25%)]\tLoss: 2890.617188\tLR: [3.0555555555555556e-06]\n",
            "Train Epoch: 48 [3840/14335 (27%)]\tLoss: 2812.958984\tLR: [3.035714285714286e-06]\n",
            "Train Epoch: 48 [4096/14335 (29%)]\tLoss: 3458.951416\tLR: [3.015873015873016e-06]\n",
            "Train Epoch: 48 [4352/14335 (30%)]\tLoss: 3099.789795\tLR: [2.9960317460317463e-06]\n",
            "Train Epoch: 48 [4608/14335 (32%)]\tLoss: 3375.309814\tLR: [2.9761904761904763e-06]\n",
            "Train Epoch: 48 [4864/14335 (34%)]\tLoss: 2707.294678\tLR: [2.9563492063492062e-06]\n",
            "Train Epoch: 48 [5120/14335 (36%)]\tLoss: 2944.530762\tLR: [2.9365079365079366e-06]\n",
            "Train Epoch: 48 [5376/14335 (38%)]\tLoss: 2901.235596\tLR: [2.916666666666667e-06]\n",
            "Train Epoch: 48 [5632/14335 (39%)]\tLoss: 2987.013672\tLR: [2.896825396825397e-06]\n",
            "Train Epoch: 48 [5888/14335 (41%)]\tLoss: 3153.519287\tLR: [2.876984126984127e-06]\n",
            "Train Epoch: 48 [6144/14335 (43%)]\tLoss: 2813.690918\tLR: [2.8571428571428573e-06]\n",
            "Train Epoch: 48 [6400/14335 (45%)]\tLoss: 3063.879395\tLR: [2.837301587301587e-06]\n",
            "Train Epoch: 48 [6656/14335 (46%)]\tLoss: 3447.439697\tLR: [2.8174603174603176e-06]\n",
            "Train Epoch: 48 [6912/14335 (48%)]\tLoss: 2565.064697\tLR: [2.797619047619048e-06]\n",
            "Train Epoch: 48 [7168/14335 (50%)]\tLoss: 2800.523438\tLR: [2.777777777777778e-06]\n",
            "Train Epoch: 48 [7424/14335 (52%)]\tLoss: 3073.435303\tLR: [2.757936507936508e-06]\n",
            "Train Epoch: 48 [7680/14335 (54%)]\tLoss: 3245.289307\tLR: [2.7380952380952382e-06]\n",
            "Train Epoch: 48 [7936/14335 (55%)]\tLoss: 2998.952637\tLR: [2.7182539682539686e-06]\n",
            "Train Epoch: 48 [8192/14335 (57%)]\tLoss: 2746.714600\tLR: [2.6984126984126986e-06]\n",
            "Train Epoch: 48 [8448/14335 (59%)]\tLoss: 3052.300537\tLR: [2.6785714285714285e-06]\n",
            "Train Epoch: 48 [8704/14335 (61%)]\tLoss: 3450.261230\tLR: [2.658730158730159e-06]\n",
            "Train Epoch: 48 [8960/14335 (62%)]\tLoss: 2957.702148\tLR: [2.638888888888889e-06]\n",
            "Train Epoch: 48 [9216/14335 (64%)]\tLoss: 2922.532227\tLR: [2.6190476190476192e-06]\n",
            "Train Epoch: 48 [9472/14335 (66%)]\tLoss: 3015.531006\tLR: [2.5992063492063496e-06]\n",
            "Train Epoch: 48 [9728/14335 (68%)]\tLoss: 2953.087891\tLR: [2.5793650793650795e-06]\n",
            "Train Epoch: 48 [9984/14335 (70%)]\tLoss: 2984.791016\tLR: [2.5595238095238095e-06]\n",
            "Train Epoch: 48 [10240/14335 (71%)]\tLoss: 3119.839844\tLR: [2.53968253968254e-06]\n",
            "Train Epoch: 48 [10496/14335 (73%)]\tLoss: 3181.673584\tLR: [2.51984126984127e-06]\n",
            "Train Epoch: 48 [10752/14335 (75%)]\tLoss: 2921.575928\tLR: [2.5e-06]\n",
            "Train Epoch: 48 [11008/14335 (77%)]\tLoss: 2965.069092\tLR: [2.48015873015873e-06]\n",
            "Train Epoch: 48 [11264/14335 (79%)]\tLoss: 2981.035156\tLR: [2.4603174603174605e-06]\n",
            "Train Epoch: 48 [11520/14335 (80%)]\tLoss: 2843.838379\tLR: [2.4404761904761905e-06]\n",
            "Train Epoch: 48 [11776/14335 (82%)]\tLoss: 2741.319092\tLR: [2.420634920634921e-06]\n",
            "Train Epoch: 48 [12032/14335 (84%)]\tLoss: 3081.775146\tLR: [2.4007936507936512e-06]\n",
            "Train Epoch: 48 [12288/14335 (86%)]\tLoss: 3102.978760\tLR: [2.3809523809523808e-06]\n",
            "Train Epoch: 48 [12544/14335 (88%)]\tLoss: 2559.145020\tLR: [2.361111111111111e-06]\n",
            "Train Epoch: 48 [12800/14335 (89%)]\tLoss: 2835.520264\tLR: [2.3412698412698415e-06]\n",
            "Train Epoch: 48 [13056/14335 (91%)]\tLoss: 3278.220947\tLR: [2.3214285714285715e-06]\n",
            "Train Epoch: 48 [13312/14335 (93%)]\tLoss: 2927.328857\tLR: [2.301587301587302e-06]\n",
            "Train Epoch: 48 [13568/14335 (95%)]\tLoss: 2916.914062\tLR: [2.281746031746032e-06]\n",
            "Train Epoch: 48 [13824/14335 (96%)]\tLoss: 2685.879639\tLR: [2.2619047619047617e-06]\n",
            "Train Epoch: 48 [14025/14335 (98%)]\tLoss: 3077.532715\tLR: [2.242063492063492e-06]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 48 is 0.9425641025641026, Best ROC Score is:0.9757894736842105\n",
            "Starting Epoch 49\n",
            "Train Epoch: 49 [0/14335 (0%)]\tLoss: 2873.677002\tLR: [2.2222222222222225e-06]\n",
            "Train Epoch: 49 [256/14335 (2%)]\tLoss: 2981.285156\tLR: [2.2023809523809525e-06]\n",
            "Train Epoch: 49 [512/14335 (4%)]\tLoss: 2840.660889\tLR: [2.1825396825396824e-06]\n",
            "Train Epoch: 49 [768/14335 (5%)]\tLoss: 3151.946533\tLR: [2.1626984126984128e-06]\n",
            "Train Epoch: 49 [1024/14335 (7%)]\tLoss: 3109.149902\tLR: [2.142857142857143e-06]\n",
            "Train Epoch: 49 [1280/14335 (9%)]\tLoss: 3005.072266\tLR: [2.123015873015873e-06]\n",
            "Train Epoch: 49 [1536/14335 (11%)]\tLoss: 3456.110596\tLR: [2.1031746031746035e-06]\n",
            "Train Epoch: 49 [1792/14335 (12%)]\tLoss: 3359.378174\tLR: [2.0833333333333334e-06]\n",
            "Train Epoch: 49 [2048/14335 (14%)]\tLoss: 2962.467773\tLR: [2.0634920634920634e-06]\n",
            "Train Epoch: 49 [2304/14335 (16%)]\tLoss: 2706.623535\tLR: [2.0436507936507938e-06]\n",
            "Train Epoch: 49 [2560/14335 (18%)]\tLoss: 3040.555420\tLR: [2.023809523809524e-06]\n",
            "Train Epoch: 49 [2816/14335 (20%)]\tLoss: 2827.855713\tLR: [2.003968253968254e-06]\n",
            "Train Epoch: 49 [3072/14335 (21%)]\tLoss: 2873.178711\tLR: [1.984126984126984e-06]\n",
            "Train Epoch: 49 [3328/14335 (23%)]\tLoss: 3064.745605\tLR: [1.9642857142857144e-06]\n",
            "Train Epoch: 49 [3584/14335 (25%)]\tLoss: 3119.325928\tLR: [1.9444444444444444e-06]\n",
            "Train Epoch: 49 [3840/14335 (27%)]\tLoss: 3001.125000\tLR: [1.9246031746031747e-06]\n",
            "Train Epoch: 49 [4096/14335 (29%)]\tLoss: 2666.382568\tLR: [1.9047619047619051e-06]\n",
            "Train Epoch: 49 [4352/14335 (30%)]\tLoss: 2914.243408\tLR: [1.8849206349206349e-06]\n",
            "Train Epoch: 49 [4608/14335 (32%)]\tLoss: 2707.515625\tLR: [1.8650793650793652e-06]\n",
            "Train Epoch: 49 [4864/14335 (34%)]\tLoss: 2710.320801\tLR: [1.8452380952380954e-06]\n",
            "Train Epoch: 49 [5120/14335 (36%)]\tLoss: 3102.261475\tLR: [1.8253968253968256e-06]\n",
            "Train Epoch: 49 [5376/14335 (38%)]\tLoss: 3029.114746\tLR: [1.8055555555555555e-06]\n",
            "Train Epoch: 49 [5632/14335 (39%)]\tLoss: 2623.459229\tLR: [1.7857142857142857e-06]\n",
            "Train Epoch: 49 [5888/14335 (41%)]\tLoss: 3038.394531\tLR: [1.7658730158730158e-06]\n",
            "Train Epoch: 49 [6144/14335 (43%)]\tLoss: 2967.789307\tLR: [1.7460317460317462e-06]\n",
            "Train Epoch: 49 [6400/14335 (45%)]\tLoss: 2939.711914\tLR: [1.7261904761904764e-06]\n",
            "Train Epoch: 49 [6656/14335 (46%)]\tLoss: 3013.664551\tLR: [1.7063492063492063e-06]\n",
            "Train Epoch: 49 [6912/14335 (48%)]\tLoss: 2900.902832\tLR: [1.6865079365079365e-06]\n",
            "Train Epoch: 49 [7168/14335 (50%)]\tLoss: 3115.695801\tLR: [1.6666666666666667e-06]\n",
            "Train Epoch: 49 [7424/14335 (52%)]\tLoss: 2971.419189\tLR: [1.646825396825397e-06]\n",
            "Train Epoch: 49 [7680/14335 (54%)]\tLoss: 2761.607422\tLR: [1.6269841269841272e-06]\n",
            "Train Epoch: 49 [7936/14335 (55%)]\tLoss: 2916.148926\tLR: [1.6071428571428572e-06]\n",
            "Train Epoch: 49 [8192/14335 (57%)]\tLoss: 3066.961670\tLR: [1.5873015873015873e-06]\n",
            "Train Epoch: 49 [8448/14335 (59%)]\tLoss: 2834.887207\tLR: [1.5674603174603175e-06]\n",
            "Train Epoch: 49 [8704/14335 (61%)]\tLoss: 2974.062500\tLR: [1.5476190476190476e-06]\n",
            "Train Epoch: 49 [8960/14335 (62%)]\tLoss: 3267.212646\tLR: [1.5277777777777778e-06]\n",
            "Train Epoch: 49 [9216/14335 (64%)]\tLoss: 2977.651123\tLR: [1.507936507936508e-06]\n",
            "Train Epoch: 49 [9472/14335 (66%)]\tLoss: 2791.490723\tLR: [1.4880952380952381e-06]\n",
            "Train Epoch: 49 [9728/14335 (68%)]\tLoss: 3176.095459\tLR: [1.4682539682539683e-06]\n",
            "Train Epoch: 49 [9984/14335 (70%)]\tLoss: 2882.045898\tLR: [1.4484126984126985e-06]\n",
            "Train Epoch: 49 [10240/14335 (71%)]\tLoss: 2699.792969\tLR: [1.4285714285714286e-06]\n",
            "Train Epoch: 49 [10496/14335 (73%)]\tLoss: 3697.799072\tLR: [1.4087301587301588e-06]\n",
            "Train Epoch: 49 [10752/14335 (75%)]\tLoss: 3307.622559\tLR: [1.388888888888889e-06]\n",
            "Train Epoch: 49 [11008/14335 (77%)]\tLoss: 2733.715332\tLR: [1.3690476190476191e-06]\n",
            "Train Epoch: 49 [11264/14335 (79%)]\tLoss: 3134.468506\tLR: [1.3492063492063493e-06]\n",
            "Train Epoch: 49 [11520/14335 (80%)]\tLoss: 3378.032959\tLR: [1.3293650793650794e-06]\n",
            "Train Epoch: 49 [11776/14335 (82%)]\tLoss: 3169.046875\tLR: [1.3095238095238096e-06]\n",
            "Train Epoch: 49 [12032/14335 (84%)]\tLoss: 2952.297607\tLR: [1.2896825396825398e-06]\n",
            "Train Epoch: 49 [12288/14335 (86%)]\tLoss: 2762.401855\tLR: [1.26984126984127e-06]\n",
            "Train Epoch: 49 [12544/14335 (88%)]\tLoss: 3237.621338\tLR: [1.25e-06]\n",
            "Train Epoch: 49 [12800/14335 (89%)]\tLoss: 3237.314209\tLR: [1.2301587301587303e-06]\n",
            "Train Epoch: 49 [13056/14335 (91%)]\tLoss: 2736.069092\tLR: [1.2103174603174604e-06]\n",
            "Train Epoch: 49 [13312/14335 (93%)]\tLoss: 2871.859131\tLR: [1.1904761904761904e-06]\n",
            "Train Epoch: 49 [13568/14335 (95%)]\tLoss: 3220.870361\tLR: [1.1706349206349208e-06]\n",
            "Train Epoch: 49 [13824/14335 (96%)]\tLoss: 2691.757080\tLR: [1.150793650793651e-06]\n",
            "Train Epoch: 49 [14025/14335 (98%)]\tLoss: 2850.111328\tLR: [1.1309523809523809e-06]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 49 is 0.9691358024691357, Best ROC Score is:0.9757894736842105\n",
            "Starting Epoch 50\n",
            "Train Epoch: 50 [0/14335 (0%)]\tLoss: 3203.126953\tLR: [1.1111111111111112e-06]\n",
            "Train Epoch: 50 [256/14335 (2%)]\tLoss: 3016.132080\tLR: [1.0912698412698412e-06]\n",
            "Train Epoch: 50 [512/14335 (4%)]\tLoss: 3109.902100\tLR: [1.0714285714285716e-06]\n",
            "Train Epoch: 50 [768/14335 (5%)]\tLoss: 2987.556152\tLR: [1.0515873015873017e-06]\n",
            "Train Epoch: 50 [1024/14335 (7%)]\tLoss: 3209.833252\tLR: [1.0317460317460317e-06]\n",
            "Train Epoch: 50 [1280/14335 (9%)]\tLoss: 2796.253906\tLR: [1.011904761904762e-06]\n",
            "Train Epoch: 50 [1536/14335 (11%)]\tLoss: 3061.866455\tLR: [9.92063492063492e-07]\n",
            "Train Epoch: 50 [1792/14335 (12%)]\tLoss: 2623.363281\tLR: [9.722222222222222e-07]\n",
            "Train Epoch: 50 [2048/14335 (14%)]\tLoss: 3155.106689\tLR: [9.523809523809526e-07]\n",
            "Train Epoch: 50 [2304/14335 (16%)]\tLoss: 2907.234619\tLR: [9.325396825396826e-07]\n",
            "Train Epoch: 50 [2560/14335 (18%)]\tLoss: 3001.003906\tLR: [9.126984126984128e-07]\n",
            "Train Epoch: 50 [2816/14335 (20%)]\tLoss: 3161.270020\tLR: [8.928571428571428e-07]\n",
            "Train Epoch: 50 [3072/14335 (21%)]\tLoss: 2819.658203\tLR: [8.730158730158731e-07]\n",
            "Train Epoch: 50 [3328/14335 (23%)]\tLoss: 3109.621338\tLR: [8.531746031746032e-07]\n",
            "Train Epoch: 50 [3584/14335 (25%)]\tLoss: 2411.102051\tLR: [8.333333333333333e-07]\n",
            "Train Epoch: 50 [3840/14335 (27%)]\tLoss: 3143.438232\tLR: [8.134920634920636e-07]\n",
            "Train Epoch: 50 [4096/14335 (29%)]\tLoss: 3442.573242\tLR: [7.936507936507937e-07]\n",
            "Train Epoch: 50 [4352/14335 (30%)]\tLoss: 3405.293701\tLR: [7.738095238095238e-07]\n",
            "Train Epoch: 50 [4608/14335 (32%)]\tLoss: 2969.436279\tLR: [7.53968253968254e-07]\n",
            "Train Epoch: 50 [4864/14335 (34%)]\tLoss: 2759.232422\tLR: [7.341269841269842e-07]\n",
            "Train Epoch: 50 [5120/14335 (36%)]\tLoss: 3221.757568\tLR: [7.142857142857143e-07]\n",
            "Train Epoch: 50 [5376/14335 (38%)]\tLoss: 2932.843994\tLR: [6.944444444444445e-07]\n",
            "Train Epoch: 50 [5632/14335 (39%)]\tLoss: 3250.216553\tLR: [6.746031746031746e-07]\n",
            "Train Epoch: 50 [5888/14335 (41%)]\tLoss: 3159.546875\tLR: [6.547619047619048e-07]\n",
            "Train Epoch: 50 [6144/14335 (43%)]\tLoss: 2778.812500\tLR: [6.34920634920635e-07]\n",
            "Train Epoch: 50 [6400/14335 (45%)]\tLoss: 2743.006104\tLR: [6.150793650793651e-07]\n",
            "Train Epoch: 50 [6656/14335 (46%)]\tLoss: 3118.324463\tLR: [5.952380952380952e-07]\n",
            "Train Epoch: 50 [6912/14335 (48%)]\tLoss: 3077.139404\tLR: [5.753968253968255e-07]\n",
            "Train Epoch: 50 [7168/14335 (50%)]\tLoss: 2739.033936\tLR: [5.555555555555556e-07]\n",
            "Train Epoch: 50 [7424/14335 (52%)]\tLoss: 3202.803223\tLR: [5.357142857142858e-07]\n",
            "Train Epoch: 50 [7680/14335 (54%)]\tLoss: 3078.028320\tLR: [5.158730158730158e-07]\n",
            "Train Epoch: 50 [7936/14335 (55%)]\tLoss: 2866.955811\tLR: [4.96031746031746e-07]\n",
            "Train Epoch: 50 [8192/14335 (57%)]\tLoss: 3058.182861\tLR: [4.761904761904763e-07]\n",
            "Train Epoch: 50 [8448/14335 (59%)]\tLoss: 3258.484863\tLR: [4.563492063492064e-07]\n",
            "Train Epoch: 50 [8704/14335 (61%)]\tLoss: 3013.789795\tLR: [4.3650793650793655e-07]\n",
            "Train Epoch: 50 [8960/14335 (62%)]\tLoss: 2722.168701\tLR: [4.1666666666666667e-07]\n",
            "Train Epoch: 50 [9216/14335 (64%)]\tLoss: 2943.825928\tLR: [3.9682539682539683e-07]\n",
            "Train Epoch: 50 [9472/14335 (66%)]\tLoss: 2777.402588\tLR: [3.76984126984127e-07]\n",
            "Train Epoch: 50 [9728/14335 (68%)]\tLoss: 3004.622070\tLR: [3.5714285714285716e-07]\n",
            "Train Epoch: 50 [9984/14335 (70%)]\tLoss: 3435.996094\tLR: [3.373015873015873e-07]\n",
            "Train Epoch: 50 [10240/14335 (71%)]\tLoss: 2780.992432\tLR: [3.174603174603175e-07]\n",
            "Train Epoch: 50 [10496/14335 (73%)]\tLoss: 3551.391357\tLR: [2.976190476190476e-07]\n",
            "Train Epoch: 50 [10752/14335 (75%)]\tLoss: 2420.364014\tLR: [2.777777777777778e-07]\n",
            "Train Epoch: 50 [11008/14335 (77%)]\tLoss: 3101.689209\tLR: [2.579365079365079e-07]\n",
            "Train Epoch: 50 [11264/14335 (79%)]\tLoss: 2526.740234\tLR: [2.3809523809523814e-07]\n",
            "Train Epoch: 50 [11520/14335 (80%)]\tLoss: 2952.246094\tLR: [2.1825396825396828e-07]\n",
            "Train Epoch: 50 [11776/14335 (82%)]\tLoss: 3250.635254\tLR: [1.9841269841269841e-07]\n",
            "Train Epoch: 50 [12032/14335 (84%)]\tLoss: 3010.734863\tLR: [1.7857142857142858e-07]\n",
            "Train Epoch: 50 [12288/14335 (86%)]\tLoss: 2922.040771\tLR: [1.5873015873015874e-07]\n",
            "Train Epoch: 50 [12544/14335 (88%)]\tLoss: 3107.601318\tLR: [1.388888888888889e-07]\n",
            "Train Epoch: 50 [12800/14335 (89%)]\tLoss: 2958.238770\tLR: [1.1904761904761907e-07]\n",
            "Train Epoch: 50 [13056/14335 (91%)]\tLoss: 3174.525879\tLR: [9.920634920634921e-08]\n",
            "Train Epoch: 50 [13312/14335 (93%)]\tLoss: 2831.880859\tLR: [7.936507936507937e-08]\n",
            "Train Epoch: 50 [13568/14335 (95%)]\tLoss: 2885.442383\tLR: [5.9523809523809535e-08]\n",
            "Train Epoch: 50 [13824/14335 (96%)]\tLoss: 3050.646973\tLR: [3.9682539682539686e-08]\n",
            "Train Epoch: 50 [14025/14335 (98%)]\tLoss: 3029.106689\tLR: [1.9841269841269843e-08]\n",
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Evaluation ROC Score in epoch 50 is 0.9506172839506173, Best ROC Score is:0.9757894736842105\n",
            "Total Training Time: 5:19:49.031155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZWxeZodKs17"
      },
      "source": [
        "#work with model that had the best validation roc_auc\n",
        "model = load_model(model_name)"
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZHc28mmTGIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b162392-7682-4618-da26-1630fa55bcc6"
      },
      "source": [
        "#anom_scores, targets = evaluate_one_index(model, test_loader, device)\n",
        "anom_scores, targets = evaluate(model, test_loader, device, NUMBER_OF_FRAMES, number_of_batches_eval=1000)"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Progress: 0/3071\n",
            "Progress: 20/3071\n",
            "Progress: 40/3071\n",
            "Progress: 60/3071\n",
            "Progress: 80/3071\n",
            "Progress: 100/3071\n",
            "Progress: 120/3071\n",
            "Progress: 140/3071\n",
            "Progress: 160/3071\n",
            "Progress: 180/3071\n",
            "Progress: 200/3071\n",
            "Progress: 220/3071\n",
            "Progress: 240/3071\n",
            "Progress: 260/3071\n",
            "Progress: 280/3071\n",
            "Progress: 300/3071\n",
            "Progress: 320/3071\n",
            "Progress: 340/3071\n",
            "Progress: 360/3071\n",
            "Progress: 380/3071\n",
            "Progress: 400/3071\n",
            "Progress: 420/3071\n",
            "Progress: 440/3071\n",
            "Progress: 460/3071\n",
            "Progress: 480/3071\n",
            "Progress: 500/3071\n",
            "Progress: 520/3071\n",
            "Progress: 540/3071\n",
            "Progress: 560/3071\n",
            "Progress: 580/3071\n",
            "Progress: 600/3071\n",
            "Progress: 620/3071\n",
            "Progress: 640/3071\n",
            "Progress: 660/3071\n",
            "Progress: 680/3071\n",
            "Progress: 700/3071\n",
            "Progress: 720/3071\n",
            "Progress: 740/3071\n",
            "Progress: 760/3071\n",
            "Progress: 780/3071\n",
            "Progress: 800/3071\n",
            "Progress: 820/3071\n",
            "Progress: 840/3071\n",
            "Progress: 860/3071\n",
            "Progress: 880/3071\n",
            "Progress: 900/3071\n",
            "Progress: 920/3071\n",
            "Progress: 940/3071\n",
            "Progress: 960/3071\n",
            "Progress: 980/3071\n",
            "Progress: 1000/3071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "6g5XlymJCQ_U",
        "outputId": "277bc968-5140-4786-d6b3-b0b696fb9fc0"
      },
      "source": [
        "print(len(losses))\n",
        "print(losses)\n",
        "#x_values = [x for x in range(0, len(losses))]\n",
        "plt.plot(losses, label='loss')\n",
        "plt.title(f'Loss of Model: {model_name}')\n",
        "plt.xlabel('steps')\n",
        "plt.ylabel('loss')\n",
        "plt.savefig('/content/drive/My Drive/models/transformers/loss_curve_' + model_name + ''.join(NORMAL_CLASSES) + \"_\" + str(roc_auc) + \".jpg\")\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8022789ee2fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#x_values = [x for x in range(0, len(losses))]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Loss of Model: {model_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9A6rZApGWcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "d6e2284c-8eb1-48c5-a1a2-7fe9a6342bbd"
      },
      "source": [
        "\n",
        "current_time = time.asctime( time.localtime(time.time()) )\n",
        "\n",
        "fp_rate, tp_rate, _ = roc_curve(targets, anom_scores, pos_label=1)\n",
        "roc_auc = roc_auc_score(targets, anom_scores)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fp_rate, tp_rate, color='blue', label=f\"ROC_AUC ={roc_auc}\")\n",
        "\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve of ' + model_name +' with normal Classes: ' + ''.join(NORMAL_CLASSES))\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('/content/drive/My Drive/models/transformers/roc_graph' + model_name + ''.join(NORMAL_CLASSES) + \"_\" + str(roc_auc) + \".jpg\")\n",
        "plt.show()"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnqcTXZJGreX"
      },
      "source": [
        "import pytorch_model_summary as pms\n",
        "summary = pms.summary(model, torch.ones(16, 22, 256).to(device))"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC950s26HEkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20be7e29-747a-4c60-d41b-1344e9138073"
      },
      "source": [
        "print(\"Traininig finished! Saving Config...\")\n",
        "from google.colab import files\n",
        "\n",
        "#save hyperparams and result:\n",
        "with open('/content/drive/My Drive/models/transformers/hyper_params' + model_name + '.txt', 'w') as f:\n",
        "  f.write(f\"Model Name: {model_name}\\n\" +\n",
        "          f\"Epochs: {EPOCHS}, Training Time: {training_finished - training_start} BatchSize: {BATCH_SIZE}, Optimizer: {optimizer}, Total Steps: {total_steps}, Warm up Steps: {warm_up_steps}\\n\" +\n",
        "          f\"SAMPLE_RATE = {SAMPLE_RATE}, N_FFT/WINDOW_SIZE = {N_FFT}, HOP_LENGTH = {HOP_LENGTH}, N_MELS = {N_MELS}\\n\" + \n",
        "          f\"NUMBER_OF_FRAMES: {NUMBER_OF_FRAMES}, EMBEDDING_SIZE = {EMBEDDING_SIZE}, N_HEADS = {N_HEADS}, N_ENCODER_LAYERS = {N_ENCODER_LAYERS}, DROPOUT = {DROPOUT}, DIM_FEED_FORWARD = {DIM_FEED_FORWARD}\\n\"+\n",
        "          f\"Normal Classes: {NORMAL_CLASSES}, Anomalous Classes: {ANOMALOUS_CLASSES}, ROC_AUC Score: {roc_auc}  \\n\\n {summary}\")"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traininig finished! Saving Config...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}