{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNnrCfvznIzQ1vZDsaYIrCg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohEder/bachelor_thesis_audio_ml/blob/master/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIBl-ybmvCJ7",
        "outputId": "35ebe91c-f7ab-4735-a076-8f0bbcdc43e3"
      },
      "source": [
        "!pip install torchaudio\n",
        "!pip install pytorch-model-summary"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchaudio) (3.7.4.3)\n",
            "Requirement already satisfied: pytorch-model-summary in /usr/local/lib/python3.7/dist-packages (0.1.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch-model-summary) (1.9.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-model-summary) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-model-summary) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-model-summary) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdn43FFt6jp0"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import torchaudio\n",
        "import pandas as pd\n",
        "import os\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "import math\n",
        "import time\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import softmax\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm9hEziO6l5F"
      },
      "source": [
        "#just copied the official import script for the dataset, custom preprocessing happens afterwards\n",
        "\"\"\" Import script for IDMT-Traffic dataset\n",
        "Ref:\n",
        "    J. Abeßer, S. Gourishetti, A. Kátai, T. Clauß, P. Sharma, J. Liebetrau: IDMT-Traffic: An Open Benchmark\n",
        "    Dataset for Acoustic Traffic Monitoring Research, EUSIPCO, 2021\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "__author__ = 'Jakob Abeßer (jakob.abesser@idmt.fraunhofer.de)'\n",
        "\n",
        "\n",
        "def import_idmt_traffic_dataset(fn_txt: str = \"idmt_traffic_all\") -> pd.DataFrame:\n",
        "    \"\"\" Import IDMT-Traffic dataset\n",
        "    Args:\n",
        "        fn_txt (str): Text file with all WAV files\n",
        "    Returns:\n",
        "        df_dataset (pd.Dataframe): File-wise metadata\n",
        "            Columns:\n",
        "                'file': WAV filename,\n",
        "                'is_background': True if recording contains background noise (no vehicle), False else\n",
        "                'date_time': Recording time (YYYY-MM-DD-HH-mm)\n",
        "                'location': Recording location\n",
        "                'speed_kmh': Speed limit at recording site (km/h), UNK if unknown,\n",
        "                'sample_pos': Sample position (centered) within the original audio recording,\n",
        "                'daytime': M(orning) or (A)fternoon,\n",
        "                'weather': (D)ry or (W)et road condition,\n",
        "                'vehicle': (B)us, (C)ar, (M)otorcycle, or (T)ruck,\n",
        "                'source_direction': Source direction of passing vehicle: from (L)eft or from (R)ight,\n",
        "                'microphone': (SE)= (high-quality) sE8 microphones, (ME) = (low-quality) MEMS microphones (ICS-43434),\n",
        "                'channel': Original stereo pair channel (12) or (34)\n",
        "    \"\"\"\n",
        "    # load file list\n",
        "    df_files = pd.read_csv(fn_txt, names=('file',))\n",
        "    fn_file_list = df_files['file'].to_list()\n",
        "\n",
        "    # load metadata from file names\n",
        "    df_dataset = []\n",
        "\n",
        "    for f, fn in enumerate(fn_file_list):\n",
        "        fn = fn.replace('.wav', '')\n",
        "        parts = fn.split('_')\n",
        "\n",
        "        # background noise files\n",
        "        if '-BG' in fn:\n",
        "            date_time, location, speed_kmh, sample_pos, mic, channel = parts\n",
        "            vehicle, source_direction, weather, daytime = 'None', 'None', 'None', 'None'\n",
        "            is_background = True\n",
        "\n",
        "        # files with vehicle passings\n",
        "        else:\n",
        "            date_time, location, speed_kmh, sample_pos, daytime, weather, vehicle_direction, mic, channel = parts\n",
        "            vehicle, source_direction = vehicle_direction\n",
        "            is_background = False\n",
        "\n",
        "        channel = channel.replace('-BG', '')\n",
        "        speed_kmh = speed_kmh.replace('unknownKmh', 'UNK')\n",
        "        speed_kmh = speed_kmh.replace('Kmh', '')\n",
        "\n",
        "        df_dataset.append({'file': fn,\n",
        "                           'is_background': is_background,\n",
        "                           'date_time': date_time,\n",
        "                           'location': location,\n",
        "                           'speed_kmh': speed_kmh,\n",
        "                           'sample_pos': sample_pos,\n",
        "                           'daytime': daytime,\n",
        "                           'weather': weather,\n",
        "                           'vehicle': vehicle,\n",
        "                           'source_direction': source_direction,\n",
        "                           'microphone': mic,\n",
        "                           'channel': channel})\n",
        "\n",
        "    df_dataset = pd.DataFrame(df_dataset, columns=('file', 'is_background', 'date_time', 'location', 'speed_kmh', 'sample_pos', 'daytime', 'weather', 'vehicle',\n",
        "                                                   'source_direction', 'microphone', 'channel'))\n",
        "\n",
        "    return df_dataset"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53-c6jdM6uLg"
      },
      "source": [
        "\"\"\"\n",
        "Anomalous Sound Transformer Model for my Bachelor Thesis\n",
        "\"\"\"\n",
        "\n",
        "__author__ = 'Johannes Eder (Jo.Eder@campus.lmu.de)'\n",
        "\n",
        "#print(len(all_data[all_data.is_background])) #8144 -> #9362 labbelled background sounds\n",
        "#print(len(all_data[all_data.vehicle == 'C'])) #7804\n",
        "#print(len(all_data[all_data.vehicle == 'M'])) #430\n",
        "#print(len(all_data[all_data.vehicle == 'T'])) #1022\n",
        "#print(len(all_data[all_data.vehicle == 'B'])) #106"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAnBzNvl7V9Q",
        "outputId": "8bf79efc-d48c-4467-8b2a-355808633062"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gArrnoTR7aiz"
      },
      "source": [
        "CLASSES = ['None','C','T', 'M', 'B'] #Background Noise, Car, Truck, Motorcycle, Bus\n",
        "NORMAL_CLASSES = ['None', 'C']\n",
        "ANOMALOUS_CLASSES = ['T', 'M', 'B']\n",
        "\n",
        "SAMPLE_RATE = 22500\n",
        "N_FFT=2048 #is also window size\n",
        "HOP_LENGTH=1024\n",
        "N_MELS=128\n",
        "NUMBER_OF_FRAMES = 2\n",
        "melspectogram = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=SAMPLE_RATE,\n",
        "        n_fft=N_FFT, # Frame Size\n",
        "        hop_length=HOP_LENGTH, #here half the frame size\n",
        "        n_mels=N_MELS\n",
        "    )\n",
        "\n",
        "transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(mode='L'),\n",
        "    #transforms.Grayscale(num_output_channels=3),\n",
        "    #transforms.Resize([224, 224]),\n",
        "    #transforms.RandomCrop(size=[N_MELS, NUMBER_OF_FRAMES]), #only train on random slice of the spectogram\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "AUDIO_DIR = \"/content/drive/My Drive/datasets/IDMT_Traffic/audio\"\n",
        "train_annotations = \"/content/drive/My Drive/datasets/IDMT_Traffic/annotation/eusipco_2021_train.csv\"\n",
        "test_annotatons = \"/content/drive/My Drive/datasets/IDMT_Traffic/annotation/eusipco_2021_test.csv\"\n",
        "all_annotations_txt = \"/content/drive/My Drive/datasets/IDMT_Traffic/annotation/idmt_traffic_all.txt\"\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BATCH_SIZE_VAL = 1\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "\n",
        "NUMBER_OF_FRAMES = 2\n",
        "EMBEDDING_SIZE = 128\n",
        "N_HEADS = 4\n",
        "N_ENCODER_LAYERS = 4\n",
        "DROPOUT = 0.0 #is dropout needed for AD? no\n",
        "DIM_FEED_FORWARD = 256\n",
        "input_dim = 256\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjg39EMZ75O1"
      },
      "source": [
        "class IdmtTrafficDataSet(Dataset):\n",
        "\n",
        "    def __init__(self, annotations_file, audio_dir, audio_transformation, transformation, target_sample_rate, normal_classes):\n",
        "        self.annotations =  annotations_file if isinstance(annotations_file, pd.DataFrame) else pd.read_csv(annotations_file)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.audio_transformation = audio_transformation\n",
        "        self.transformation = transformation\n",
        "        self.target_sample_rate = target_sample_rate\n",
        "        #self.classes = ['None','C','T', 'M', 'B']\n",
        "        self.normal_classes = normal_classes\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        audio_sample_path = self._get_audio_sample_path(index)\n",
        "        label = self._get_audio_sample_label(index)\n",
        "        signal, sr = torchaudio.load(audio_sample_path)\n",
        "        signal = self._resample(signal, sr) #adjust sample rates\n",
        "        # signal -> (num_channels, samples) i.e. (2, 16000)\n",
        "        signal  = self._mix_down(signal) #stereo to mono\n",
        "        signal = self.audio_transformation(signal) #(1, 16000) -> torch.Size([1, 64, 63])\n",
        "        signal = self.transformation(signal)\n",
        "        #label = self.normal_classes.index(label)\n",
        "        label = 0 if label in self.normal_classes else 1\n",
        "        return signal, label\n",
        "\n",
        "    def _resample(self, signal, sr):\n",
        "        if sr != self.target_sample_rate:\n",
        "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
        "            signal = resampler(signal)\n",
        "        return signal\n",
        "    \n",
        "    def _mix_down(self, signal):\n",
        "        if signal.shape[0] > 1: #(2, 16000)\n",
        "            #mean operation: aggregating multiple channels\n",
        "            signal = torch.mean(signal, 0, True)\n",
        "        return signal\n",
        "\n",
        "    def _get_audio_sample_path(self, index):\n",
        "        path = os.path.join(self.audio_dir, self.annotations.iloc[index, 0])\n",
        "        return path + '.wav'\n",
        "\n",
        "    def _get_audio_sample_label(self, index):\n",
        "        return self.annotations.iloc[index, 9]"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf1bYw0c8Qe6"
      },
      "source": [
        "def get_normal_and_anomalous_data(normal_classes, anomalous_classes, audio_dir, annotations, batch_size):\n",
        "    if len((set(normal_classes) & set(anomalous_classes))) > 0:\n",
        "      raise Exception(\"Intersection between normal and anomalous classes should be empty!\")\n",
        "\n",
        "    all_data = import_idmt_traffic_dataset(annotations)\n",
        "\n",
        "    normal_data = all_data[all_data.vehicle.isin(normal_classes)]\n",
        "    anomalous_data = all_data[all_data.vehicle.isin(anomalous_classes)]\n",
        "\n",
        "    test_ratio = len(normal_data) // 10 #roughly ten percent of normal data to test\n",
        "    train_data, test_data_normal = train_test_split(normal_data, test_size=0.1, random_state=RANDOM_SEED)\n",
        "    #train_data = normal_data.iloc[:6000, :] #test data needs to have some amount of normal data as well, maybe take always 10 percent of the normal trainig data as anomalous\n",
        "    train_data = adjust_sample_number_to_batch_size(train_data, batch_size)\n",
        "    print(f\"training with {len(train_data)} (normal) samples\")\n",
        "\n",
        "    number_of_normal_test_sampels = len(test_data_normal)\n",
        "    print(f\"testing with {number_of_normal_test_sampels} normal samples\")\n",
        "\n",
        "    #sample same number of anomalous data to test\n",
        "    number_anomlous = number_of_normal_test_sampels if number_of_normal_test_sampels < len(anomalous_data) else len(anomalous_data)\n",
        "    anomalous_data = anomalous_data.sample(number_anomlous)\n",
        "    print(f\"testing with {len(anomalous_data)} anomalous samples\")\n",
        "\n",
        "    frames = [anomalous_data, test_data_normal]\n",
        "    concatenated_test_data = pd.concat(frames)\n",
        "    concatenated_test_data.reset_index(drop=True, inplace=True)\n",
        "    concatenated_test_data = adjust_sample_number_to_batch_size(concatenated_test_data, batch_size)\n",
        "\n",
        "    normal_train_data = IdmtTrafficDataSet(train_data, audio_dir, melspectogram, transforms, SAMPLE_RATE, normal_classes)\n",
        "    test_data = IdmtTrafficDataSet(concatenated_test_data, audio_dir, melspectogram, transforms, SAMPLE_RATE, normal_classes)\n",
        "\n",
        "    return normal_train_data, test_data\n",
        "\n",
        "def adjust_sample_number_to_batch_size(data, batch_size):\n",
        "  if len(data) % batch_size == 0:\n",
        "    print(\"no data discarded.\")\n",
        "    return data\n",
        "  else:\n",
        "    remainder = len(data) % batch_size\n",
        "    print(str(remainder + 1) + \" samples discarded.\")\n",
        "    return data.iloc[remainder + 1:,:]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwLjzSG0HkIC"
      },
      "source": [
        "class TransformerModel(nn.Module):\n",
        "  def __init__(self, d_model, input_dim, n_heads, dim_feedforward, n_encoder_layers, dropout=0.5):\n",
        "    super(TransformerModel, self).__init__()\n",
        "    self.model_type = 'Transformer'\n",
        "    self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "    encoder_layers = TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "    self.transformer_encoder = TransformerEncoder(encoder_layers, n_encoder_layers)\n",
        "    self.patch_embedding = PatchEmbedding(input_dim, d_model)\n",
        "    self.input_dim = input_dim\n",
        "    self.d_model = d_model\n",
        "    self.decoder = Decoder(d_model, input_dim)\n",
        "\n",
        "    self.mask_token = nn.Parameter(torch.randn(d_model, requires_grad=True))\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    initrange = 0.1\n",
        "    #self.patch_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "    #self.decoder.bias.data.zero_()\n",
        "    #self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "  \n",
        "  def forward(self, input, mask_index=None):\n",
        "    embedded = self.patch_embedding(input) * math.sqrt(self.input_dim) #is scaling necessary? yes, otherwise values are incredibly small\n",
        "    embedded_masked, mask_idxs = self.mask_embedded_tokens(embedded)\n",
        "    pos_encoded_embedded = self.pos_encoder(embedded_masked)\n",
        "    transformer_out = self.transformer_encoder(pos_encoded_embedded)\n",
        "    output = self.decoder(transformer_out)\n",
        "    return output, mask_idxs\n",
        "\n",
        "  def mask_embedded_tokens(self, input, specific_mask_idx=None):\n",
        "    if specific_mask_idx != None:\n",
        "      assert specific_mask_idx < input.shape[1]\n",
        "    number_of_specs = input.shape[0]\n",
        "    input_masked = []\n",
        "    masks_index_list = []\n",
        "    for i in range(number_of_specs):\n",
        "      mask_idx = specific_mask_idx if specific_mask_idx != None else random.randint(0, input.shape[1]-1)\n",
        "\n",
        "      input[i, mask_idx, :] = self.mask_token\n",
        "\n",
        "      input_masked.append(input[i,:,:]) #maybe just tuples (current_spec_masked, mask_idx)\n",
        "      masks_index_list.append(torch.as_tensor(mask_idx))\n",
        "\n",
        "    return torch.stack(input_masked), torch.stack(masks_index_list)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLVUdkvh1zBH"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, transformer_out, out_put_total):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.input_dim = transformer_out\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(in_features=transformer_out, out_features=transformer_out),  #evtl 2*d_model\n",
        "        nn.GELU(),\n",
        "        nn.Linear(in_features=transformer_out, out_features=out_put_total),)\n",
        "    \n",
        "  def forward(self, input):\n",
        "    x = self.mlp(input)\n",
        "    return x"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be9tNwH7OyRC"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, embedding_dim, dropout=0.1, max_len=5000):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    pe = torch.zeros(max_len, embedding_dim)\n",
        "    #print(f\"Shape: {pe.shape}\")\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    #print(f\"Position shape: {position.shape}\")\n",
        "    div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "    self.register_buffer('pe', pe)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = x + self.pe[:x.size(0), :]\n",
        "    return self.dropout(x)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s468t9EyVgkm"
      },
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self, input_dim, embedding_dimension):\n",
        "    super().__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.embedding_layer = nn.Linear(input_dim, embedding_dimension)\n",
        "  \n",
        "  def forward(self, input_data):\n",
        "    embedding = self.embedding_layer(input_data)\n",
        "    return embedding"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar2Mdd-ZpjuB"
      },
      "source": [
        "def patch_batch(input_batch, number_of_frames):\n",
        "  #input of shape (batch_size, channels, mel_filters, frames)\n",
        "  unfold = nn.Unfold(kernel_size=(input_batch.shape[2], NUMBER_OF_FRAMES), stride=NUMBER_OF_FRAMES) #patching the spectogram\n",
        "  unfolded_batch = unfold(input_batch) #(batch_size, features, number_of_patches)\n",
        "  unfolded_batch = unfolded_batch.transpose(1, 2) #(batch_size, number_of_patches, features)\n",
        "  return unfolded_batch"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D68e7futiiU",
        "outputId": "c2e13674-8de4-44b7-cf6e-a49e0d6f372a"
      },
      "source": [
        "train_data, test_data = get_normal_and_anomalous_data(NORMAL_CLASSES, ANOMALOUS_CLASSES, audio_dir=AUDIO_DIR, annotations=all_annotations_txt, batch_size=BATCH_SIZE)\n",
        "first_sample, first_label = train_data[0]\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE_VAL, shuffle=True)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18 samples discarded.\n",
            "training with 14335 (normal) samples\n",
            "testing with 1595 normal samples\n",
            "testing with 1558 anomalous samples\n",
            "18 samples discarded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79YOWc_meQ1R"
      },
      "source": [
        "transformer = TransformerModel(EMBEDDING_SIZE, input_dim, N_HEADS, DIM_FEED_FORWARD, N_ENCODER_LAYERS)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JahOBDTmNt4"
      },
      "source": [
        "def mask_input_batch(input, device, specific_mask_idx=None):\n",
        "  if specific_mask_idx != None:\n",
        "    assert specific_mask_idx < input.shape[1]\n",
        "  number_of_specs = input.shape[0]\n",
        "  input_masked = []\n",
        "  masks_index_list = []\n",
        "  for i in range(number_of_specs):\n",
        "    mask_idx = specific_mask_idx if specific_mask_idx != None else random.randint(0, input.shape[1]-1)\n",
        "    mask = torch.ones(input.shape[1], input.shape[2])\n",
        "    mask[mask_idx, :] = 0\n",
        "    current_spec_masked = input[i, : , :].mul(mask)\n",
        "\n",
        "    input_masked.append(current_spec_masked) #maybe just tuples (current_spec_masked, mask_idx)\n",
        "    masks_index_list.append(torch.as_tensor(mask_idx))\n",
        "\n",
        "  return torch.stack(input_masked), torch.stack(masks_index_list)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "1lvKxqAHkdEM",
        "outputId": "73d3ca22-418f-47b4-ab18-2c030dc015dd"
      },
      "source": [
        "\"\"\"old\n",
        "def mask_embedded_tokens(input, mask_token, specific_mask_idx=None):\n",
        "  if specific_mask_idx != None:\n",
        "    assert specific_mask_idx < input.shape[1]\n",
        "  number_of_specs = input.shape[0]\n",
        "  input_masked = []\n",
        "  masks_index_list = []\n",
        "  for i in range(number_of_specs):\n",
        "    mask_idx = specific_mask_idx if specific_mask_idx != None else random.randint(0, input.shape[1]-1)\n",
        "\n",
        "    \n",
        "    input[i, mask_idx, :] = mask_token\n",
        "\n",
        "    input_masked.append(input[i,:,:]) #maybe just tuples (current_spec_masked, mask_idx)\n",
        "    masks_index_list.append(torch.as_tensor(mask_idx))\n",
        "\n",
        "  return torch.stack(input_masked), torch.stack(masks_index_list)\n",
        "  \"\"\""
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'old\\ndef mask_embedded_tokens(input, mask_token, specific_mask_idx=None):\\n  if specific_mask_idx != None:\\n    assert specific_mask_idx < input.shape[1]\\n  number_of_specs = input.shape[0]\\n  input_masked = []\\n  masks_index_list = []\\n  for i in range(number_of_specs):\\n    mask_idx = specific_mask_idx if specific_mask_idx != None else random.randint(0, input.shape[1]-1)\\n\\n    \\n    input[i, mask_idx, :] = mask_token\\n\\n    input_masked.append(input[i,:,:]) #maybe just tuples (current_spec_masked, mask_idx)\\n    masks_index_list.append(torch.as_tensor(mask_idx))\\n\\n  return torch.stack(input_masked), torch.stack(masks_index_list)\\n  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "057KTyNylH8s"
      },
      "source": [
        "def calculate_loss_masked(input_batch, output_batch, mask_idxs, sum_up):\n",
        "  print(input_batch.shape)\n",
        "  loss_func = nn.MSELoss();\n",
        "  loss_per_batch = 0\n",
        "  for i in range(len(mask_idxs)):\n",
        "    input_at_masked = input_batch[i, mask_idxs[i], :]\n",
        "    output_at_masked = output_batch[i, mask_idxs[i], :]\n",
        "    loss = loss_func(input_at_masked, output_at_masked)\n",
        "    #print(loss)\n",
        "    loss_per_batch += loss\n",
        "  return loss_per_batch\n",
        "\n",
        "\n",
        "def calculate_loss_total(input, output):\n",
        "  loss = nn.MSELoss();\n",
        "  return loss(input, output)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "phUe045ofcZf",
        "outputId": "374d3c74-c0b1-4496-c857-8c543846bbab"
      },
      "source": [
        "\"\"\"tryout\n",
        "input_test = torch.rand(64, 22, 256) #22 frames\n",
        "output, mask_idxs = transformer(input_test)\n",
        "print(output.shape)\n",
        "print(output[0, mask_idxs[0],:]) #mask token\n",
        "print(mask_idxs[0])\n",
        "loss= calculate_loss_masked(input_test, output, mask_idxs, True)\n",
        "print(loss_per_batch)\n",
        "\"\"\""
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'tryout\\ninput_test = torch.rand(64, 22, 256) #22 frames\\noutput, mask_idxs = transformer(input_test)\\nprint(output.shape)\\nprint(output[0, mask_idxs[0],:]) #mask token\\nprint(mask_idxs[0])\\nloss= calculate_loss_masked(input_test, output, mask_idxs, True)\\nprint(loss_per_batch)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "HsXP-g_RimLh",
        "outputId": "a86d6759-aa53-479f-f11a-b6a83cdd0bd9"
      },
      "source": [
        "#tryout, not relevant for the model\n",
        "\"\"\"\n",
        "input_sample = torch.rand(1, 1, 128, 44) #one sample\n",
        "data_batch = patch_batch(input_sample, NUMBER_OF_FRAMES)\n",
        "print(data_batch.shape) #torch.Size([16, 22, 256])\n",
        "#every patch needs to be masked once and the masked loss calculated added and divided by number of patches\n",
        "loss_total = 0\n",
        "for i in range(data_batch.shape[1]):\n",
        "  masked_input, mask_idxs = mask_input_batch(data_batch, i)\n",
        "  output = transformer(masked_input)\n",
        "  loss = calculate_loss_masked(data_batch, output, mask_idxs, True) # last argument (sum) does not make a difference for batch size 1\n",
        "  loss_total +=loss\n",
        "#loss_total /= data_batch.shape[1] #divide by number of patches\n",
        "print(loss_total)\n",
        "\n",
        "#masked_input, mask_idxs = mask_input_batch(data_batch, 2)\n",
        "#print(masked_input)\n",
        "#output = transformer(data_batch)\n",
        "#print(output)\n",
        "#loss = calculate_loss_masked(data_batch, output, mask_idxs, False)\n",
        "#print(len(loss))\n",
        "#print(loss)\n",
        "#loss_mse_total = calculate_loss_total(data_batch, output)\n",
        "#print(loss_mse_total)\n",
        "\"\"\""
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ninput_sample = torch.rand(1, 1, 128, 44) #one sample\\ndata_batch = patch_batch(input_sample, NUMBER_OF_FRAMES)\\nprint(data_batch.shape) #torch.Size([16, 22, 256])\\n#every patch needs to be masked once and the masked loss calculated added and divided by number of patches\\nloss_total = 0\\nfor i in range(data_batch.shape[1]):\\n  masked_input, mask_idxs = mask_input_batch(data_batch, i)\\n  output = transformer(masked_input)\\n  loss = calculate_loss_masked(data_batch, output, mask_idxs, True) # last argument (sum) does not make a difference for batch size 1\\n  loss_total +=loss\\n#loss_total /= data_batch.shape[1] #divide by number of patches\\nprint(loss_total)\\n\\n#masked_input, mask_idxs = mask_input_batch(data_batch, 2)\\n#print(masked_input)\\n#output = transformer(data_batch)\\n#print(output)\\n#loss = calculate_loss_masked(data_batch, output, mask_idxs, False)\\n#print(len(loss))\\n#print(loss)\\n#loss_mse_total = calculate_loss_total(data_batch, output)\\n#print(loss_mse_total)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC49Nt07fofv"
      },
      "source": [
        "#LEARNING_RATE = 0.001\n",
        "optimizer =  torch.optim.AdamW(transformer.parameters())\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "EPOCHS = 100\n",
        "def train(model,learning_rate, optimizer, scheduler, epoch, device):\n",
        "  print(f\"Starting Epoch {epoch}\")\n",
        "  for batch_index, (data_batch, _) in enumerate(train_loader):\n",
        "    #print(data_batch.shape)\n",
        "    data_batch = patch_batch(data_batch, NUMBER_OF_FRAMES)\n",
        "    data_batch = data_batch.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output, mask_idxs = model(data_batch)\n",
        "    loss = calculate_loss_masked(data_batch, output, mask_idxs, True)\n",
        "    #loss_total = calculate_loss_total(data_batch, output)\n",
        "    #print(f\"Loss patches: {loss}\\nLoss total: {loss_total}\")\n",
        "    loss.backward()\n",
        "    #torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optimizer.step()\n",
        "    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR: {}'.format(epoch, batch_index * len(data_batch), len(train_loader.dataset),100. * batch_index / len(train_loader), loss.item(), LEARNING_RATE))\n"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2NudZ295rdD"
      },
      "source": [
        "def evaluate(model, val_loader, device, number_of_frames, number_of_batches_eval=None):\n",
        "  #currently the batch size for evaluation needs to be 1\n",
        "  total_anom_scores = []\n",
        "  total_targets = []\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch_number, data in enumerate(val_loader, 0):\n",
        "      if (number_of_batches_eval != None) and (batch_number > number_of_batches_eval):\n",
        "        break\n",
        "      if (batch_number % 10 == 0):\n",
        "        print(f\"Progress: {batch_number}/{len(val_loader)}\")\n",
        "      inputs, target = data\n",
        "      inputs = inputs.to(device)\n",
        "      #print(inputs.shape)\n",
        "      inputs = patch_batch(inputs, NUMBER_OF_FRAMES)\n",
        "      #print(inputs.shape) #(n_spectograms, n_patches, features)\n",
        "      #every patch needs to be masked once and the masked loss calculated added and divided by number of patches\n",
        "      loss_total_current_spec = 0\n",
        "      for i in range(inputs.shape[1]): #iterate through patches\n",
        "        output, index = model(inputs, i) #patch i gets masked\n",
        "        print(output)\n",
        "        print(index)\n",
        "        loss = calculate_loss_masked(inputs, output, index, True) # last argument (sum) does not make a difference for batch size 1\n",
        "        loss_total_current_spec += loss\n",
        "      \n",
        "      loss_total_current_spec /= inputs.shape[1] #divide by number of patches\n",
        "      #print(loss_total_current_spec)\n",
        "      total_anom_scores.append(loss_total_current_spec.cpu().numpy()) #coverting to numpy for processing with scikit\n",
        "      total_targets.append(target)\n",
        "    return total_anom_scores, total_targets"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "717_tKu_A3z9"
      },
      "source": [
        "def evaluate_one_index(model, val_loader, device):\n",
        "  total_anom_scores = []\n",
        "  total_targets = []\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for _, data in enumerate(val_loader, 0):\n",
        "      inputs, batch_targets = data\n",
        "      #print(inputs.shape)\n",
        "      inputs = inputs.to(device)\n",
        "      inputs = patch_batch(inputs, NUMBER_OF_FRAMES)\n",
        "      #print(inputs.shape) #(n_spectograms, n_patches, features)\n",
        "      masked_input, mask_idxs = mask_input_batch(inputs,device, 10) #calculate mask for every spectogram in the batch at index\n",
        "      outputs = model(masked_input) #(n_spectograms, n_patches_reconstruces, features)\n",
        "      batch_anom_scores = calculate_loss_masked(inputs, outputs, mask_idxs, True)\n",
        "      print(batch_anom_scores)\n",
        "\n",
        "      total_anom_scores.append(batch_anom_scores.cpu().numpy()) #coverting to numpy for processing with scikit\n",
        "      total_targets += [x.cpu().numpy() for x in batch_targets]\n",
        "    return total_anom_scores, total_targets"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ljNdSSLFukl"
      },
      "source": [
        "def save_model(model, name):\n",
        "  name += '.pth'\n",
        "  torch.save(model, '/content/drive/My Drive/models/transformers/' + name)\n",
        "  return name\n",
        "\n",
        "def load_model(name):\n",
        "  name +='.pth'\n",
        "  model = torch.load('/content/drive/My Drive/models/transformers/' + name)\n",
        "  return model\n",
        "\n",
        "MODEL_NAME = 'transformer_02_mask_token_100ep_adam_mlp_decoder'"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_lDE1p4m1h2",
        "outputId": "f8edd537-4e57-46d3-8d25-8da79c01d1d4"
      },
      "source": [
        "transformer.to(device)\n",
        "transformer.train() #mode\n",
        "roc_auc_best = 0\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  train(transformer, LEARNING_RATE, optimizer, scheduler, epoch, device)\n",
        "  val_anom_scores, val_targets = evaluate(transformer, test_loader, device, NUMBER_OF_FRAMES, number_of_batches_eval=50) #batch size in evalution is only one\n",
        "  roc_auc = roc_auc_score(val_targets, val_anom_scores)\n",
        "  if roc_auc > roc_auc_best:\n",
        "    save_model(transformer, MODEL_NAME +f'_{epoch}')\n",
        "    roc_auc_best = roc_auc\n",
        "    print(f\"saved model with best validaton in epoch{epoch}\")\n",
        "  print(f\"Evaluation ROC Score in epoch {epoch} is {roc_auc}\")\n",
        "  scheduler.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Epoch 1\n",
            "torch.Size([64, 22, 256])\n",
            "Train Epoch: 1 [0/14335 (0%)]\tLoss: 6.718110\tLR: 0.001\n",
            "torch.Size([64, 22, 256])\n",
            "Train Epoch: 1 [64/14335 (0%)]\tLoss: 5.071492\tLR: 0.001\n",
            "torch.Size([64, 22, 256])\n",
            "Train Epoch: 1 [128/14335 (1%)]\tLoss: 3.651944\tLR: 0.001\n",
            "torch.Size([64, 22, 256])\n",
            "Train Epoch: 1 [192/14335 (1%)]\tLoss: 4.492076\tLR: 0.001\n",
            "torch.Size([64, 22, 256])\n",
            "Train Epoch: 1 [256/14335 (2%)]\tLoss: 3.794538\tLR: 0.001\n",
            "torch.Size([64, 22, 256])\n",
            "Train Epoch: 1 [320/14335 (2%)]\tLoss: 5.137134\tLR: 0.001\n",
            "torch.Size([64, 22, 256])\n",
            "Train Epoch: 1 [384/14335 (3%)]\tLoss: 2.809138\tLR: 0.001\n",
            "torch.Size([64, 22, 256])\n",
            "Train Epoch: 1 [448/14335 (3%)]\tLoss: 3.377309\tLR: 0.001\n",
            "torch.Size([64, 22, 256])\n",
            "Train Epoch: 1 [512/14335 (4%)]\tLoss: 3.466027\tLR: 0.001\n",
            "torch.Size([64, 22, 256])\n",
            "Train Epoch: 1 [576/14335 (4%)]\tLoss: 3.461933\tLR: 0.001\n",
            "torch.Size([64, 22, 256])\n",
            "Train Epoch: 1 [640/14335 (4%)]\tLoss: 3.727867\tLR: 0.001\n",
            "torch.Size([64, 22, 256])\n",
            "Train Epoch: 1 [704/14335 (5%)]\tLoss: 3.137779\tLR: 0.001\n",
            "torch.Size([64, 22, 256])\n",
            "Train Epoch: 1 [768/14335 (5%)]\tLoss: 3.272444\tLR: 0.001\n",
            "torch.Size([64, 22, 256])\n",
            "Train Epoch: 1 [832/14335 (6%)]\tLoss: 3.323972\tLR: 0.001\n",
            "torch.Size([64, 22, 256])\n",
            "Train Epoch: 1 [896/14335 (6%)]\tLoss: 3.735456\tLR: 0.001\n",
            "torch.Size([64, 22, 256])\n",
            "Train Epoch: 1 [960/14335 (7%)]\tLoss: 2.661706\tLR: 0.001\n",
            "torch.Size([64, 22, 256])\n",
            "Train Epoch: 1 [1024/14335 (7%)]\tLoss: 3.233422\tLR: 0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZWxeZodKs17"
      },
      "source": [
        "#work with model that had the best validation roc_auc\n",
        "model = load_model(MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZHc28mmTGIn"
      },
      "source": [
        "#anom_scores, targets = evaluate_one_index(model, test_loader, device)\n",
        "anom_scores, targets = evaluate(model, test_loader, device, NUMBER_OF_FRAMES, number_of_batches_eval=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwVshIaHNTMt"
      },
      "source": [
        "print(len(anom_scores))\n",
        "print(len(targets))\n",
        "print(anom_scores[5])\n",
        "print(targets[5])\n",
        "print(anom_scores[1])\n",
        "print(targets[1])\n",
        "print(anom_scores[3])\n",
        "print(targets[3])\n",
        "print(anom_scores[50])\n",
        "print(targets[50])\n",
        "print(anom_scores[100])\n",
        "print(targets[100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9A6rZApGWcd"
      },
      "source": [
        "\n",
        "current_time = time.asctime( time.localtime(time.time()) )\n",
        "\n",
        "fp_rate, tp_rate, _ = roc_curve(targets, anom_scores, pos_label=1)\n",
        "roc_auc = roc_auc_score(targets, anom_scores)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fp_rate, tp_rate, color='blue', label=f\"ROC_AUC ={roc_auc}\")\n",
        "\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve of ' + MODEL_NAME +' with normal Classes: ' + ''.join(NORMAL_CLASSES))\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('/content/drive/My Drive/models/transformers/roc_graph' + MODEL_NAME+ ''.join(NORMAL_CLASSES) + \"_\" + str(roc_auc) + \".jpg\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnqcTXZJGreX"
      },
      "source": [
        "import pytorch_model_summary as pms\n",
        "summary = pms.summary(model, torch.ones(16, 22, 256).to(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC950s26HEkX"
      },
      "source": [
        "print(\"Traininig finished! Saving Config...\")\n",
        "from google.colab import files\n",
        "\n",
        "#save hyperparams and result:\n",
        "with open('/content/drive/My Drive/models/transformers/hyper_params' + MODEL_NAME + '.txt', 'w') as f:\n",
        "  f.write(f\"Model Name: {MODEL_NAME}, Epochs: {EPOCHS}, LR: {LEARNING_RATE}, BatchSize: {BATCH_SIZE}, SAMPLE_RATE = {SAMPLE_RATE}, N_FFT/WINDOW_SIZE = {N_FFT}, HOP_LENGTH = {HOP_LENGTH}, N_MELS = {N_MELS}\" \n",
        "          + f\"NUMBER_OF_FRAMES: {NUMBER_OF_FRAMES}, EMBEDDING_SIZE = {EMBEDDING_SIZE}, N_HEADS = {N_HEADS}, N_ENCODER_LAYERS = {N_ENCODER_LAYERS}, DROPOUT = {DROPOUT}, DIM_FEED_FORWARD = {DIM_FEED_FORWARD}\"\n",
        "  + f\"\\nNormal Classes: {NORMAL_CLASSES}, Anomalous Classes: {ANOMALOUS_CLASSES}, ROC_AUC Score: {roc_auc}  \\n\\n {summary}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}