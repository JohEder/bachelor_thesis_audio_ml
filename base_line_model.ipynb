{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "base_line_model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMIieJZFUUzp+3aLyLaekB/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohEder/bachelor_thesis_audio_ml/blob/master/base_line_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-Vvaf0GA9sS",
        "outputId": "8ba17c5a-c74a-4078-e0ae-225bf7bb6048"
      },
      "source": [
        "!pip install torchaudio"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchaudio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/20/eab40caad8f4b97f5e91a5de8ba5ec29115e08fa4c9a808725490b7b4844/torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchaudio) (3.7.4.3)\n",
            "Installing collected packages: torchaudio\n",
            "Successfully installed torchaudio-0.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "og7X9VAnBFWh"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset \n",
        "import torchaudio\n",
        "import pandas as pd\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-cB239lBNBT",
        "outputId": "c36bba58-6300-4ad7-ee53-d11f8df4d7e5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkIJ1NyiBPG8",
        "outputId": "c2979ba9-efca-4420-cdfc-3d1ddbfab249"
      },
      "source": [
        "!ls \"/content/drive/My Drive\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " ATT00001.gdoc\n",
            "'Colab Notebooks'\n",
            "'Data Science and Machine Learning.gslides'\n",
            " datasets\n",
            " models\n",
            "'ProbenBisWeihnachten (1).txt.gdoc'\n",
            " ProbenBisWeihnachten.txt.gdoc\n",
            "'Project Edwinter.gdoc'\n",
            "'Seminar Paper: Handout and Literature.gdoc'\n",
            "'Um Antwort wird gebeten (1).gform'\n",
            "'Um Antwort wird gebeten.gform'\n",
            "'User Interview.gdoc'\n",
            "'User Interviews Drink Mates'\n",
            " vorläufige.gdoc\n",
            " VVZafa183ad-b65e-4fbb-9681-0bac29b42558.rtf.gdoc\n",
            "'Wie soll unsere App heißen?_exported_on_Tue May 05 2020 17:18:32 GMT+0530 (IST).gsheet'\n",
            "'Wie soll unsere App heißen? .gform'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pch1Z3AhBSaP"
      },
      "source": [
        "class IdmtTrafficDataSet(Dataset):\n",
        "\n",
        "    \n",
        "\n",
        "    def __init__(self, annotations_file, audio_dir, audio_transformation, transformation, target_sample_rate):\n",
        "        self.annotations = pd.read_csv(annotations_file)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.audio_transformation = audio_transformation\n",
        "        self.transformation = transformation\n",
        "        self.target_sample_rate = target_sample_rate\n",
        "        self.classes = ['None','C','T', 'M', 'B']\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        audio_sample_path = self._get_audio_sample_path(index)\n",
        "        label = self._get_audio_sample_label(index)\n",
        "        signal, sr = torchaudio.load(audio_sample_path)\n",
        "        signal = self._resample(signal, sr) #adjust sample rates\n",
        "        # signal -> (num_channels, samples) i.e. (2, 16000)\n",
        "        signal  = self._mix_down(signal) #stereo to mono\n",
        "        signal = self.audio_transformation(signal) #(1, 16000) -> torch.Size([1, 64, 63])\n",
        "        signal = self.transformation(signal)\n",
        "        return signal, self.classes.index(label)\n",
        "\n",
        "    def _resample(self, signal, sr):\n",
        "        if sr != self.target_sample_rate:\n",
        "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
        "            signal = resampler(signal)\n",
        "        return signal\n",
        "    \n",
        "    def _mix_down(self, signal):\n",
        "        if signal.shape[0] > 1: #(2, 16000)\n",
        "            #mean operation: aggregating multiple channels\n",
        "            signal = torch.mean(signal, 0, True)\n",
        "        return signal\n",
        "\n",
        "    def _get_audio_sample_path(self, index):\n",
        "        path = os.path.join(self.audio_dir, self.annotations.iloc[index, 1])\n",
        "        return path + '.wav'\n",
        "\n",
        "    def _get_audio_sample_label(self, index):\n",
        "        return self.annotations.iloc[index, 9]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaADhdVqBWL8"
      },
      "source": [
        "SAMPLE_RATE = 22500\n",
        "N_FFT=2048 #is also window size\n",
        "HOP_LENGTH=1024\n",
        "N_MELS=128\n",
        "melspectogram = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=SAMPLE_RATE,\n",
        "        n_fft=N_FFT, # Frame Size\n",
        "        hop_length=HOP_LENGTH, #here half the frame size\n",
        "        n_mels=N_MELS\n",
        "    )\n",
        "\n",
        "transforms = transforms.Compose([\n",
        "    #transforms.ToPILImage(mode='L'),\n",
        "    #transforms.Grayscale(num_output_channels=3),\n",
        "    #transforms.Resize([224, 224]),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTzdntp5BcfQ"
      },
      "source": [
        "def get_train_and_val_idmt():\n",
        "    AUDIO_DIR = \"/content/drive/My Drive/datasets/IDMT_Traffic/audio\"\n",
        "    train_annotations = \"/content/drive/My Drive/datasets/IDMT_Traffic/annotation/eusipco_2021_train.csv\"\n",
        "    test_annotatons = \"/content/drive/My Drive/datasets/IDMT_Traffic/annotation/eusipco_2021_test.csv\"\n",
        "    train_data = IdmtTrafficDataSet(train_annotations, AUDIO_DIR,melspectogram, transforms, SAMPLE_RATE)\n",
        "    test_data = IdmtTrafficDataSet(test_annotatons, AUDIO_DIR,melspectogram, transforms, SAMPLE_RATE)\n",
        "    return train_data, test_data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrZEtykXBfB5",
        "outputId": "45a17157-bba3-4114-ad77-3122a4eb54bd"
      },
      "source": [
        "train_data, val_data = get_train_and_val_idmt()\n",
        "first_sample, first_label = train_data[0]\n",
        "input_dim = first_sample.shape[1] *first_sample.shape[2]\n",
        "print(f\"Train Data Shape: {first_sample.shape}\")\n",
        "print(input_dim)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Data Shape: torch.Size([3, 128, 44])\n",
            "5632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71EkxTEJCXAK"
      },
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "  def __init__(self, input_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = nn.Sequential(\n",
        "        #FC(Input, 64, ReLU)\n",
        "        nn.Linear(in_features=input_dim, out_features=64),\n",
        "        nn.ReLU(),\n",
        "        #FC(64, 32, ReLU),\n",
        "        nn.Linear(in_features=64, out_features=32),\n",
        "        nn.ReLU(),\n",
        "        #FC(32, 16, ReLU)\n",
        "        nn.Linear(in_features=32, out_features=16),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.decoder = nn.Sequential(\n",
        "        #FC(16, 32,ReLU)\n",
        "        nn.Linear(in_features=16, out_features=32),\n",
        "        nn.ReLU(),\n",
        "        #FC(32, 64, ReLU), \n",
        "        nn.Linear(in_features=32, out_features=64),\n",
        "        nn.ReLU(),\n",
        "        #FC(64, Output, none)\n",
        "        nn.Linear(in_features=64, out_features=input_dim),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, input_data):\n",
        "    z = self.encoder(input_data)\n",
        "    output = self.decoder(z)\n",
        "    return output"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4NP5lFZKzys"
      },
      "source": [
        "LEARNING_RATE = 0.0001\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 16\n",
        "BATCH_SIZE_VAL = 51\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "autoencoder = AutoEncoder(input_dim=input_dim)\n",
        "\n",
        "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=LEARNING_RATE)\n",
        "loss_func = nn.MSELoss()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Dc4hNgaN1HO"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE_VAL, shuffle=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCeOehFjOdrr"
      },
      "source": [
        "def train(model, input_dim, device, train_loader, optimizer, epoch, loss_func):\n",
        "  print(\"Starting Training.\")\n",
        "  model.to(device)\n",
        "  model.train() #set mode\n",
        "  for batch_index, (data_batch, _) in enumerate(train_loader):\n",
        "    data_batch = data_batch.view(-1, input_dim).to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data_batch)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = loss_func(output, data_batch)\n",
        "    loss.backward()                 \n",
        "    optimizer.step()\n",
        "    \n",
        "    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_index * len(data_batch), len(train_loader.dataset),100. * batch_index / len(train_loader), loss.item()))\n",
        "    return loss"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PumyQPpQVm2",
        "outputId": "6a9d4e60-cbc0-4226-94fa-b1e237f2c234"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  epoch_loss = 0\n",
        "  batch_loss = train(autoencoder, input_dim, device, train_loader, optimizer, epoch, loss_func)\n",
        "  epoch_loss += batch_loss.item()\n",
        "  print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, EPOCHS, epoch_loss))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Training.\n",
            "Train Epoch: 0 [0/5872 (0%)]\tLoss: 0.128137\n",
            "epoch : 1/1, loss = 0.128137\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}